### 9.7.16. TensorCore 5th Generation Family Instructions 

#### 9.7.16.1. [Tensor Memory](#tensor-memory)[](#tensor-memory "Permalink to this headline")

The 5th generation TensorCore has dedicated on-chip memory that is specialized for use by
TensorCore operations. This Tensor Memory is organized as a two-dimensional matrix where
the horizontal rows are called lanes and the vertical columns are called columns.

On architecture `sm_100a`/`sm_100f`, the 5th generation TensorCore’s Tensor Memory has a
two-dimensional structure of 512 columns and 128 rows per CTA, with each cell being 32-bits in size.

Restrictions on threads accessing the Tensor Memory via the load and store operations
are specified in [Access restrictions](#tcgen05-tensor-memory-ld-st-access-restrictions).

##### 9.7.16.1.1. [Tensor Memory Addressing](#tensor-memory-addressing)[](#tensor-memory-addressing "Permalink to this headline")

Tensor Memory addresses are 32-bit wide and specify two components.

1. Lane index
2. Column index

The layout is as follows:

> |  |  |
> | --- | --- |
> | 31 16 | 15 0 |
> | Lane index | Column index |

[Figure 182](#tensor-memory-layout) shows the view of the Tensor Memory Layout within CTA.

![_images/tensor-memory-layout.png](_images/tensor-memory-layout.png)


Figure 182 Tensor Memory Layout and Addressing[](#tensor-memory-layout "Permalink to this image")

##### 9.7.16.1.2. [Tensor Memory Allocation](#tensor-memory-allocation)[](#tensor-memory-allocation "Permalink to this headline")

The Tensor Memory is dynamically allocated. The Tensor Memory must be allocated by a single
warp in a CTA using the
[Tensor Memory Allocation and Management Instructions](#tcgen05-memory-alloc-manage-instructions).

The allocation and deallocation of [Tensor Memory](#tensor-memory) is performed in terms of
columns. The unit of allocation is 32 columns and the number of columns being allocated must be
a power of 2. When a column is allocated, all 128 lanes of the column are allocated.

All of the Tensor Memory that was allocated in a kernel, must be explicitly deallocated
before the kernel exits.

#### 9.7.16.2. [Matrix and Data Movement Shape](#tcgen05-matrix-data-movement-shape)[](#tcgen05-matrix-data-movement-shape "Permalink to this headline")

There are two kinds of shapes involved.

1. Shapes in the data movement operations
2. Shapes in the MMA operations

##### 9.7.16.2.1. [Matrix Shape](#tcgen05-matrix-shape)[](#tcgen05-matrix-shape "Permalink to this headline")

The matrix multiply and accumulate operations support a limited set of shapes for the operand matrices
`A`, `B` and `D`. The shapes of all three matrix operands are collectively described by the tuple
*MxNxK* where `A` is *MxK* matrix, `B` is a *KxN* matrix, and `D` is a *MxN* matrix.

[Table 39](#tcgen05-kind-shapes) shows matrix shapes that are supported for the specified types for the
`tcgen05.mma` operation.

Table 39 Various combinations of .kind and shapes[](#tcgen05-kind-shapes "Permalink to this table")











| Various Combinations | | | | | | Shapes Supported | | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| .kind::\* | Has .ws | CTA Group | Sparsity | dtype | atype/btype |
| `kind::f16` | No `.ws` | 1 | Dense | `.f16` | `.f16` | 64xNxK  128xNxK | N = {8, 16, 24, … 256} steps of 8 | K = 16 |
| `.f32` | `.f16`, `.bf16` |
| Sparse | `.f16` | `.f16` | K = 32 |
| `.f32` | `.f16`, `.bf16` |
| 2 | Dense | `.f16` | `.f16` | 128xNxK  256xNxK | N = {16, 32, … 256} steps of 16 | K = 16 |
| `.f32` | `.f16`, `.bf16` |
| Sparse | `.f16` | `.f16` | K = 32 |
| `.f32` | `.f16`, `.bf16` |
| `.ws` | 1 | Dense | `.f16` | `.f16` | 32xNxK  64xNxK  128xNxK | N = {64, 128, 256} | K = 16 |
| `.f32` | `.f16`, `.bf16` |
| Sparse | `.f16` | `.f16` | N = {64, 128} | K = 32 |
| `.f32` | `.f16`, `.bf16` |
| 2 | Either | `.f16` | `.f16` | Invalid | | |
| `.f32` | `.f16`, `.bf16` |
| `.kind::tf32` | No `.ws` | 1 | Dense | `.f32` | `.tf32` | 64xNxK  128xNxK | N = {8, 16, 24, … 256} steps of 8 | K = 8 |
| Sparse | K = 16 |
| 2 | Dense | 128xNxK  256xNxK | N = {16, 32, … 256} steps of 16 | K = 8 |
| Sparse | K = 16 |
| `.ws` | 1 | Dense | 32xNxK 64xNxK 128xNxK | N = {64, 128, 256} | K = 8 |
| Sparse | N = {64, 128} | K = 16 |
| 2 | Dense | Invalid | | |
| Sparse |
| `.kind::f8f6f4` | No `.ws` | 1 | Dense | `.f32`  `.f16` | `.e4m3`,  `.e5m2`,  `.e2m3`,  `.e3m2`,  `.e2m1` | 64xNxK  128xNxK | N = {8, 16, … 256} steps of 8 | K = 32 |
| Sparse | K = 64 |
| 2 | Dense | 128xNxK  256xNxK | N = {16, 32, … 256} steps of 16 | K = 32 |
| Sparse | K = 64 |
| `.ws` | 1 | Dense | 32xNxK 64xNxK 128xNxK | N = {64, 128, 256} | K = 32 |
| Sparse | N = {64, 128} | K = 64 |
| 2 | Dense | Invalid | | |
| Sparse |
| `.kind::mxf8f6f4` | No `.ws` | 1 | Dense | `.f32` | `.e4m3`,  `.e5m2`,  `.e2m3`,  `.e3m2`,  `.e2m1`  X  (Scale)  `.ue8m0` | 128xNxK | N = {8, 16, … 256} steps of 8 | K = 32 |
| Sparse | K = 64 |
| 2 | Dense | 128xNxK  256xNxK | N = {16, 32, … 256} steps of 16 | K = 32 |
|
| Sparse | 256xNxK | K = 64 |
| `.ws` | 1 | Dense | Invalid | | |
| Sparse |
| 2 | Dense |
| Sparse |
| `.kind::i8` | No `.ws` | 1 | Dense | `.s32` | `.s8`, `.u8` | 64xNxK  128xNxK | N = {8, 16, 24, 32, 48, … 256}  steps of 16 after N > 32 | K = 32 |
| Sparse | K = 64 |
| 2 | Dense | 128xNxK  256xNxK | N = {32, 64, … 256} steps of 32 | K = 32 |
| Sparse | K = 64 |
| `.ws` | 1 | Dense | 32xNxK 64xNxK 128xNxK | N = {64, 128, 256} | K = 32 |
| Sparse | N = {64, 128} | K = 64 |
| 2 | Dense | Invalid | | |
| Sparse |
| `.kind::mxf4` | No `.ws` | 1 | Dense | `.f32` | `.e2m1`  X  (Scale)  `.ue8m0` | 128xNxK | N = {8, 16, … 256} steps of 8 | K = 64 |
| Sparse | K = 128 |
| 2 | Dense | 128xNxK 256xNxK 256xNxK1 | N = {16, 32, … 256} steps of 16 | K = 64  K1 = 96 |
|
| Sparse | 256xNxK | K = 128 |
| `.ws` | 1 / 2 | Either | Invalid | | |
| `.kind::mxf4nvf4` | No `.ws` | 1 | Dense | `.f32` | `.e2m1`  X  (Scale)  `.ue8m0`,  `.ue4m3` | 128xNxK | N = {8, 16, … 256} steps of 8 | K = 64 |
| Sparse | K = 128 |
| 2 | Dense | 128xNxK 256xNxK 256xNxK1 | N = {16, 32, … 256} steps of 16 | K = 64  K1 = 96 |
|
| Sparse | 256xNxK | K = 128 |
| `.ws` | 1 / 2 | Either | Invalid | | |

###### 9.7.16.2.1.1. [Target ISA Note](#tcgen05-matrix-shape-target-isa-note)[](#tcgen05-matrix-shape-target-isa-note "Permalink to this headline")

* K = 96 is only supported for target architecture `sm_103a`.

##### 9.7.16.2.2. [Specifying Matrix Shape](#tcgen05-specify-matrix-shape)[](#tcgen05-specify-matrix-shape "Permalink to this headline")

*M* and *N* can be specified in the [Instruction descriptor](#tcgen05-instruction-descriptor).

*K* cannot be explicitly specified but is implicitly determined by the MMA-kind
and the sparsity, as shown in the [Table 39](#tcgen05-kind-shapes).

##### 9.7.16.2.3. [Data Movement Shape](#tcgen05-data-movement-shape)[](#tcgen05-data-movement-shape "Permalink to this headline")

The data movement shape indicates the dimension of the data to be moved to or from the
[Tensor Memory](#tensor-memory). These shapes are described as a tuple `lane x size` where:

* `lane` indicates the number of rows in the [Tensor Memory](#tensor-memory); and
* `size` indicates the amount of data, in units of bits (b), across the columns in the
  [Tensor Memory](#tensor-memory).

The following shapes are supported by various tcgen05 operations:

| Shape | tcgen05.<op> |
| --- | --- |
| `.16x64b`, `.16x128b`, `.16x256b`, `.16x32bx2`, `.32x32b` | `.ld` / `.st` |
| `.4x256b`, `.32x128b`, `.64x128b`, `.128x256b`, `.128x128b` | `.cp` |
| `.31x256b` (implicit) | `.shift` |

###### 9.7.16.2.3.1. [Memory Layout](#tcgen05-memory-layout)[](#tcgen05-memory-layout "Permalink to this headline")

The following shows the layout of the matrix fragments across threads of the warp.

###### 9.7.16.2.3.1.1. [Matrix fragments for shape .32x32b](#tcgen05-matrix-fragments-shape-3232b)[](#tcgen05-matrix-fragments-shape-3232b "Permalink to this headline")

A `tcgen05{.ld,.st}.32x32b` instruction has the following data vector register.

| Fragment | Elements (low to high) |
| --- | --- |
| A vector expression containing `.num` number of `.b32` registers as mentioned in the [Table 47](#tcgen05-num-shapes-ld). | r0, r1, … |

A warp executing `tcgen05{.ld,.st}.32x32b` will access 32 lanes of the Tensor Memory.
It loads from or stores to each of the lane (32 \* .num)-bits of data as shown in
[Figure 183](#tcgen05-mma-fragment-3232b).

![_images/tcgen05-mma-fragment-3232b.png](_images/tcgen05-mma-fragment-3232b.png)


Figure 183 Matrix Fragment for shape .32x32b[](#tcgen05-mma-fragment-3232b "Permalink to this image")

###### 9.7.16.2.3.1.2. [Matrix fragments for shape .16x64b](#tcgen05-matrix-fragments-shape-6464b)[](#tcgen05-matrix-fragments-shape-6464b "Permalink to this headline")

A `tcgen05{.ld,.st}.16x64b` instruction has the following data vector register.

| Fragment | Elements (low to high) |
| --- | --- |
| A vector expression containing `.num` number of `.b32` registers as mentioned in the [Table 47](#tcgen05-num-shapes-ld). | r0, r1, … |

A warp executing `tcgen05{.ld,.st}.16x64b` will access 16 lanes of the Tensor Memory.
It loads from or stores to each of the lane (64 \* .num)-bits of data as shown in
[Figure 184](#tcgen05-mma-fragment-1664b).

![_images/tcgen05-mma-fragment-1664b.png](_images/tcgen05-mma-fragment-1664b.png)


Figure 184 Matrix Fragment for shape .16x64b[](#tcgen05-mma-fragment-1664b "Permalink to this image")

###### 9.7.16.2.3.1.3. [Matrix fragments for shape .16x128b](#tcgen05-matrix-fragments-shape-16128b)[](#tcgen05-matrix-fragments-shape-16128b "Permalink to this headline")

A `tcgen05{.ld,.st}.16x128b` instruction has the following data vector register.

| Fragment | Elements (low to high) |
| --- | --- |
| A vector expression containing `.num` number of `.b32` registers as mentioned in the [Table 47](#tcgen05-num-shapes-ld). | r0, r1, … |

A warp executing `tcgen05{.ld,.st}.16x128b` will access 16 lanes of the Tensor Memory.
It loads from or stores to each of the lane (128 \* .num)-bits of data as shown in
[Figure 185](#tcgen05-mma-fragment-16128b).

![_images/tcgen05-mma-fragment-16128b.png](_images/tcgen05-mma-fragment-16128b.png)


Figure 185 Matrix Fragment for shape .16x128b[](#tcgen05-mma-fragment-16128b "Permalink to this image")

###### 9.7.16.2.3.1.4. [Matrix fragments for shape .16x256b](#tcgen05-matrix-fragments-shape-16256b)[](#tcgen05-matrix-fragments-shape-16256b "Permalink to this headline")

A `tcgen05{.ld,.st}.16x256b` instruction has the following data vector register.

| Fragment | Elements (low to high) |
| --- | --- |
| A vector expression containing `.num` number of `.b32` registers as mentioned in the [Table 47](#tcgen05-num-shapes-ld). | r0, r1, r2, r3, … |

A warp executing `tcgen05{.ld,.st}.16x256b` will access 16 lanes of the Tensor Memory.
It loads from or stores to each of the lane (256 \* .num)-bits of data as shown in
[Figure 186](#tcgen05-mma-fragment-16256b).

![_images/tcgen05-mma-fragment-16256b.png](_images/tcgen05-mma-fragment-16256b.png)


Figure 186 Matrix Fragment for shape .16x256b[](#tcgen05-mma-fragment-16256b "Permalink to this image")

###### 9.7.16.2.3.1.5. [Matrix fragments for shape .16x32bx2](#tcgen05-matrix-fragments-shape-1632b2)[](#tcgen05-matrix-fragments-shape-1632b2 "Permalink to this headline")

A `tcgen05{.ld,.st}.16x32bx2` instruction has the following data vector register.

| Fragment | Elements (low to high) |
| --- | --- |
| A vector expression containing `.num` number of `.b32` registers as mentioned in the [Table 47](#tcgen05-num-shapes-ld). | r0, r1, … |

A warp executing `tcgen05{.ld,.st}.16x32bx2` will access 16 lanes of the Tensor Memory.
It loads from or stores to each of the lane (32 \* .num)-bits of data as shown in
[Figure 187](#tcgen05-mma-fragment-1632b2).

![_images/tcgen05-mma-fragment-1632b2.png](_images/tcgen05-mma-fragment-1632b2.png)


Figure 187 Matrix Fragment for shape .16x32bx2[](#tcgen05-mma-fragment-1632b2 "Permalink to this image")

#### 9.7.16.3. [Major-ness supported by Strides](#tcgen05-majorness-supported-by-strides)[](#tcgen05-majorness-supported-by-strides "Permalink to this headline")

There are two strides involved while accessing a matrix from shared memory:

1. Leading dimension stride (byte offset or absolute address)
2. Stride dimension byte offset

##### 9.7.16.3.1. [Leading Dimension Stride: relative offset or absolute address](#tcgen05-leading-dimension-byte-offset)[](#tcgen05-leading-dimension-byte-offset "Permalink to this headline")

There are two modes of Leading Dimension Strides as described below.
Bit #52 in the [Shared memory descriptor](#tcgen05-shared-memory-descriptor) is used to distinguish between two modes.

###### 9.7.16.3.1.1. [Relative offset mode](#tcgen05-leading-dimension-byte-offset-relative-offset)[](#tcgen05-leading-dimension-byte-offset-relative-offset "Permalink to this headline")

In this mode, the leading dimension stride is specified as a relative byte offset between the
columns as explained in the below table.
The leading dimension stride can either be specified as a relative offset between the columns
or as an absolute byte address of next buffer. The leading dimension stride is defined
differently for transposed and non-transposed matrices. The leading dimension stride is defined
as follows for matrices whose element types are normalized to 128-bits:

| Major-ness | Definition |
| --- | --- |
| K-Major | * No-Swizzling: the stride from the first column to the second column   of the 8x2 tile in the 128-bit element type normalized matrix. * Swizzled layouts: not used, assumed to be 1. |
| MN-Major | * Interleave: stride from the first 8 columns to the next 8 columns. * Swizzled layouts: stride from the first (swizzle-byte-size/16) rows   to the next (swizzle-byte-size/16) rows. |

###### 9.7.16.3.1.2. [Absolute address mode for K dimension being 48B](#tcgen05-leading-dimension-byte-offset-absolute-address)[](#tcgen05-leading-dimension-byte-offset-absolute-address "Permalink to this headline")

The `tcgen05.mma` instruction with *K-dimension* of 48B would overflow the 128B
shared memory boundary if the data is packed contiguously.

In this case, the absolute address mode can be used to break up the data in the
shared memory into two chunks such that both these chunks are laid out within
the aligned 128-byte address boundary.
The leading dimension absolute address can point to the second data chunk in the shared memory.

###### 9.7.16.3.1.2.1. [Restrictions on the Leading Dimension Absolute Address Stride](#tcgen05-leading-dimension-byte-offset-absolute-address-restriction)[](#tcgen05-leading-dimension-byte-offset-absolute-address-restriction "Permalink to this headline")

Following are the restrictions on the absolute address stride mode:

1. Only 128B swizzle (with 16B atomicity) is supported.
2. Only K-Major mode is supported. That is, the transpose bits(bits #15 and #16) in
   [Instruction descriptor](#tcgen05-instruction-descriptor) must be 0.
3. The matrix base offset must be 0.

##### 9.7.16.3.2. [Stride Dimension Byte Offset](#tcgen05-stride-dimension-byte-offset)[](#tcgen05-stride-dimension-byte-offset "Permalink to this headline")

The stride dimension byte offset is defined differently for transposed and non-transposed
matrices. The stride dimension byte offset is defined as follows for matrices whose element
types are normalized to 128-bits:

| Major-ness | Definition |
| --- | --- |
| K-Major | The offset from the first 8 rows to the next 8 rows. |
| MN-Major | * Interleave: offset from the first row to the next row. * Swizzled layout: offset from the first 8 columns to the next 8   columns |

##### 9.7.16.3.3. [Canonical Layouts](#tcgen05-canonical-layouts)[](#tcgen05-canonical-layouts "Permalink to this headline")

In terms of [CuTe layouts](https://docs.nvidia.com/cutlass/media/docs/cpp/cute/01_layout.html)
the canonical layout can be expressed as follows:

| Major- ness | Swizzling mode | Canonical Layout without swizzling | [Swizzling](https://github.com/NVIDIA/cutlass/blob/bf9da7b76c766d7ee7d536afc77880a4ef1f1156/include/cute/swizzle.hpp) on the previous column |
| --- | --- | --- | --- |
| MN- major | No-swizzling or Interleaved | ((T,1,m),(8,k)):((1,T,SBO),(1T,LBO)) | Swizzle<0, 4, 3> |
| 32B Swizzling | ((T,2,m),(8,k)):((1,T,LBO),(2T,SBO)) | Swizzle<1, 4, 3> |
| 64B Swizzling | ((T,4,m),(8,k)):((1,T,LBO),(4T,SBO)) | Swizzle<2, 4, 3> |
| 128B Swizzling | ((T,8,m),(8,k)):((1,T,LBO),(8T,SBO)) | Swizzle<3, 4, 3> |
| K- major | No-swizzling or Interleaved | ((8,m),(T,2k)):((1T,SBO),(1,LBO)) | Swizzle<0, 4, 3> |
| 32B Swizzling | ((8,m),(T,2k)):((2T,SBO),(1,T)) | Swizzle<1, 4, 3> |
| 64B Swizzling | ((8,m),(T,2k)):((4T,SBO),(1,T)) | Swizzle<2, 4, 3> |
| 128B Swizzling | ((8,m),(T,2k)):((8T,SBO),(1,T)) | Swizzle<3, 4, 3> |

where

* T = 128 / sizeof-elements-in-bits
  T represents scale factor which normalizes matrix element types to 128-bits.
* m represents the number of repeating patterns across rows.
* k represents the number of repeating patterns across columns.

Examples

* K-Major, no-swizzling and tf32 type: [Figure 188](#tcgen05-k-no-swizzle-tf32)

  ![_images/async-warpgroup-k-no-swizzle-tf32.png](_images/async-warpgroup-k-no-swizzle-tf32.png)


  Figure 188 K major, no-swizzling and tf32 type[](#tcgen05-k-no-swizzle-tf32 "Permalink to this image")

  the strides and related details are as follows:

  Exact layout : Swizzle<0,4,3> o ((8,2),(4,4)):((4,32),(1,64))

  Canonical Layout :Swizzle<0,4,3> o ((8,m),(T,2k)):((1T,SBO),(1,LBO))

  | Parameters | Value |
  | --- | --- |
  | T | 4 |
  | m | 2 |
  | k | 2 |
  | LBO (relative offset) | 64\*sizeof(tf32) |
  | SBO | 32\*sizeof(tf32) |
  | Encoding of LBO in descriptor | (LBO) >> 4 = 16 |
  | Encoding of SBO in descriptor | (SBO) >> 4 = 8 |
* K-Major, 32B swizzling and tf32 type: [Figure 189](#tcgen05-k-32b-swizzle-tf32)

  ![_images/async-warpgroup-k-32B-swizzle-tf32.png](_images/async-warpgroup-k-32B-swizzle-tf32.png)


  Figure 189 K major, 32B swizzling and tf32 type[](#tcgen05-k-32b-swizzle-tf32 "Permalink to this image")

  the strides and related details are as follows:

  Exact layout : Swizzle<1,4,3> o ((8,2),(4,4)):((8,64),(1,4))

  Canonical Layout :Swizzle<1,4,3> o ((8,m),(T,2k)):((2T,SBO),(1,T))

  | Parameters | Value |
  | --- | --- |
  | T | 4 |
  | m | 2 |
  | k | 2 |
  | LBO (relative offset) | NA |
  | SBO | 64\*sizeof(tf32) |
  | Encoding of LBO in descriptor | 1 (assumed) |
  | Encoding of SBO in descriptor | (SBO) >> 4 = 16 |
* MN-Major, no-swizzling and bf16 type: [Figure 190](#tcgen05-mn-no-swizzle-bf16)

  ![_images/async-warpgroup-mn-no-swizzle-bf16.png](_images/async-warpgroup-mn-no-swizzle-bf16.png)


  Figure 190 MN major, no-swizzling and bf16 type[](#tcgen05-mn-no-swizzle-bf16 "Permalink to this image")

  the strides and related details are as follows:

  Exact layout : Swizzle<0,4,3> o ((8,1,2),(8,2)):((1,8,64),(8,128))

  Canonical Layout :Swizzle<0,4,3> o ((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))

  | Parameters | Value |
  | --- | --- |
  | T | 8 |
  | m | 2 |
  | k | 2 |
  | LBO (relative offset) | 128\*sizeof(bf16) |
  | SBO | 64\*sizeof(bf16) |
  | Encoding of LBO in descriptor | (LBO) >> 4 = 16 |
  | Encoding of SBO in descriptor | (SBO) >> 4 = 8 |
* MN-Major, 32B swizzling and bf16 type: [Figure 191](#tcgen05-mn-32b-swizzle-bf16)

  ![_images/async-warpgroup-mn-32B-swizzle-bf16.png](_images/async-warpgroup-mn-32B-swizzle-bf16.png)


  Figure 191 MN major, 32B swizzling and bf16 type[](#tcgen05-mn-32b-swizzle-bf16 "Permalink to this image")

  the strides and related details are as follows:

  Exact layout : Swizzle<1,4,3> o ((8,2,2),(8,2)):((1,8,128),(16,256))

  Canonical Layout :Swizzle<1,4,3> o ((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))

  | Parameters | Value |
  | --- | --- |
  | T | 8 |
  | m | 2 |
  | k | 2 |
  | LBO (relative offset) | 128\*sizeof(bf16) |
  | SBO | 256\*sizeof(bf16) |
  | Encoding of LBO in descriptor | (LBO) >> 4 = 16 |
  | Encoding of SBO in descriptor | (SBO) >> 4 = 32 |
* MN-Major, 64B swizzling and bf16 type: [Figure 192](#tcgen05-mn-64b-swizzle-bf16)

  ![_images/async-warpgroup-mn-64B-swizzle-bf16.png](_images/async-warpgroup-mn-64B-swizzle-bf16.png)


  Figure 192 MN major, 64B swizzling and bf16 type[](#tcgen05-mn-64b-swizzle-bf16 "Permalink to this image")

  the strides and related details are as follows:

  Exact layout : Swizzle<2,4,3> o ((8,4,2),(8,2)):((1,8,256),(32,512))

  Canonical Layout :Swizzle<2,4,3> o ((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))

  | Parameters | Value |
  | --- | --- |
  | T | 8 |
  | m | 2 |
  | k | 2 |
  | LBO (relative offset) | 256\*sizeof(bf16) |
  | SBO | 512\*sizeof(bf16) |
  | Encoding of LBO in descriptor | (LBO) >> 4 = 32 |
  | Encoding of SBO in descriptor | (SBO) >> 4 = 64 |

#### 9.7.16.4. [Matrix Descriptors](#tcgen05-matrix-descriptors)[](#tcgen05-matrix-descriptors "Permalink to this headline")

There are three kinds of matrix descriptors used by the `tcgen05` family of instructions.

##### 9.7.16.4.1. [Shared memory descriptor](#tcgen05-shared-memory-descriptor)[](#tcgen05-shared-memory-descriptor "Permalink to this headline")

The shared memory descriptor describes the properties of multiplicand matrix in shared
memory including its location in the shared memory of the current *CTA*. It is a 64-bit
value contained in a register with the following layout:

Table 40 Shared memory descriptor layout[](#tcgen05-shared-memory-desc-layout "Permalink to this table")





| Bit-field | Size in bits | Description |
| --- | --- | --- |
| 0-13 | 14 | matrix-descriptor-encode (Matrix start address) |
| 16-29 | 14 | matrix-descriptor-encode ([Leading dimension byte offset relative](#tcgen05-leading-dimension-byte-offset))  OR  matrix-descriptor-encode ([Leading dimension byte address absolute](#tcgen05-leading-dimension-byte-offset)) |
| 32-45 | 14 | matrix-descriptor-encode ([Stride dimension byte offset](#tcgen05-stride-dimension-byte-offset)) |
| 46-48 | 3 | Fixed constant value of 0b001 |
| 49-51 | 3 | Matrix base offset |
| 52 | 1 | Leading dimension stride mode: - 0: byte offset relative - 1: byte address absolute |
| 53-60 | 8 | Fixed constant value of 0xb00000000 |
| 61-63 | 3 | Specifies the swizzling mode to be used: 0. No swizzling 1. 128-Byte with 32B atomic swizzling 2. 128-Byte swizzling 4. 64-Byte swizzling 6. 32-Byte swizzling  Note: Values 3, 5 and 7 are invalid |

where matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4

The value of base offset is 0 when the repeating pattern of the specified swizzling mode
starts as per shown in [Table 41](#tcgen05-start-addr-swizzle-mode).

Table 41 Starting address of repeating pattern for various swizzling modes[](#tcgen05-start-addr-swizzle-mode "Permalink to this table")




| Swizzling mode | Starting address of the repeating pattern |
| --- | --- |
| 128-Byte swizzle | 1024-Byte boundary |
| 64-Byte swizzle | 512-Byte boundary |
| 32-Byte swizzle | 256-Byte boundary |

Otherwise, the base offset must be a non-zero value, computed using the following formula:
`base offset = (pattern start addr >> 0x7) & 0x7`

The following must be 16-byte aligned:

1. Matrix start address
2. Leading dimension byte offset
3. Stride dimension byte offset

###### 9.7.16.4.1.1. [Target ISA Note](#tcgen05-shared-memory-descriptor-target-isa-note)[](#tcgen05-shared-memory-descriptor-target-isa-note "Permalink to this headline")

* The byte address mode for the leading dimension stride is supported on `sm_103a`.

##### 9.7.16.4.2. [Instruction descriptor](#tcgen05-instruction-descriptor)[](#tcgen05-instruction-descriptor "Permalink to this headline")

The instruction descriptor describes the shapes, types and other details of all the matrices
and the matrix-multiplication-and-accumulation operation. It is a 32-bit value in registers
and the exact layout is dependent on the MMA-Kind:

Table 42 Instruction descriptor format for .kind::tf32, .kind::f16, .kind::f8f6f4 and .kind::i8[](#tcgen05-instuction-desc-kind-tf32-f16-f8f6f4 "Permalink to this table")









| Bits | Size  (bits) | Description | Values | | | |
| --- | --- | --- | --- | --- | --- | --- |
| .kind::tf32 | .kind::f16 | .kind::f8f6f4 | .kind::i8 |
| 0-1 | 2 | [Sparsity selector](#tcgen05-sparse-matrices-sparsity-selector), if Sparsity is enabled | 0-3 | | | |
| 2 | 1 | Sparsity | Dense = 0  Sparse = 1 | | | |
| 3 | 1 | Saturate for integer types | 0 (NA) | | | No Saturate = 0 Saturate = 1 |
| 4-5 | 2 | dtype (Matrix D type) | F32 = 1 | F16 = 0 F32 = 1 | | S32 = 2 |
| 6 | 1 | Reserved | 0 | | | |
| 7-9 | 3 | atype (Matrix A type) | TF32 = 2 | F16 = 0  BF16 = 1 | E4M3 = 0 E5M2 = 1 E2M3 = 3 E3M2 = 4 E2M1 = 5 | Unsigned 8b = 0  Signed 8b = 1 |
| 10-12 | 3 | btype (Matrix B type) |
| 13 | 1 | Negate A Matrix | No Negate = 0  Negate = 1 | | | No Negate = 0 |
| 14 | 1 | Negate B Matrix |
| 15 | 1 | Transpose A Matrix | No Transpose = 0  Transpose = 1 | | | |
| 16 | 1 | Transpose B Matrix |
| 17-22 | 6 | N, Dimension of Matrix B (3 LSBs not included) | N >> 3 | | | |
| 23 | 1 | Reserved | 0 | | | |
| 24-28 | 5 | M, Dimension of Matrix A (4 LSBs not included) | M >> 4 | | | |
| 29 | 1 | Reserved | 0 | | | |
| 30-31 | 2 | Maximum shift while attempting B matrix -reuse in `.ws` | no shift = 0 maximum shift of 8 = 1 maximum shift of 16 = 2 maximum shift of 32 = 3 | | | |

Table 43 Instruction descriptor format for .kind::mxf8f6f4[](#tcgen05-instuction-desc-kind-mxf8f6f4 "Permalink to this table")






| Bits | Size  (bits) | Description | Values |
| --- | --- | --- | --- |
| .kind::mxf8f6f4 |
| 0-1 | 2 | Reserved | 0 |
| 2 | 1 | Sparsity | Dense = 0  Sparse = 1 |
| 3 | 1 | Reserved | 0 |
| 4-5 | 2 | [Matrix B Scale Factor Data ID](#tcgen05-mma-scale-factor-b) | 0-3 |
| 6 | 1 | Reserved | 0 |
| 7-9 | 3 | atype (Matrix A type) | E4M3 = 0 E5M2 = 1 E2M3 = 3 E3M2 = 4 E2M1 = 5 |
| 10-12 | 3 | btype (Matrix B type) |
| 13 | 1 | Negate A Matrix | No Negate = 0  Negate = 1 |
| 14 | 1 | Negate B Matrix |
| 15 | 1 | Transpose A Matrix | No Transpose = 0  Transpose = 1 |
| 16 | 1 | Transpose B Matrix |
| 17-22 | 6 | N, Dimension of Matrix B (3 LSBs not included) | N >> 3 |
| 23 | 1 | Scale Matrix Type, for both scale\_A / scale\_B | UE8M0 = 1 |
| 24-26 | 3 | Reserved | 0 |
| 27-28 | 2 | M, Dimension of Matrix A (7 LSBs not included) | M >> 7 |
| 29-30 | 2 | [Matrix A Scale Factor Data ID](#tcgen05-mma-scale-factor-a) | 0-3 |
| 31 | 1 | Reserved | 0 |

Table 44 Instruction descriptor format for .kind::mxf4 and .kind::mxf4nvf4[](#tcgen05-instuction-desc-kind-mxf4-mxf4nvf4 "Permalink to this table")







| Bits | Size  (bits) | Description | Values | |
| --- | --- | --- | --- | --- |
| .kind::mxf4 | .kind::mxf4nvf4 |
| 0-1 | 2 | Reserved | 0 | |
| 2 | 1 | Sparsity | Dense = 0  Sparse = 1 | |
| 3 | 1 | Reserved | 0 | |
| 4-5 | 2 | [Matrix B Scale Factor Data ID](#tcgen05-mma-scale-factor-b) | 0 or 2 | |
| 6 | 1 | Reserved | 0 | |
| 7-9 | 3 | atype (Matrix A type) | E2M1 = 1 | |
| 10-11 | 2 | btype (Matrix B type) |
| 12 | 1 | Reserved | 0 | |
| 13 | 1 | Negate A Matrix | No Negate = 0  Negate = 1 | |
| 14 | 1 | Negate B Matrix |
| 15 | 1 | Transpose A Matrix | No Transpose = 0 | |
| 16 | 1 | Transpose B Matrix |
| 17-22 | 6 | N, Dimension of Matrix B (3 LSBs not included) | N >> 3 | |
| 23 | 1 | Scale Matrix Type, for both scale\_A / scale\_B | UE8M0 = 1 | UE4M3 = 0 |
| 24-26 | 3 | Reserved | 0 | |
| 27-28 | 2 | M, Dimension of Matrix A (7 LSBs not included) | M >> 7 | |
| 29-30 | 2 | [Matrix A Scale Factor Data ID](#tcgen05-mma-scale-factor-a) | 0 or 2 | |
| 31 | 1 | K Dimension | (Dense K=64 / Sparse K=128) = 0  (Dense K=96) = 1 | |

##### 9.7.16.4.3. [Zero-Column Mask Descriptor](#tcgen05-zero-column-mask-descriptor)[](#tcgen05-zero-column-mask-descriptor "Permalink to this headline")

The zero-column mask descriptor is used to generate a mask that specifies which columns of
`B` matrix will have zero value for the MMA operation regardless of the values present in
the shared memory. The total size of the generated mask is N-bits.

A 0-bit in the mask specifies that values of the corresponding column in matrix `B` should
be used for the MMA operation. A 1-bit in the mask specifies 0s must be used for the entire
column for the MMA operation.

The zero-column mask descriptor is a 64-bit value in registers with the following layout:

Table 45 Zero-Column Mask descriptor layout[](#tcgen05-zero-column-mask-desc "Permalink to this table")






| Bits | Size (bits) | Field Name | Description |
| --- | --- | --- | --- |
| 0-7 | 8 | Start Count 0 (sc0) | Specifies the LSBs that must be skipped  for sub-mask mask-i |
| 8-15 | 8 | Start Count 1 (sc1) |
| 16-23 | 8 | Start Count 2 (sc2) |
| 24-31 | 8 | Start Count 3 (sc3) |
| 32 | 1 | First Span 0 (fs0) | Specifies the starting value for  sub-mask mask-i |
| 33 | 1 | First Span 1 (fs1) |
| 34 | 1 | First Span 2 (fs2) |
| 35 | 1 | First Span 3 (fs3) |
| 36-38 | 3 | Reserved |  |
| 39 | 1 | Non-Zero Mask | Value 0 indicates generated mask will have all 0s Value 1 indicates the mask has to be generated |
| 40-47 | 8 | Skip Span | (Count of consecutive columns where B matrix is used) - 1 |
| 48-55 | 8 | Use Span | (Count of consecutive columns where 0s ar used) - 1 |
| 56-61 | 6 | Column Shift | Shifts column by specified amount. Thus allows MMA on non-0 starting column. Max shift amount = 16 for M=32 Max shift amount = 32 otherwise |

The zero-column mask is made up of one or more sub-mask depending on M, as shown in the table:

| M | Zero-Column Mask breakup | Sub-masks | First Span used | Start Column used |
| --- | --- | --- | --- | --- |
| 128 | Single sub-mask of size N-bits | mask0 | fs0 | sc0 |
| 64 | Two sub-masks, each with size of N/2 bits | mask0, mask1 | fs0, fs1 | sc0, sc1 |
| 32 | Four sub-masks, each with size of N/4 bits | mask0, mask1 mask2, mask3 | fs0, fs1, fs2, fs3 | sc0, sc1, sc2, sc3 |

The following table shows the coverage of the sub-masks across N-dimension:

| Sub-mask | M | | |
| --- | --- | --- | --- |
| 128 | 64 | 32 |
| mask0 | Columns [0, N-1] | Columns [0, N/2-1] | Columns [0, N/4-1] |
| mask1 | – | Columns [N/2, N-1] | Columns [N/4, N/2-1] |
| mask2 | – | – | Columns [N/2, (N/4\*3)-1] |
| mask3 | – | – | Columns [(N/4\*3), N-1] |

The following examples shows zero-column mask descriptor and their corresponding mask generated:

1. Example 1: M = 128

   Input zero-column mask descriptor:

   | Start count | First span | Non-Zero Mask | Skip Span | Use Span | Shift |
   | --- | --- | --- | --- | --- | --- |
   | {0, 0, 0, 0} | {0, 0, 0, 0} | 0 | 4 | 3 | 0 |

   Output zero-column mask: 0x0.

   As Non-Zero Mask field is 0, the mask is 0x0. All the columns of the matrix `B` will be used
   for the MMA operation.
2. Example 2: M = 128

   Input zero-column mask descriptor:

   | Start count | First span | Non-Zero Mask | Skip Span | Use Span | Shift |
   | --- | --- | --- | --- | --- | --- |
   | {-, -, -, 0} | {-, -, -, 0} | 1 | 2 | 3 | 0 |

   Output mask0: 0b … 111 0000 111 0000 (size = N)
3. Example 3: M = 64

   Input zero-column mask descriptor:

   | Start count {.., sc1, sc0} | First span {.., fs1, fs0} | Non-Zero Mask | Skip Span | Use Span | Shift |
   | --- | --- | --- | --- | --- | --- |
   | {-, -, 0, 0} | {-, -, 0, 1} | 1 | 2 | 3 | 0 |

   Output mask0: 0b … 111 0000 111 0000 111

   Output masl1: 0b … 0000 111 0000 111 0000
4. Example 4: M = 32

   Input zero-column mask descriptor:

   | Start count {sc3, sc2, sc1, sc0} | First span {fs3, fs2, fs1, fs0} | Non-Zero Mask | Skip Span | Use Span | Shift |
   | --- | --- | --- | --- | --- | --- |
   | {1, 2, 1, 0} | {0, 0, 1, 1} | 1 | 2 | 3 | 2 |

   Output mask0: 0b … 0000 111 0000 111

   Output mask1: 0b … 0000 111 0000 11

   Output mask2: 0b … 111 0000 111 00

   Output mask3: 0b … 111 0000 111 000

   If N = 128 then `B` Matrix with columns from 2 to 129 will be used for the MMA operation,
   due to the shift of 2.

#### 9.7.16.5. [Issue Granularity](#tcgen05-issue-granularity)[](#tcgen05-issue-granularity "Permalink to this headline")

Each of the `tcgen05` operation has different requirements for the number of
threads/warps that needs to issue them.

The following table lists the execution granularity requirements of each of the
`tcgen05` operation:

Table 46 Execution granularity requirements for tcgen05 operations[](#tcgen05-ops-execution-granularity "Permalink to this table")





| tcgen05 operation | .cta\_group | Issue Granularity |
| --- | --- | --- |
| ``` .mma,  .cp,  .shift,  .commit ``` | ::1 | An issue from a single thread in the current CTA would initiate the base operation. |
| ::2 | Issue from a single thread from the [CTA-Pair](#tcgen05-cta-pair) would initiate the base operation. When the current CTA issues the operation, the peer CTA should be active and should not have exited. |
| ``` .alloc,  .dealloc,  .relinquish_alloc_permit ``` | ::1 | Issue from a single warp in the current CTA would initiate the allocation management instruction. |
| ::2 | Issue from two warps, one in each of the current CTA and its [Peer CTA](#tcgen05-peer-cta), collectively needs to perform the operation. When the current CTA issues the operation, the peer CTA should be active and should not have exited. |
| ``` .ld,  .st,  .wait::{ld, st} ``` | N/A | Issue from a warp in the current CTA can access only 1/4 of the Tensor Memory of the current CTA. So, a warpgroup is needed to access the entire Tensor Memory of the current CTA. |
| ``` .fence::* ``` | N/A | A thread needs to fence all its accesses to the tensor memory that it wants to order with other accesses to the tensor memory from other threads. |

##### 9.7.16.5.1. [CTA Pair](#tcgen05-cta-pair)[](#tcgen05-cta-pair "Permalink to this headline")

Any 2 CTAs within the cluster whose `%cluster_ctarank` differs by the last bit only
is said to form a CTA pair.

Within a CTA pair, the CTA whose last bit in the `%cluster_ctarank` is:

* 0 is termed the even numbered CTA within the CTA pair.
* 1 is termed as the odd numbered CTA within the CTA pair.

Most of the `tcgen05` operations can either execute at a single CTA level granularity OR
at a CTA pair level granularity. When a `tcgen05` operation is performed at CTA pair
granularity, the Tensor Memory of both the CTAs within the CTA pair are accessed. The set
of threads that need to issue the `tcgen05` operation is listed in the
[Issue Granularity](#tcgen05-issue-granularity).

##### 9.7.16.5.2. [Peer CTA](#tcgen05-peer-cta)[](#tcgen05-peer-cta "Permalink to this headline")

The peer CTA of the odd CTA within the CTA pair is the even CTA in the same pair.
Similarly, the peer CTA of the even CTA within the CTA pair is the odd CTA in the same pair.

#### 9.7.16.6. [Memory Consistency Model for 5th generation of TensorCore operations](#tcgen05-memory-consistency-model)[](#tcgen05-memory-consistency-model "Permalink to this headline")

Ordering of `tcgen05` instructions is described in terms of two key concepts:

1. Pipelined tcgen05 instructions
2. Specialized tcgen05-specific inter-thread synchronization mechanisms.

These concepts combine to form four canonical synchronization patterns, as described further below.

##### 9.7.16.6.1. [Asynchronous Operations](#tcgen05-memory-consistency-model-async-operations)[](#tcgen05-memory-consistency-model-async-operations "Permalink to this headline")

The tcgen05 family of instructions are divided into 2 categories:

1. Asynchronous instructions:

   These `tcgen05` operations are not inherently ordered with respect to
   other `tcgen05` operations in the same thread (unless pipelined as mentioned below).
2. Synchronous instructions:

   These `tcgen05` operations are inherently ordered with respect to other `tcgen05`
   operations in the same order.

   The Tensor Memory allocation related instructions that access shared memory maintain
   same-address ordering with respect to non-`tcgen05` instructions.

The following table lists the category of each of the `tcgen05` instruction:

| tcgen05.\* operation | Category |
| --- | --- |
| `.alloc` | Synchronous  instructions |
| `.dealloc` |
| `.relinquish_alloc_permit` |
| `.fence::*` |
| `.wait::*` |
| `.commit` |
| `.mma` | Asynchronous  instructions |
| `.cp` |
| `.shift` |
| `.ld` |
| `.st` |

##### 9.7.16.6.2. [Pipelined tcgen05 Instructions](#tcgen05-memory-consistency-model-pipelined-instructions)[](#tcgen05-memory-consistency-model-pipelined-instructions "Permalink to this headline")

The asynchronous `tcgen05` operations may execute and complete in a different order than they
were issued. However, some specific pairs of the asynchronous `tcgen05` instructions form
`tcgen05` pipelines, where in the two asynchronous operations are guaranteed to execute in
the same order as the instructions that issued them. The specific pairings are as follows:

1. `tcgen05.mma.cta_group::N` -> `tcgen05.mma.cta_group::N` (same N and accumulator and shape)
2. `tcgen05.copy.cta_group::N` -> `tcgen05.mma.cta_group::N` (same N)
3. `tcgen05.shift.cta_group::N` -> `tcgen05.mma.cta_group::N` (same N)
4. `tcgen05.shift.cta_group::N` -> `tcgen05.cp.4x256b.cta_group::N` (same N)
5. `tcgen05.mma.cta_group::N` -> `tcgen05.shift.cta_group::N` (same N)

###### 9.7.16.6.2.1. [Implicitly pipelined tcgen05 Instructions](#tcgen05-memory-consistency-model-pipelined-instructions-implicit)[](#tcgen05-memory-consistency-model-pipelined-instructions-implicit "Permalink to this headline")

Instructions `tcgen05.commit` and `tcgen05.wait` are implicitly pipelined with respect
to previously issued `tcgen05.{mma,cp,shift}` and `tcgen05.{ld,st}` instructions
respectively that they track from the same thread.

###### 9.7.16.6.2.1.1. [mbarrier based completion mechanism](#tcgen05-memory-consistency-model-mbarrier-completion)[](#tcgen05-memory-consistency-model-mbarrier-completion "Permalink to this headline")

Completion of the following instruction’s asynchronous operations is observed
through the mbarrier based waiting mechanism:

1. `tcgen05.mma`
2. `tcgen05.cp`
3. `tcgen05.shift`

`tcgen05.commit` is used to track the completion of the above asynchronous instructions.

Following are the implicitly pipelined `tcgen05` instruction pairing that uses mbarrier
based completion mechanism:

* `tcgen05.mma.cta_group::N` -> `tcgen05.commit.cta_group::N` (same N)
* `tcgen05.cp.cta_group::N` -> `tcgen05.commit.cta_group::N` (same N)
* `tcgen05.shift.cta_group::N` -> `tcgen05.commit.cta_group::N` (same N)

###### 9.7.16.6.2.1.2. [`tcgen05.wait` instruction based completion mechanism](#tcgen05-memory-consistency-model-wait-completion)[](#tcgen05-memory-consistency-model-wait-completion "Permalink to this headline")

Completion of the following instruction’s asynchronous operations is observed through
`tcgen05.wait` based waiting mechanism:

1. `tcgen05.ld`
2. `tcgen05.st`

`tcgen05.wait::ld` and `tcgen05.wait::st` is used to track the completion of the
`tcgen05.ld` and `tcgen05.st` asynchronous instructions.

Following are the implicitly pipelined `tcgen05` instruction pairing that uses
`tcgen05.wait` based completion mechanism:

* `tcgen05.ld` -> `tcgen05.wait::ld`
* `tcgen05.st` -> `tcgen05.wait::st`

##### 9.7.16.6.3. [Specialized Inter-thread Synchronization for tcgen05 instructions](#tcgen05-memory-consistency-model-inter-thread-sync)[](#tcgen05-memory-consistency-model-inter-thread-sync "Permalink to this headline")

The `tcgen05` instructions support a specialized inter-thread synchronization which are
optimized for `tcgen05` family of instructions. The standard memory consistency model
synchronization mechanisms also apply to the `tcgen05` family of instructions.

The [TensorCore 5th Generation Specialized Synchronization Operations](#tcgen05-special-sync-operations) section contains the specialized inter-thread
synchronization for tcgen05 instructions.

The `tcgen05.fence::before_thread_sync` and `tcgen05.fence::after_thread_sync` composes
with execution ordering instructions, like morally strong `ld`/`st`/`atom` instructions,
`mbarrier` instruction, `barrier` instructions and so on, to establish an ordering between
the `tcgen05` operations across threads. The asynchronous `tcgen05` instructions that are
ordered across threads also form a `tcgen05` pipeline.

An asynchronous `tcgen05` operation prior to a `tcgen05.fence::before_thread_sync` is ordered
before all subsequent `tcgen05` and the execution ordering operations.

An asynchronous `tcgen05` operation subsequent to a `tcgen05.fence::after_thread_sync` is
ordered after all the prior `tcgen05` and the execution ordering operations.

##### 9.7.16.6.4. [Canonical synchronization patterns](#tcgen05-memory-consistency-model-canonical-sync-patterns)[](#tcgen05-memory-consistency-model-canonical-sync-patterns "Permalink to this headline")

Using the above rules, the following are the five canonical synchronization patterns:

###### 9.7.16.6.4.1. [Pipelined instructions, same thread](#tcgen05-memory-consistency-model-canonical-sync-patterns-pipelined-same-thread)[](#tcgen05-memory-consistency-model-canonical-sync-patterns-pipelined-same-thread "Permalink to this headline")

In this pattern, no explicit ordering mechanism is needed and the ordering guarantee is
provided by the pipelined instruction pairing.

Example:

```
tcgen05.mma

tcgen05.mma (same shape and accumulator)
```

The two instructions will be executed in program order.

###### 9.7.16.6.4.2. [Non-pipelined instructions, same thread](#tcgen05-memory-consistency-model-canonical-sync-patterns-non-pipelined-same-thread)[](#tcgen05-memory-consistency-model-canonical-sync-patterns-non-pipelined-same-thread "Permalink to this headline")

In this pattern, explicit waiting mechanisms are used to wait for the completion of the
asynchronous `tcgen05` operations.

Example 1:

```
tcgen05.st

tcgen05.wait::st

tcgen05.ld
```

`tcgen05.wait::st` is used to wait for the completion of the prior asynchronous
instruction `tcgen05.st`.

Example 2:

```
tcgen05.mma [d], ...

tcgen05.commit.mbarrier::arrive::one

mbarrier.try_wait.relaxed.cluster (loop until successful)

tcgen05.fence::after_thread_sync

tcgen05.ld [d], ...
```

For the completion of the asynchronous `tcgen05.mma`, `tcgen05.commit` is used.

As `tcgen05.ld` is an asynchronous operation, the instruction `tcgen05.fence::after_thread_sync`
is needed.

No explicit `tcgen05.fence::before_thread_sync` is needed as this is implicitly performed by
`tcgen05.commit`. The combination of `tcgen05.mma` and `tcgen05.commit` forms a
conceptual asynchronous pipeline and establishes execution ordering.

```
tcgen05.mma [d], ...

tcgen05.fence::before_thread_sync

mbarrier::arrive
```

###### 9.7.16.6.4.3. [Pipelined instructions, different thread](#tcgen05-memory-consistency-model-canonical-sync-patterns-pipelined-diff-thread)[](#tcgen05-memory-consistency-model-canonical-sync-patterns-pipelined-diff-thread "Permalink to this headline")

In this pattern, no explicit waiting mechanism is needed but proper synchronization between threads is needed.

Example:

| Thread 0 | Thread 1 |
| --- | --- |
| ``` tcgen05.cp  tcgen05.fence::before_thread_sync  mbarrier.arrive.relaxed.cluster ``` |  |
|  | ``` mbarrier.try_wait.relaxed.cluster // loop till success  tcgen05.fence::after_thread_sync  tcgen05.mma ``` |

###### 9.7.16.6.4.4. [Non-pipelined instructions, different thread](#tcgen05-memory-consistency-model-canonical-sync-patterns-non-pipelined-diff-thread)[](#tcgen05-memory-consistency-model-canonical-sync-patterns-non-pipelined-diff-thread "Permalink to this headline")

In this pattern, the producer threads that issue the asynchronous `tcgen05` instructions
must explicitly wait for the instructions’ completion before synchronizing with the consumer threads.

Example 1:

| Thread 0 | Thread 1 |
| --- | --- |
| ``` tcgen05.ld  tcgen05.wait::ld  tcgen05.fence::before_thread_sync  mbarrier.arrive.relaxed.cluster ``` |  |
|  | ``` mbarrier.try_wait.relaxed.cluster // loop till success  tcgen05.fence::after_thread_sync  tcgen05.mma ``` |

Example 1:

| Thread 0 | Thread 1 |
| --- | --- |
| ``` tcgen05.mma  tcgen05.commit.mbarrier::arrive::one [mbar] ``` |  |
|  | ``` mbarrier.try_wait.relaxed.cluster [mbar] // loop till success  tcgen05.fence::after_thread_sync  tcgen05.ld ``` |

The synchronization mechanisms can also be composed with each other. For example:

| Thread 0 | Thread 1 |
| --- | --- |
| ``` tcgen05.mma  tcgen05.commit.mbarrier::arrive::one [bar1]  mbarrier.try_wait.relaxed.cluster [bar1] // loop  ...  tcgen05.fence::after_thread_sync  ...// completion is guaranteed  tcgen05.fence::before_thread_sync  mbarrier.arrive.relaxed.cluster [bar2] // loop  ... ``` |  |
|  | ``` mbarrier.try_wait.relaxed.cluster [bar2] // loop  ...  tcgen05.fence::after_thread_sync  tcgen05.ld ``` |

###### 9.7.16.6.4.5. [Register dependencies, same thread](#tcgen05-memory-consistency-model-canonical-sync-patterns-reg-dependency-same-thread)[](#tcgen05-memory-consistency-model-canonical-sync-patterns-reg-dependency-same-thread "Permalink to this headline")

For `tcgen05.ld`, an intra-thread ordering through true register dependency will be respected
regardless of the presence or absence of other forms of synchronization. This form of register
dependency does not imply any other form of ordering. For example, a register dependency does
not imply that a dependee instruction’s memory accesses will be performed before a dependent
instruction’s memory accesses. To enforce such memory orderings and avoiding anti-dependency
hazards around `tcgen05.ld`, `tcgen05.wait::ld` must be used.

Example:

```
tcgen05.ld %r1, ...;

tcgen05.mma ..., %r1, ...;
```

##### 9.7.16.6.5. [Shared Memory Accesses](#tcgen05-memory-consistency-model-smem-access)[](#tcgen05-memory-consistency-model-smem-access "Permalink to this headline")

The shared memory accesses by `tcgen05.mma` and `tcgen05.cp` operations are performed
in the asynchronous proxy (async proxy).

Accessing the same memory location across miltiple proxies needs a cross-proxy fence.
For the async proxy, `fence.proxy.async` should be used to synchronize memory between
generic proxy and the async proxy.

#### 9.7.16.7. [Tensor Memory Allocation and Management Instructions](#tcgen05-memory-alloc-manage-instructions)[](#tcgen05-memory-alloc-manage-instructions "Permalink to this headline")

##### 9.7.16.7.1. [Tensorcore 5th Generation Instructions: `tcgen05.alloc`, `tcgen05.dealloc`, `tcgen05.relinquish_alloc_permit`](#tcgen05-instructions-tcgen05-alloc-dealloc-relinquish-alloc-permit)[](#tcgen05-instructions-tcgen05-alloc-dealloc-relinquish-alloc-permit "Permalink to this headline")

`tcgen05.alloc`, `tcgen05.dealloc`, `tcgen05.relinquish_alloc_permit`

Dynamic [Tensor Memory](#tensor-memory) allocation management instructions

Syntax

```
tcgen05.alloc.cta_group.sync.aligned{.shared::cta}.b32  [dst], nCols;



tcgen05.dealloc.cta_group.sync.aligned.b32              taddr, nCols;



tcgen05.relinquish_alloc_permit.cta_group.sync.aligned;



.cta_group = { .cta_group::1, .cta_group::2 }
```

Description

`tcgen05.alloc` is a potentially blocking instruction which dynamically allocates
the specified number of columns in the [Tensor Memory](#tensor-memory) and writes
the address of the allocated [Tensor Memory](#tensor-memory) into shared memory
at the location specified by address operand dst. The `tcgen05.alloc` blocks if the
requested amount of [Tensor Memory](#tensor-memory) is not available and unblocks
as soon as the requested amount of [Tensor Memory](#tensor-memory) becomes
available for allocation.

Instruction `tcgen05.dealloc` deallocates the [Tensor Memory](#tensor-memory)
specified by the [Tensor Memory](#tensor-memory) address `taddr`. The operand
`taddr` must point to a previous [Tensor Memory](#tensor-memory) allocation.

All of the Tensor Memory that was allocated using `tcgen05.alloc` instruction in a kernel,
must be explicitly deallocated using `tcgen05.dealloc` before the kernel exits.

The unsigned 32-bit operand `nCols` specify the number of columns to be allocated or
de-allocated. The unit of allocation and de-allocation is 32 columns and all of lanes
per column. The number of columns must be a power of 2. The operand `nCols` must be
within the range [32, 512]. The number of columns allocated should not increase between
any two allocations in the execution order within the CTA. Operand `nCols` must be
power of 2.

Instruction `tcgen05.relinquish_alloc_permit` specifies that the CTA of the executing
thread is relinquishing the right to allocate [Tensor Memory](#tensor-memory). So,
it is illegal for a CTA to perform `tcgen05.alloc` after any of its constituent threads
execute `tcgen05.relinquish_alloc_permit`.

If no state space is specified then [Generic Addressing](#generic-addressing) is used.
If the address specified by `dst` does not fall within the address window of
`.shared::cta` state space then the behavior is undefined.

Qualifier `.cta_group` specifies the number of CTAs involved in the allocation and
de-allocation operation. When `.cta_group::1` is specified, one warp from the CTA must
perform the allocation and de-allocation. When `.cta_group::2` is specified, one warp
from each of the [peer CTAs](#tcgen05-peer-cta) must collectively perform the allocation and
de-allocation. Refer to the [Issue Granularity](#tcgen05-issue-granularity) section.
When `.cta_group::2` is specified, the issuing warp must make sure that peer CTA is launched
and is still active.

All `tcgen05` instructions within a kernel must specify the same value for the `.cta_group`
qualifier.

The mandatory `.sync` qualifier indicates that the instruction causes the executing thread
to wait until all threads in the warp execute the same instruction before resuming execution.

The mandatory `.aligned` qualifier indicates that all threads in the warp must execute the
same instruction. In conditionally executed code, the instruction should only be used if it
is known that all threads in the warp evaluate the condition identically, otherwise behavior
is undefined.

The behavior of the instruction is undefined if all the threads in the warp do not use the
same values of `nCols`, or if any thread in the warp has exited.

The store operation in `tcgen05.alloc` is treated as a weak memory operation in the
[Memory Consistency Model](#memory-consistency-model).

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Examples

```
// Example 1:



tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sMemAddr1], 32;

ld.shared.b32 taddr, [sMemAddr1];

// use taddr ...

// more allocations and its usages ...

tcgen05.dealloc.cta_group::1.sync.aligned.b32  taddr, 32;

// more deallocations ...

tcgen05.relinquish_alloc_permit.cta_group::1.sync.aligned;



// Example 2:



// Following instructions are performed by current warp and the warp in the peer-CTA:

tcgen05.alloc.cta_group::2.sync.aligned.shared::cta.b32 [sMemAddr2], 32;

ld.shared.b32 taddr, [sMemAddr2];

// use taddr ...

// more allocations and its usages ...

tcgen05.dealloc.cta_group::2.sync.aligned.b32  taddr, 32;

// more deallocations ...

tcgen05.relinquish_alloc_permit.cta_group::2.sync.aligned;
```

#### 9.7.16.8. [Tensor Memory and Register Load/Store Instructions](#tcgen05-tensor-memory-ld-st)[](#tcgen05-tensor-memory-ld-st "Permalink to this headline")

The threads of the CTA can perform the loads and stores to the [Tensor Memory](#tensor-memory)
of the CTA and move data between registers and Tensor Memory. The loads and stores of data
can be performed in certain shapes as specified in the
[Matrix and Data Movement Shape](#tcgen05-matrix-data-movement-shape) section.

##### 9.7.16.8.1. [Access restrictions](#tcgen05-tensor-memory-ld-st-access-restrictions)[](#tcgen05-tensor-memory-ld-st-access-restrictions "Permalink to this headline")

Not all threads of the CTA can access the entire Tensor Memory via the `tcgen05.ld` and
`tcgen05.st` operations.

The Tensor Memory of a CTA is divided into 4 equal chunks such that each warp of a warpgroup
in the CTA can access a chunk of the Tensor Memory. All the columns of the Tensor Memory can
be accessed by all the four warps of a warpgroup. A lane of the Tensor Memory can be accessed
by a single warp in the warpgroup. The following table describes the access restriction.

| ID of the warp within the warpgroup | Accessible Lanes |
| --- | --- |
| 0 | 0-31 |
| 1 | 32-63 |
| 2 | 64-95 |
| 3 | 96-127 |

##### 9.7.16.8.2. [Packing and Unpacking](#tcgen05-tensor-memory-ld-st-packing-unpacking)[](#tcgen05-tensor-memory-ld-st-packing-unpacking "Permalink to this headline")

Optionally, the following pack and unpack operations can be performed during the load and store:

1. Packing: two 16-bit chunks can be packed into a single 32-bit chunk in the register in `tcgen05.ld`
2. Unpacking: a single 32-bit chunk in the register can be unpacked into two 16-bit chunks in `tcgen05.st`

as shown in the [Figure 193](#tcgen05-ld-st-pack-unpack).

![_images/tcgen05-ld-st-pack-unpack.png](_images/tcgen05-ld-st-pack-unpack.png)


Figure 193 Pack/Unpack operations for tcgen05 ld/st[](#tcgen05-ld-st-pack-unpack "Permalink to this image")

##### 9.7.16.8.3. [Tensorcore 5th Generation Instructions: `tcgen05.ld`](#tcgen05-instructions-tcgen05-ld)[](#tcgen05-instructions-tcgen05-ld "Permalink to this headline")

`tcgen05.ld`

Asynchronous collective load from tensor memory into registers.

Syntax

```
// Base load instruction:



tcgen05.ld.sync.aligned.shape1.num{.pack}.b32    r, [taddr];



tcgen05.ld.sync.aligned.shape2.num{.pack}.b32    r, [taddr], immHalfSplitoff;



.shape1 = { .16x64b, .16x128b, .16x256b, .32x32b }

.shape2 = { .16x32bx2 }

.num    = { .x1, .x2, .x4, .x8, .x16, .x32, .x64, .x128 }

.pack   = { .pack::16b }



// Floating point type load along with reduction :



tcgen05.ld.red.sync.aligned.shape3.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr];



tcgen05.ld.red.sync.aligned.shape4.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr], immHalfSplitoff;



// Integer type load along with reduction :



tcgen05.ld.red.sync.aligned.shape3.num.redOp.type r, redval, [taddr];



tcgen05.ld.red.sync.aligned.shape4.num.redOp.type r, redval, [taddr], immHalfSplitoff;



.shape3 = { .32x32b   }

.shape4 = { .16x32bx2 }

.redOp  = { .min, .max }

.type   = { .u32, .s32 }
```

Description

Instruction `tcgen05.ld` asynchronously loads data from the [Tensor Memory](#tensor-memory)
at the location specified by the 32-bit address operand `taddr` into the destination
register `r`, collectively across all threads of the warps.

All the threads in the warp must specify the same value of `taddr`, which must be the
base address of the collective load operation. Otherwise, the behavior is undefined.

The `.shape` qualifier and the `.num` qualifier together determines the total
dimension of the data which is loaded from the [Tensor Memory](#tensor-memory). The `.shape`
qualifier indicates the base dimension of data to be accessed as described in the
[Data Movement Shape](#tcgen05-data-movement-shape). The `.num` qualifier indicates
the repeat factor on the base dimension resulting in the total dimension of the data that
is accessed.

The shape `.16x32bx2` performs two accesses into Tensor Memory of the shape `.16x32b`.
The base address of the first access is specified by taddr and the base address of the
second access is specified by `taddr+immHalfSplitoff`, where `immHalfSplitoff` is an
immediate argument.

The destination operand `r` is a brace-enclosed vector expression consisting of one
or more 32-bit registers as per the value of `.shape` and `.num`. The size of the
vector for various combinations of `.num` and `.shape` is shown in
[Table 47](#tcgen05-num-shapes-ld).

Table 47 Various-combinations of .num and .shape[](#tcgen05-num-shapes-ld "Permalink to this table")






| .num | .shape | | |
| --- | --- | --- | --- |
| .16x32bx2 / .16x64b / .32x32b | .16x128b | .16x256b |
| `.x1` | 1 | 2 | 4 |
| `.x2` | 2 | 4 | 8 |
| `.x4` | 4 | 8 | 16 |
| `.x8` | 8 | 16 | 32 |
| `.x16` | 16 | 32 | 64 |
| `.x32` | 32 | 64 | 128 |
| `.x64` | 64 | 128 | NA |
| `.x128` | 128 | NA | NA |

The qualifier `.red` specifies that the reduction operation specified by `.redOp` is
performed on the data that is loaded across columns in each lane. The result of the
reduction operation is written into the corresponding thread’s 32-bit destination register
operand `redVal`. When `.red` qualifier is specified, `.num` modifier must be at least
`.x2`.

The optional qualifier `.pack::16b` can be used to pack two 16-bit elements from adjacent
columns into a single 32-bit element during the load as shown in the section
[Packing and Unpacking](#tcgen05-tensor-memory-ld-st-packing-unpacking).

The mandatory `.sync` qualifier indicates that `tcgen05.ld` causes the executing thread
to wait until all threads in the warp execute the same `tcgen05.ld` instruction before
resuming execution.

The mandatory `.aligned` qualifier indicates that all threads in the warp must execute the
same `tcgen05.ld` instruction. In conditionally executed code, a `tcgen05.ld` instruction
should only be used if it is known that all threads in the warp evaluate the condition
identically, otherwise behavior is undefined.

The behavior of `tcgen05.ld` is undefined if all threads do not use the same values of `taddr`,
or if any thread in the warp has exited.

The instruction `tcgen05.ld` is performed asynchronously and more details are specified in the
section [Memory Consistency Model for 5th generation of TensorCore operations](#tcgen05-memory-consistency-model).

PTX ISA Notes

Introduced in PTX ISA version 8.6.

`tcgen05.ld.red` is introduced in PTX ISA version 8.8.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

`tcgen05.ld.red` is supported on following architectures:

* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
  + `sm_103f` or higher in the same family
* `sm_110f` or higher in the same family

Examples

```
tcgen05.ld.sync.aligned.32x32b.x2.b32     {r0, r1}, [taddr1];



tcgen05.ld.sync.aligned.16x128b.x4.b32    {r0, r1, r2, r3, r4, r5, r6, r7}, [taddr2];



tcgen05.ld.red.sync.aligned.16x32bx2.x8.u32.max {r0, r1, r2, r3, r4, r5, r6, r7},

                                                 redVal, [taddr3], 16;
```

##### 9.7.16.8.4. [Tensorcore 5th Generation Instructions: `tcgen05.st`](#tcgen05-instructions-tcgen05-st)[](#tcgen05-instructions-tcgen05-st "Permalink to this headline")

`tcgen05.st`

Asynchronous collective store to tensor memory from registers.

Syntax

```
tcgen05.st.sync.aligned.shape1.num{.unpack}.b32    [taddr], r;



tcgen05.st.sync.aligned.shape2.num{.unpack}.b32    [taddr], immHalfSplitoff, r;



.shape1 = { .16x64b, .16x128b, .16x256b, .32x32b }

.shape2 = { .16x32bx2 }

.num    = { .x1, .x2, .x4, .x8, .x16, .x32, .x64, .x128 }

.unpack = { .unpack::16b }
```

Description

Instruction `tcgen05.st` asynchronously stores data from the source register `r` into
the [Tensor Memory](#tensor-memory) at the location specified by the 32-bit address operand `taddr`,
collectively across all threads of the warps.

All the threads in the warp must specify the same value of `taddr`, which must be the base
address of the collective store operation. Otherwise, the behavior is undefined.

The `.shape` qualifier and the `.num` qualifier together determines the total dimension
of the data which is stored to the Tensor Memory. The `.shape` qualifier indicates the base
dimension of data to be accessed as described in the
[Data Movement Shape](#tcgen05-data-movement-shape). The `.num`
qualifier indicates the repeat factor on the base dimension resulting in the total dimension of
the data that is accessed.

The shape `.16x32bx2` performs two accesses into Tensor Memory of the shape `.16x32b`.
The base address of the first access is specified by `taddr` and the base address of the
second access is specified by `taddr+immHalfSplitoff`, where `immHalfSplitoff` is an
immediate argument.

The source operand `r` is a brace-enclosed vector expression consisting of one or more 32-bit
registers as per the value of `.shape` and `.num`. The size of the vector for various
combinations of `.num` and `.shape` is shown in [Table 48](#tcgen05-num-shapes-st).

Table 48 Various-combinations of .num and .shape[](#tcgen05-num-shapes-st "Permalink to this table")






| .num | .shape | | |
| --- | --- | --- | --- |
| .16x32bx2 / .16x64b / .32x32b | .16x128b | .16x256b |
| `.x1` | 1 | 2 | 4 |
| `.x2` | 2 | 4 | 8 |
| `.x4` | 4 | 8 | 16 |
| `.x8` | 8 | 16 | 32 |
| `.x16` | 16 | 32 | 64 |
| `.x32` | 32 | 64 | 128 |
| `.x64` | 64 | 128 | NA |
| `.x128` | 128 | NA | NA |

The optional qualifier `.unpack::16b` can be used to unpack a 32-bit element in the
register into two 16-bit elements and store them in adjacent columns as shown in the
section [Packing and Unpacking](#tcgen05-tensor-memory-ld-st-packing-unpacking).

The mandatory `.sync` qualifier indicates that `tcgen05.st` causes the executing
thread to wait until all threads in the warp execute the same `tcgen05.st` instruction
before resuming execution.

The mandatory `.aligned` qualifier indicates that all threads in the warp must execute
the same `tcgen05.st` instruction. In conditionally executed code, a `tcgen05.st`
instruction should only be used if it is known that all threads in the warp evaluate
the condition identically, otherwise behavior is undefined.

The behavior of `tcgen05.st` is undefined if all threads do not use the same values of
`taddr`, or if any thread in the warp has exited.

The instruction `tcgen05.st` is performed asynchronously and more details are specified
in the section [Memory Consistency Model for 5th generation of TensorCore operations](#tcgen05-memory-consistency-model).

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Examples

```
tcgen05.st.sync.aligned.16x64b.x4.b32               [taddr0], {r0,  r1,  r2,  r3};



tcgen05.st.sync.aligned.16x128b.x1.unpack::16b.b32  [taddr1], {r0,  r1};
```

##### 9.7.16.8.5. [Tensorcore 5th Generation Instructions: `tcgen05.wait`](#tcgen05-instructions-tcgen05-wait)[](#tcgen05-instructions-tcgen05-wait "Permalink to this headline")

`tcgen05.wait`

Waits for the completion of all prior asynchronous `tcgen05.ld` / `tcgen05.st` instructions.

Syntax

```
tcgen05.wait_operation.sync.aligned;



.wait_operation = { .wait::ld, .wait::st }
```

Description

Instruction `tcgen05.wait::st` causes the executing thread to block until all prior
`tcgen05.st` operations issued by the executing thread have completed.

Instruction `tcgen05.wait::ld` causes the executing thread to block until all prior
`tcgen05.ld` operations issued by the executing thread have completed.

The mandatory `.sync` qualifier indicates that `tcgen05.wait_operation` causes the
executing thread to wait until all threads in the warp execute the same `tcgen05.wait_operation`
instruction before resuming execution.

The mandatory `.aligned` qualifier indicates that all threads in the warp must execute the
same `tcgen05.wait_operation` instruction.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Examples

```
Example 1:



tcgen05.ld.sync.aligned.32x32b.x2.b32     {r0, r1}, [taddr0];



// Prevents subsequent tcgen05.mma from racing ahead of the tcgen05.ld



tcgen05.wait::ld.sync.aligned;



tcgen05.mma.cta_group::1.kind::f16   [taddr0],  a-desc,  b-desc, idesc, p;



Example 2:



tcgen05.st.sync.aligned.32x32b.x2.b32     [taddr0], {r0, r1};



// Prevents the write to taddr0 in tcgen05.mma from racing ahead of the tcgen05.st



tcgen05.wait::st.sync.aligned;



tcgen05.mma.cta_group::1.kind::f16   [taddr0],  a-desc,  b-desc, idesc, p;
```

#### 9.7.16.9. [Tensor Memory Data Movement Instructions](#tcgen05-data-movement-instructions)[](#tcgen05-data-movement-instructions "Permalink to this headline")

Data from the shared memory can be copied asynchronously to the [Tensor Memory](#tensor-memory)
using the [Tensorcore 5th Generation Instructions: tcgen05.cp](#tcgen05-instructions-tcgen05-cp) operation.

##### 9.7.16.9.1. [Optional Decompression](#tcgen05-optional-decompression)[](#tcgen05-optional-decompression "Permalink to this headline")

Optionally, during the copy, a vector of 4-bit and 6-bit
custom floating point types can be decompressed into 8-bit types.

###### 9.7.16.9.1.1. [Decompression of 4-bit floating point to 8-bit type](#tcgen05-optional-decompression-4bit-8bit)[](#tcgen05-optional-decompression-4bit-8bit "Permalink to this headline")

A contiguous set of 16 elements of 4-bits each followed by 8 bytes of padding can be converted
into 16 elements of 8-bits each as shown in [Figure 194](#tcgen05-decompression-4b8b).

![_images/tcgen05-decompression-4b8b.png](_images/tcgen05-decompression-4b8b.png)


Figure 194 Decompression from 4-bit to 8-bit[](#tcgen05-decompression-4b8b "Permalink to this image")

The individual 4-bit to 8-bit decompression would look like as shown in [Figure 195](#tcgen05-decompression-4b8b-individual).

![_images/tcgen05-decompression-4b8b-individual.png](_images/tcgen05-decompression-4b8b-individual.png)


Figure 195 Individual decompression from 4-bit to 8-bit[](#tcgen05-decompression-4b8b-individual "Permalink to this image")

###### 9.7.16.9.1.2. [Decompression of 6-bit floating point to 8-bit type](#tcgen05-optional-decompression-6bit-8bit)[](#tcgen05-optional-decompression-6bit-8bit "Permalink to this headline")

A contiguous set of 16 elements of 6-bits each followed by 4 bytes of padding is
decompressed into 16 elements of 8-bits each as shown in [Figure 196](#tcgen05-decompression-6b8b).

![_images/tcgen05-decompression-6b8b.png](_images/tcgen05-decompression-6b8b.png)


Figure 196 Decompression from 6-bit to 8-bit[](#tcgen05-decompression-6b8b "Permalink to this image")

The individual 6-bit to 8-bit decompression for types `E3M2` and `E2M3` is shown in
[Figure 197](#tcgen05-decompression-6b8b-individual1) and [Figure 198](#tcgen05-decompression-6b8b-individual2)
respectively.

![_images/tcgen05-decompression-6b8b-individual1.png](_images/tcgen05-decompression-6b8b-individual1.png)


Figure 197 Individual decompression from 6-bit to 8-bit for E3M2 type[](#tcgen05-decompression-6b8b-individual1 "Permalink to this image")


![_images/tcgen05-decompression-6b8b-individual2.png](_images/tcgen05-decompression-6b8b-individual2.png)


Figure 198 Individual decompression from 6-bit to 8-bit for E2M3 type[](#tcgen05-decompression-6b8b-individual2 "Permalink to this image")

##### 9.7.16.9.2. [Tensorcore 5th Generation Instructions: `tcgen05.cp`](#tcgen05-instructions-tcgen05-cp)[](#tcgen05-instructions-tcgen05-cp "Permalink to this headline")

`tcgen05.cp`

Initiates an asynchronous copy operation from shared memory to the [Tensor Memory](#tensor-memory).

Syntax

```
tcgen05.cp.cta_group.shape{.multicast}{.dst_fmt.src_fmt} [taddr], s-desc;



.cta_group = { .cta_group::1, .cta_group::2 }

.src_fmt   = { .b6x16_p32 , .b4x16_p64 }

.dst_fmt   = { .b8x16 }

.shape     = { .128x256b, .4x256b, .128x128b, .64x128b**, .32x128b*** }

.multicast = { .warpx2::02_13** , .warpx2::01_23**, .warpx4*** }
```

Description

Instruction `tcgen05.cp` initiates an asynchronous copy operation from shared memory to the
location specified by the address operand `taddr` in the [Tensor Memory](#tensor-memory).

The 64-bit register operand `s-desc` is the matrix descriptor which represents the source
matrix in the shared memory that needs to be copied. The format of the matrix descriptor is
described in [Matrix Descriptors](#tcgen05-matrix-descriptors).

The `.shape` qualifier indicates the dimension of data to be copied as described in the
[Data Movement Shape](#tcgen05-data-movement-shape).

Qualifier `.cta_group` specifies the number of CTAs whose [Tensor Memory](#tensor-memory) is
accessed when a single thread of a single CTA executes the `tcgen05.cp` instruction.
When `.cta_group::1` is specified, the data is copied into the [Tensor Memory](#tensor-memory)
of the current CTA. When `.cta_group::2` is specified, the data is copied into the
[Tensor Memory](#tensor-memory) of both the current and the [peer CTAs](#tcgen05-peer-cta).

All `tcgen05` instructions within a kernel must specify the same value for the `.cta_group`
qualifier.

When the qualifiers `.dst_fmt` and `.src_fmt` are specified, the data is decompressed
from the source format `.src_fmt` in the shared memory to the destination format
`.dst_fmt` in [Tensor Memory](#tensor-memory) by the copy operation. The details of source
and the destination formats as specified in the section
[Optional Decompression](#tcgen05-optional-decompression).

Some of the `.shape` qualifiers require certain `.multicast` qualifiers.

1. `.64x128b` requires `.warpx2::02_13` or `.warpx2::01_23`
2. `.32x128b` requires `.warpx4`

When the `.multicast` qualifier is specified as either `.warpx2::02_13` or
`.warpx2::01_23` then the data being copied is multicasted into warp pairs and each
warp in the warp pair receive half of the data. Warp pairs are formed as follows:

1. `.warpx2::02_13` : warps 0 and 2 form a pair; warps 1 and 3 form a pair.
2. `.warpx2::01_23` : warps 0 and 1 form a pair; warps 2 and 3 form a pair.

When the `.multicast` modifier is specified as `.warpx4` then the data being
copied is multicasted into all 4 warps.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Examples

```
tcgen05.cp.cta_group::1.128x256b                 [taddr0], sdesc0;

tcgen05.cp.cta_group::2.128x128b.b8x16.b6x16_p32 [taddr1], sdesc1;

tcgen05.cp.cta_group::1.64x128b.warpx2::02_13    [taddr2], sdesc2;
```

##### 9.7.16.9.3. [Tensorcore 5th Generation Instructions: `tcgen05.shift`](#tcgen05-instructions-tcgen05-shift)[](#tcgen05-instructions-tcgen05-shift "Permalink to this headline")

`tcgen05.shift`

Asynchronously shift down the rows of the matrix in the [Tensor Memory](#tensor-memory) for a warp.

Syntax

```
tcgen05.shift.cta_group.down  [taddr];



.cta_group = { .cta_group::1, .cta_group::2 }
```

Description

Instruction `tcgen05.shift` is an asynchronous instruction which initiates the shifting of 32-byte
elements downwards across all the rows, except the last, by one row. The address operand `taddr`
specifies the base address of the matrix in the [Tensor Memory](#tensor-memory) whose rows must
be down shifted.

The lane of the address operand `taddr` must be aligned to 32.

Qualifier `.cta_group` specifies the number of CTAs whose [Tensor Memory](#tensor-memory)
is touched when a single thread of a single CTA executes the `tcgen05.shift` instruction.
When `.cta_group::1` is specified, the shift operation is performed in the
[Tensor Memory](#tensor-memory) of the current CTA. When `.cta_group::2` is specified,
the shift operation is performed in the [Tensor Memory](#tensor-memory) of both the current and the
[peer CTAs](#tcgen05-peer-cta).

All `tcgen05` instructions within a kernel must specify the same value for the `.cta_group`
qualifier.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_103a`
* `sm_110a`

Examples

```
tcgen05.shift.down.cta_group::1 [taddr0];

tcgen05.shift.down.cta_group::2 [taddr1];
```

#### 9.7.16.10. [TensorCore 5th Generation Matrix Multiply and accumulate Operations](#tcgen05-mma)[](#tcgen05-mma "Permalink to this headline")

The 5th generation of TensorCore operations of shape *MxNxK* perform matrix
multiplication and accumulation of the form:

`D = A*B+D`

where:

* the `A` matrix has shape *MxK*, in either Tensor Memory or Shared Memory
* the `B` matrix has shape *KxN*, in Shared Memory of the current CTA and optionally in peer CTA
* the `D` matrix is of the shape *MxN*, in Tensor Memory

Optionally an input predicate can be used to disable the input from the accumulator
matrix and the following operation can be performed as

`D = A*B`

The matrix multiplication and accumulation operations are categorized into various kinds
based on input types and the throughput of the multiplication operation. The following shows the
different kinds of MMA operations that are supported:

1. `f16` : supports `f16` and `bf16` input types.
2. `tf32` : supports `tf32` input types.
3. `f8f6f4` : supports all input combinations of `f8`, `f6` and `f4` types.
4. `i8` : supports signed and unsigned 8-bit integer input types.
5. `mxf8f6f4`/`mxf4` : supports mx-floating points input types.
6. `mxf4nvf4` : supports `mxf4` type and a custom NVIDIA floating-point
   type for inputs where the type of the vector elements is 4 bits and requires a common
   scaling factor to form the complete floating-point type, similar to other mx-types.

Optionally, the 5th generation of TensorCore MMAs support dense and sparse matrix `A`.
[Sparse Matrices](#tcgen05-sparse-matrices) describes the details of the sparse matrices.

Some of the MMA-kinds requires scaling of input matrices from memory to form the matrix
`A` and matrix `B` before performing the MMA operation.
[Block Scaling](#tcgen05-block-scaling) describes the details of the scaling of matrices.

The following table show the various matrices involved in the MMA operations and the memory in
which they can reside:

| Matrix Type | Memory |
| --- | --- |
| `A` | Tensor Memory OR Shared Memory |
| `B` | Shared Memory |
| `D` | Tensor Memory |
| `Sparse Meta Data` |
| `A-Scale` / `B-Scale` |

A sequence of MMA instructions may reuse the same `A` matrix with a sequence of `B`
matrices or may reuse the same `B` matrix with a sequence of `A` matrices.
In these patterns the TensorCore may be able to laod the unchanged matrix once and reuse
it through the sequence without multiple reloads. The `A` or `B` matrices are loaded
into a TensorCore collector buffer (i.e., special cache).

An MMA instruction has an optional `collector` qualifier to specify when an `A` or `B`
matrix is new to the sequence and should be loaded, unchanged within the sequence
and should be reused, or the last use in the sequence and should be discarded.
The `collector` qualifier is used to give the TensorCore permission to reuse a previously
loaded `A` or `B` matrix; however reuse is opportunistic in that the TensorCore may
reload a matrix even when it has permission to reuse that matrix. Thus, the source
memory of an `A` or `B` matrix must not be modified while the MMA instruction using those
matrices has not completed - regardless of `collector` qualifier permissions.

The 5th generation of TensorCore MMAs can be used for general matrix multiplication OR for
convolution operations. In case of convolutions, the activations can be stored in either
matrix `A` or matrix `B` while the weights will be stored in the other matrix.

| Activation Matrix | Weights Matrix | Name of the op | Instruction Name | Collector Buffer Applicability |
| --- | --- | --- | --- | --- |
| `A` | `B` | Activation Stationary | (default `tcgen05.mma`) | Collector buffer is applicable on matrix `A` |
| `B` | `A` | Weights Stationary | `.ws` | Collector buffer is applicable on matrix `B` |

##### 9.7.16.10.1. [Transpose and Negate operations](#tcgen05-transpose-and-negate-operations)[](#tcgen05-transpose-and-negate-operations "Permalink to this headline")

The matrices `A` and `B` can be transposed by specifying the Tranpose `A` Matrix
and Transpose `B` Matrix bits in the instruction descriptor respectively.

The elements of the matrices `A` and `B` can be negated by specifying the Negate
`A` Matrix and Negate `B` Matrix bits in the instruction descriptor respectively.

The support for Transpose and Negate operation for various MMA-Kind are shown in
[Table 49](#tcgen05-transpose-negate-mma-kind).

Table 49 Transpose and Negate operation for various MMA-Kind[](#tcgen05-transpose-negate-mma-kind "Permalink to this table")





| MMA-Kind | Is Transpose A/B supported | Is Negate A/B supported |
| --- | --- | --- |
| `.kind::tf32` | Yes | Yes |
| `.kind::f16` | Yes | Yes |
| `.kind::f8f6f4` | Yes | Yes |
| `.kind::mxf8f6f4` | Yes | Yes |
| `.kind::i8` | Yes | No |
| `.kind::mxf4` | No | Yes |
| `.kind::mxf4nvf4` | No | Yes |

For `.kind::tf32`, the transpose operations on matrices `A` and `B` are supported
only with 128B swizzling mode with 32B swizzle-atomicity.

For all other MMA-Kinds, the transpose operations on matrices `A` and `B` are not supported
on 128B swizzling mode with 32B swizzle-atomicity.

[Table 50](#tcgen05-kind-shapes-8b-transpose-b) shows the valid combinations of N shape with
`.cta_group` qualifier for 8bit transpose B.

Table 50 Various combinations of N shape with .cta\_group qualifier for 8bit transpose B[](#tcgen05-kind-shapes-8b-transpose-b "Permalink to this table")




| .cta\_group | N shape |
| --- | --- |
| 1 | 16 <= N <= 256, step 16 |
| 2 | 32 <= N <= 256, step 32 |

##### 9.7.16.10.2. [Matrix Layout Organization](#tcgen05-matrix-layout-organization)[](#tcgen05-matrix-layout-organization "Permalink to this headline")

[Table 51](#tcgen05-matrices-majorness) describes the major-ness used for different matrices.

Table 51 Major-ness for different matrices[](#tcgen05-matrices-majorness "Permalink to this table")





| Matrix | Residing in Memory | Default Major-ness |
| --- | --- | --- |
| D | Tensor Memory | Row-Major |
| A | Tensor Memory |
| Shared Memory | Depends on swizzling mode. Refer [Shared Memory Layout and Swizzling](#tcgen05-shared-memory-layout-swizzling) |
| B | Shared Memory |

##### 9.7.16.10.3. [Valid Combinations of Type-Size, Major-ness and Swizzling](#tcgen05-matrix-layout-organization-valid-comb-type-size-majorness-swizzle)[](#tcgen05-matrix-layout-organization-valid-comb-type-size-majorness-swizzle "Permalink to this headline")

Table 52 Valid Combinations of Type-Size, Major-ness and Swizzling[](#tcgen05-matrices-valid-type-size-majorness-swizzle "Permalink to this table")






| Type-Size | Major-ness | Matrix | Supported Swizzle |
| --- | --- | --- | --- |
| 4-bit, 6-bit, 8-bit, 16-bit, 32-bit | Row | A | All swizzling modes |
| Column | B |
| 8-bit  16-bit | Column (transpose) | A | All except 128B swizzling with 32B atomicity |
| Row (transpose) | B |
| 32-bit | Column (transpose) | A | Only 128B swizzling with 32B atomicity |
| Row (transpose) | B |

##### 9.7.16.10.4. [Packing formats of elements in Tensor and Shared memory](#tcgen05-packing-formats)[](#tcgen05-packing-formats "Permalink to this headline")

###### 9.7.16.10.4.1. [Packing format for matrix D in Tensor Memory](#tcgen05-packing-formats-mat-d)[](#tcgen05-packing-formats-mat-d "Permalink to this headline")

The sub-word elements of matrix `D` are expected not to be packed within a 32-bit Tensor Memory word.
For example, if the type of elements of the matrix `D` is 16 bits then a Tensor Memory word
would contain a single 16-bit element in its lower 16 bits.

###### 9.7.16.10.4.2. [Packing format for matrix A and B](#tcgen05-packing-formats-mat-a-b)[](#tcgen05-packing-formats-mat-a-b "Permalink to this headline")

The 6-bit and 4-bit floating point types have different packing format requirements for
different MMA kinds in both Tensor memory and Shared memory. The requirements are as follows.

###### 9.7.16.10.4.3. [Packing format used for matrix A by `.kind::mxf8f6f4` in Tensor Memory](#tcgen05-packing-formats-mxf8f6f4-tmem)[](#tcgen05-packing-formats-mxf8f6f4-tmem "Permalink to this headline")

The individual 4-bit and the 6-bit floating point type elements must be packed in an 8-bit container
in Tensor memory as shown below. The 8-bit containers must be contiguously packed in a 32-bit Tensor
Memory word. For example, if the type of elements of the matrix `A` is 6 bits then 4 consecutive
`A` elements should be packed in one 32-bit Tensor Memory word.

* 4-bit packing format as shown in [Figure 199](#tcgen05-packing-formats-mxf8f6f4-tmem-dig1)

  ![_images/tcgen05-packing-formats-mxf8f6f4-tmem-dig1.png](_images/tcgen05-packing-formats-mxf8f6f4-tmem-dig1.png)


  Figure 199 4-bit packing format with type E2M1[](#tcgen05-packing-formats-mxf8f6f4-tmem-dig1 "Permalink to this image")
* 6-bit packing format

  + Type E3M2 as shown in [Figure 200](#tcgen05-packing-formats-mxf8f6f4-tmem-dig2)

    ![_images/tcgen05-packing-formats-mxf8f6f4-tmem-dig2.png](_images/tcgen05-packing-formats-mxf8f6f4-tmem-dig2.png)


    Figure 200 6-bit packing format with type E3M2[](#tcgen05-packing-formats-mxf8f6f4-tmem-dig2 "Permalink to this image")
  + Type E2M3 as shown in [Figure 201](#tcgen05-packing-formats-mxf8f6f4-tmem-dig3)

    ![_images/tcgen05-packing-formats-mxf8f6f4-tmem-dig3.png](_images/tcgen05-packing-formats-mxf8f6f4-tmem-dig3.png)


    Figure 201 6-bit packing format with type E2M3[](#tcgen05-packing-formats-mxf8f6f4-tmem-dig3 "Permalink to this image")

###### 9.7.16.10.4.4. [Packing format used for matrix A and B by `.kind::mxf8f6f4` in Shared Memory](#tcgen05-packing-formats-mxf8f6f4-smem)[](#tcgen05-packing-formats-mxf8f6f4-smem "Permalink to this headline")

The 4-bit and 6-bit floating point elements in shared memory must be contiguously packed along
with padding as follows.

* 4-bit packing format as shown in [Figure 202](#tcgen05-packing-formats-mxf8f6f4-smem-dig1)

  ![_images/tcgen05-packing-formats-mxf8f6f4-smem-dig1.png](_images/tcgen05-packing-formats-mxf8f6f4-smem-dig1.png)


  Figure 202 4-bit packing format[](#tcgen05-packing-formats-mxf8f6f4-smem-dig1 "Permalink to this image")
* 6-bit packing format as shown in [Figure 203](#tcgen05-packing-formats-mxf8f6f4-smem-dig2)

> ![_images/tcgen05-packing-formats-mxf8f6f4-smem-dig2.png](_images/tcgen05-packing-formats-mxf8f6f4-smem-dig2.png)
>
>
> Figure 203 6-bit packing format[](#tcgen05-packing-formats-mxf8f6f4-smem-dig2 "Permalink to this image")

###### 9.7.16.10.4.5. [Packing format used for matrix A by `.kind::mxf4` and `.kind::mxf4nvf4` in Tensor Memory](#tcgen05-packing-formats-mxf4-tmem)[](#tcgen05-packing-formats-mxf4-tmem "Permalink to this headline")

Two 4-bit floating point type elements must be packed in an 8-bit container in Tensor memory as
shown in [Figure 204](#tcgen05-packing-formats-mxf4-tmem-dig1) for `mxf4`.

![_images/tcgen05-packing-formats-mxf4-tmem-dig1.png](_images/tcgen05-packing-formats-mxf4-tmem-dig1.png)


Figure 204 4-bit packing format with type E2M1[](#tcgen05-packing-formats-mxf4-tmem-dig1 "Permalink to this image")

###### 9.7.16.10.4.6. [Packing format used for matrix A and B by `.kind::mxf4` and `.kind::mxf4nvf4` in Shared Memory](#tcgen05-packing-formats-mxf4-smem)[](#tcgen05-packing-formats-mxf4-smem "Permalink to this headline")

The packing format for 4-bit floating point elements in shared memory is to pack two 4-bit
elements in a 8-bit container, with no padding.

##### 9.7.16.10.5. [Data Path Layout Organization](#tcgen05-data-path-layout-organization)[](#tcgen05-data-path-layout-organization "Permalink to this headline")

Different MMA variants access the tensor memory with different layout organization.
The following table lists the various layouts:

| M | cta\_group | A-Sparsity | Is .ws mode | Datapath organization | Layout ID | Tensor Memory Datapath Lane Alignment |
| --- | --- | --- | --- | --- | --- | --- |
| 32 | ::1 | Either | Yes | 1x4 | [Layout G](#tcgen05-data-path-layout-g) | 0 |
| 64 | ::1 | Either | Yes | 2x3 | [Layout E](#tcgen05-data-path-layout-e) | 0 |
| 64 | ::1 | Either | No | 4x1 (1/2 datapath utilized) | [Layout F](#tcgen05-data-path-layout-f) | 0 or 16 |
| 128 | ::1 | Either | Either | 4x1 | [Layout D](#tcgen05-data-path-layout-d) | 0 |
| 128 | ::2 | Dense | N/A | 2x2 | [Layout B](#tcgen05-data-path-layout-b) | 0 |
| 128 | ::2 | Sparse | N/A | 4x1 (1/2 datapath utilized) | [Layout C](#tcgen05-data-path-layout-c) | 0 or 16 |
| 256 | ::2 | Either | N/A | 4x1 | [Layout A](#tcgen05-data-path-layout-a) | 0 |

The layouts which utilize only half the datapath lanes, i.e.,
[Layout F](#tcgen05-data-path-layout-f) and
[Layout C](#tcgen05-data-path-layout-c), must use the same Tensor Memory
lane alignment across matrices `A`, `D` and the sparsity metadata matrix.

The following shows the warps that can access the Tensor Memory regions via
`tcgen05.ld` / `tcgen05.st` along with the addresses for various Tensor Memory Layouts.

###### 9.7.16.10.5.1. [Layout A (M = 256)](#tcgen05-data-path-layout-a)[](#tcgen05-data-path-layout-a "Permalink to this headline")

Layout organization for M = 256 is shown in [Figure 205](#tcgen05-data-path-layout-a1).

![_images/tcgen05-data-path-layout-a1.png](_images/tcgen05-data-path-layout-a1.png)


Figure 205 Layout organization for M = 256[](#tcgen05-data-path-layout-a1 "Permalink to this image")

Addresses for the above region to be used in `tcgen05.ld` / `tcgen05.st`
is shown in [Figure 206](#tcgen05-data-path-layout-a2)

![_images/tcgen05-data-path-layout-a2.png](_images/tcgen05-data-path-layout-a2.png)


Figure 206 Addresses to use in `tcgen05.ld` / `tcgen05.st`[](#tcgen05-data-path-layout-a2 "Permalink to this image")

###### 9.7.16.10.5.2. [Layout B (M = 128 + cta-group::2 + Dense A matrix)](#tcgen05-data-path-layout-b)[](#tcgen05-data-path-layout-b "Permalink to this headline")

Layout organization for M = 128 + .cta\_group::2 + Dense A matrix is shown in
[Figure 207](#tcgen05-data-path-layout-b1).

![_images/tcgen05-data-path-layout-b1.png](_images/tcgen05-data-path-layout-b1.png)


Figure 207 Layout organization for M = 128 + .cta\_group::2 + Dense A matrix[](#tcgen05-data-path-layout-b1 "Permalink to this image")

Addresses for the above region to be used in `tcgen05.ld` / `tcgen05.st`
is shown in [Figure 208](#tcgen05-data-path-layout-b2)

![_images/tcgen05-data-path-layout-b2.png](_images/tcgen05-data-path-layout-b2.png)


Figure 208 Addresses to use in `tcgen05.ld` / `tcgen05.st`[](#tcgen05-data-path-layout-b2 "Permalink to this image")

###### 9.7.16.10.5.3. [Layout C (M = 128 + cta-group::2 + Sparse A matrix)](#tcgen05-data-path-layout-c)[](#tcgen05-data-path-layout-c "Permalink to this headline")

Layout organization for M = 128 + .cta\_group::2 + Sparse A matrix is shown in
[Figure 209](#tcgen05-data-path-layout-c1).

![_images/tcgen05-data-path-layout-c1.png](_images/tcgen05-data-path-layout-c1.png)


Figure 209 Layout organization for M = 128 + .cta\_group::2 + Sparse A matrix[](#tcgen05-data-path-layout-c1 "Permalink to this image")

Addresses for the above region to be used in `tcgen05.ld` / `tcgen05.st`
is shown in [Figure 210](#tcgen05-data-path-layout-c2)

![_images/tcgen05-data-path-layout-c2.png](_images/tcgen05-data-path-layout-c2.png)


Figure 210 Addresses to use in `tcgen05.ld` / `tcgen05.st`[](#tcgen05-data-path-layout-c2 "Permalink to this image")

###### 9.7.16.10.5.4. [Layout D (M = 128 + cta-group::1)](#tcgen05-data-path-layout-d)[](#tcgen05-data-path-layout-d "Permalink to this headline")

Layout organization for M = 128 + .cta\_group::1 is shown in
[Figure 211](#tcgen05-data-path-layout-d1).

![_images/tcgen05-data-path-layout-d1.png](_images/tcgen05-data-path-layout-d1.png)


Figure 211 Layout organization for M = 128 + .cta\_group::1[](#tcgen05-data-path-layout-d1 "Permalink to this image")

Addresses for the above region to be used in `tcgen05.ld` / `tcgen05.st`
is shown in [Figure 212](#tcgen05-data-path-layout-d2)

![_images/tcgen05-data-path-layout-d2.png](_images/tcgen05-data-path-layout-d2.png)


Figure 212 Addresses to use in `tcgen05.ld` / `tcgen05.st`[](#tcgen05-data-path-layout-d2 "Permalink to this image")

###### 9.7.16.10.5.5. [Layout E (M = 64 + .ws mode)](#tcgen05-data-path-layout-e)[](#tcgen05-data-path-layout-e "Permalink to this headline")

Layout organization for M = 64 + .ws mode is shown in
[Figure 213](#tcgen05-data-path-layout-e1).

![_images/tcgen05-data-path-layout-e1.png](_images/tcgen05-data-path-layout-e1.png)


Figure 213 Layout organization for M = 64 + .ws mode[](#tcgen05-data-path-layout-e1 "Permalink to this image")

Addresses for the above region to be used in `tcgen05.ld` / `tcgen05.st`
is shown in [Figure 214](#tcgen05-data-path-layout-e2)

![_images/tcgen05-data-path-layout-e2.png](_images/tcgen05-data-path-layout-e2.png)


Figure 214 Addresses to use in `tcgen05.ld` / `tcgen05.st`[](#tcgen05-data-path-layout-e2 "Permalink to this image")

###### 9.7.16.10.5.6. [Layout F (M = 64 + non .ws mode)](#tcgen05-data-path-layout-f)[](#tcgen05-data-path-layout-f "Permalink to this headline")

Layout organization for M = 64 + non .ws mode is shown in
[Figure 215](#tcgen05-data-path-layout-f1).

![_images/tcgen05-data-path-layout-f1.png](_images/tcgen05-data-path-layout-f1.png)


Figure 215 Layout organization for M = 64 + non .ws mode[](#tcgen05-data-path-layout-f1 "Permalink to this image")

Addresses for the above region to be used in `tcgen05.ld` / `tcgen05.st`
is shown in [Figure 216](#tcgen05-data-path-layout-f2)

![_images/tcgen05-data-path-layout-f2.png](_images/tcgen05-data-path-layout-f2.png)


Figure 216 Addresses to use in `tcgen05.ld` / `tcgen05.st`[](#tcgen05-data-path-layout-f2 "Permalink to this image")

###### 9.7.16.10.5.7. [Layout G (M = 32)](#tcgen05-data-path-layout-g)[](#tcgen05-data-path-layout-g "Permalink to this headline")

Layout organization for M = 32 is shown in
[Figure 217](#tcgen05-data-path-layout-g1).

![_images/tcgen05-data-path-layout-g1.png](_images/tcgen05-data-path-layout-g1.png)


Figure 217 Layout organization for M = 32[](#tcgen05-data-path-layout-g1 "Permalink to this image")

Addresses for the above region to be used in `tcgen05.ld` / `tcgen05.st`
is shown in [Figure 218](#tcgen05-data-path-layout-g2)

![_images/tcgen05-data-path-layout-g2.png](_images/tcgen05-data-path-layout-g2.png)


Figure 218 Addresses to use in `tcgen05.ld` / `tcgen05.st`[](#tcgen05-data-path-layout-g2 "Permalink to this image")

##### 9.7.16.10.6. [Shared Memory Layout and Swizzling](#tcgen05-shared-memory-layout-swizzling)[](#tcgen05-shared-memory-layout-swizzling "Permalink to this headline")

If the bit `Transpose A Matrix` / `Transpose B Matrix` in the
[Instruction descriptor](#tcgen05-instruction-descriptor) is 0, then *K-major* is
used for matrix `A` / `B` respectively. If the bit `Transpose A Matrix` in the
[Instruction descriptor](#tcgen05-instruction-descriptor) is 1 then *M-major* is
used for matrix `A`. If the bit `Transpose B Matrix` in the
[Instruction descriptor](#tcgen05-instruction-descriptor) is 1, then *N-major* is
used for matrix `B`.

In a column-major default BLAS library such as cuBLAS, the matrices `A` and `B` with and
without transpose can be classified as either *K-Major* or *M-or-N-Major* as shown in the
following table:

|  | Non-Transposed | Transposed |
| --- | --- | --- |
| A | K-major | M-major |
| B | K-major | N-major |

To avoid confusion with `A`, `B`, `row-major`, `col-major`, `transpose`, and
`non-transpose`, we will use *MN-Major* and *K-Major* throughout this section.

The matrices in the shared memory are made up of one or more “swizzle layout atom”.
The exact layout of these swizzle atoms depends on the swizzling mode, swizzle-atomicity,
and the leading dimension. The layout of the swizzle are shown in
[Table 53](#tcgen05-smem-swizzle-mode)

Table 53 Layout for swizzle atoms[](#tcgen05-smem-swizzle-mode "Permalink to this table")





| Swizzling mode and Swizzle-Atomicity | Leading Dimension | Swizzle atom layout (128b element) |
| --- | --- | --- |
| 128B Swizzling with 32B atomicity | M/N | 8x4 |
| – | – |
| 128B Swizzling with 16B atomicity | M/N | 8x8 |
| K | 8x8 |
| 64B Swizzling Mode | M/N | 4x8 |
| K | 8x4 |
| 32B Swizzling Mode | M/N | 2x8 |
| K | 8x2 |
| None | M/N | 1x8 |
| K | 8x1 |

The above shapes are for elements of size 128 bits. For smaller element sizes, the same shapes
would get multiplied along the leading dimension by a factor of `128 / sizeof_bits(Element)`.
For example, 128B MN major swizzle atom would have a shape of (8\*(128/32))x8 = 32x8 for
tf32 tensor core inputs.

Some example Layouts of *MxK* or *KxN* matrices with various swizzling modes, and are in units
of 128b elements as shown by each colored cell as shown in
[Figure 219](#tcgen05-smem-layout-128b-32b-atom-mn),
[Figure 220](#tcgen05-smem-layout-128b-mn),
[Figure 221](#tcgen05-smem-layout-128b-k),
[Figure 222](#tcgen05-smem-layout-64b-mn),
[Figure 223](#tcgen05-smem-layout-64b-k),
[Figure 224](#tcgen05-smem-layout-32b-mn),
[Figure 225](#tcgen05-smem-layout-32b-k),
[Figure 226](#tcgen05-smem-layout-no-swizzle-mn),
[Figure 227](#tcgen05-smem-layout-no-swizzle-k).

![_images/tcgen05-smem-layout-128B-32B-atom-mn.png](_images/tcgen05-smem-layout-128B-32B-atom-mn.png)


Figure 219 MN major 128B swizzling with 32B atomicity[](#tcgen05-smem-layout-128b-32b-atom-mn "Permalink to this image")


![_images/tcgen05-smem-layout-128B-mn.png](_images/tcgen05-smem-layout-128B-mn.png)


Figure 220 MN major 128B swizzling[](#tcgen05-smem-layout-128b-mn "Permalink to this image")


![_images/tcgen05-smem-layout-128B-k.png](_images/tcgen05-smem-layout-128B-k.png)


Figure 221 K major 128B swizzling[](#tcgen05-smem-layout-128b-k "Permalink to this image")


![_images/tcgen05-smem-layout-64B-mn.png](_images/tcgen05-smem-layout-64B-mn.png)


Figure 222 MN major 64B swizzling[](#tcgen05-smem-layout-64b-mn "Permalink to this image")


![_images/tcgen05-smem-layout-64B-k.png](_images/tcgen05-smem-layout-64B-k.png)


Figure 223 K major 64B swizzling[](#tcgen05-smem-layout-64b-k "Permalink to this image")


![_images/tcgen05-smem-layout-32B-mn.png](_images/tcgen05-smem-layout-32B-mn.png)


Figure 224 MN major 32B swizzling[](#tcgen05-smem-layout-32b-mn "Permalink to this image")


![_images/tcgen05-smem-layout-32B-k.png](_images/tcgen05-smem-layout-32B-k.png)


Figure 225 K major 32B swizzling[](#tcgen05-smem-layout-32b-k "Permalink to this image")


![_images/tcgen05-smem-layout-no-swizzle-mn.png](_images/tcgen05-smem-layout-no-swizzle-mn.png)


Figure 226 MN major no-swizzling mode[](#tcgen05-smem-layout-no-swizzle-mn "Permalink to this image")


![_images/tcgen05-smem-layout-no-swizzle-k.png](_images/tcgen05-smem-layout-no-swizzle-k.png)


Figure 227 K major no-swizzling mode[](#tcgen05-smem-layout-no-swizzle-k "Permalink to this image")

Following are some of the examples of the 128B swizzling layout for `tf32` element type.

* K-Major: [Figure 228](#tcgen05-smem-layout-k)

  > ![_images/tcgen05-smem-layout-k.png](_images/tcgen05-smem-layout-k.png)
  >
  >
  > Figure 228 K major[](#tcgen05-smem-layout-k "Permalink to this image")
* MN-Major: [Figure 229](#tcgen05-smem-layout-mn)

  > ![_images/tcgen05-smem-layout-mn.png](_images/tcgen05-smem-layout-mn.png)
  >
  >
  > Figure 229 MN major[](#tcgen05-smem-layout-mn "Permalink to this image")

##### 9.7.16.10.7. [Block Scaling](#tcgen05-block-scaling)[](#tcgen05-block-scaling "Permalink to this headline")

The `tcgen05.mma` instructions with the following `.kind` qualifier:

* `.kind::mxf8f6f4`
* `.kind::mxf4`
* `.kind::mxf4nvf4`

perform matrix multiplication with block scaling. This operation has the following form:

`(A * scale_A)  * (B * scale_B) + D`

where `scale_A` and `scale_B` are matrices residing in [Tensor Memory](#tensor-memory).

For a `scale_A` matrix of shape *M x SFA\_N*, each row of matrix `A` is divided into
*SFA\_N* number of chunks and each chunk of a row is multiplied with the corresponding
element in the *SF\_A* of the same row.

Similarly, for a `scale_B` matrix of shape *SFB\_M x N*, each column of matrix `B` is
divided into the *SFB\_M* number of chunks and each chunk of a column is multiplied with
the corresponding element in the *SF\_B* of the same column.

Scale factors for `A` and `B` matrices need to be duplicated to all 32 lane partitions
of tensor memory.

[Figure 230](#tcgen05-mma-block-scaling) shows an example of `tcgen05.mma` with block scaling of
`scale_vec::2X`.

![_images/tcgen05-mma-block-scaling.png](_images/tcgen05-mma-block-scaling.png)


Figure 230 `tcgen05.mma` with block scaling of `scale_vec::2X`[](#tcgen05-mma-block-scaling "Permalink to this image")

###### 9.7.16.10.7.1. [Valid combinations of scale\_vectorsize with types and MMA-Kind](#tcgen05-mma-scale-valid-vec-size)[](#tcgen05-mma-scale-valid-vec-size "Permalink to this headline")

The shape of *scale\_A* and *scale\_B* matrices depend on the `.scale_vectorsize` as shown in
[Table 54](#tcgen05-mma-scale-valid-comb).

Table 54 Valid combinations of scale\_vectorsize and shapes[](#tcgen05-mma-scale-valid-comb "Permalink to this table")







| .scale\_vectorsize | .kind::\* | K | Shape of scale\_A | Shape of scale\_B |
| --- | --- | --- | --- | --- |
| `.scale_vec::1X` | `.kind::mxf8f6f4` | All supported values of K | M x 1 | 1 x N |
| `.scale_vec::2X` | `.kind::mxf4`, `.kind::mxf4nvf4` | All supported values of K | M x 2 | 2 x N |
| `.scale_vec::4X` | `.kind::mxf4nvf4` | All supported values of K | M x 4 | 4 x N |
| `.block16` | `.kind::mxf4nvf4` | K = 96 | M x 6 | 6 x N |
| All supported values of K except 96 | M x 4 | 4 x N |
| `.block32` | `.kind::mxf4`, `.kind::mxf4nvf4` | K = 96 | M x 3 | 3 x N |
| All supported values of K except 96 | M x 2 | 2 x N |
| `.kind::mxf8f6f4` | All supported values of K | M x 1 | 1 x N |

The valid combination of the exact element types and the `.scale_vectorsize` are listed in
[Table 55](#tcgen05-mma-scale-valid-comb-detail).

Table 55 Valid combinations of scale\_vectorsize with types and MMA-Kind[](#tcgen05-mma-scale-valid-comb-detail "Permalink to this table")






| .kind::\* | Element Data Type | Scale Data Type | .scale\_vectorsize |
| --- | --- | --- | --- |
| `.kind::mxf8f6f4` | E4M3, E5M2, E2M3 E3M2, E2M1 | UE8M0 | `.scale_vec::1X` / `.block32` |
| `.kind::mxf4` | E2M1 | UE8M0 | `.scale_vec::2X` / `.block32` |
| `.kind::mxf4nvf4` | E2M1 | UE8M0 | `.scale_vec::2X` / `.block32`, `.scale_vec::4X` / `.block16` |
| E2M1 | UE4M3 | `.scale_vec::4X` / `.block16` |

New `.blockN` qualifiers are aliases for `.scale_vec::NX` qualifiers as:

* `.block32` is alias for `.scale_vec::1X` or `.scale_vec::2X`
  based on `.kind` and K dimension
* `.block16` is alias for `.scale_vec::4X`

###### 9.7.16.10.7.2. [Scale Factor A ID](#tcgen05-mma-scale-factor-a)[](#tcgen05-mma-scale-factor-a "Permalink to this headline")

The value of the scale factor `A ID` selects the sub-columns in the Tensor Memory to
form the scale factor `A` matrix, which is used to scale the matrix `A`.

The following shows the scale factor matrix layout for various scale vector sizes:

###### 9.7.16.10.7.2.1. [Layout of the Scale Factor A Matrix for scale\_vec::1X/block32 with K=32/K=64](#tcgen05-mma-scale-factor-a-layout-1x)[](#tcgen05-mma-scale-factor-a-layout-1x "Permalink to this headline")

There is one scale factor per row of the `A` matrix with block size as 32 and the scale factor must be provided in
1-byte aligned sub-column of the Tensor Memory. *SFA\_ID* specifies the byte offset in the
Tensor Memory word that must be used for the scale factor matrix.
[Figure 231](#tcgen05-mma-scale-factor-a-1x-dig) shows which sub-columns get selected for
different values of *SFA\_ID*.

![_images/tcgen05-mma-scale-factor-a-1x-dig.png](_images/tcgen05-mma-scale-factor-a-1x-dig.png)


Figure 231 Layout of scale factor A matrix with scale\_vec::1X/block32 with K=32/K=64[](#tcgen05-mma-scale-factor-a-1x-dig "Permalink to this image")

For example, if *SFA\_ID* is 0, then all the green columns are selected to form the scale factor
matrix. Similarly, *SFA\_ID* values of 1, 2 and 3 would select the blue, yellow, and red columns,
respectively.

###### 9.7.16.10.7.2.2. [Layout of the Scale Factor A Matrix for scale\_vec::2X/block32 with K=64/K=128](#tcgen05-mma-scale-factor-a-layout-2x)[](#tcgen05-mma-scale-factor-a-layout-2x "Permalink to this headline")

There are two scale factors per row of the `A` matrix with block size as 32 and the scale factor must be provided in
2-byte aligned sub-column of the Tensor Memory. *SFA\_ID* specifies the half word offset in the
Tensor Memory word that must be used for the scale factor matrix.
[Figure 232](#tcgen05-mma-scale-factor-a-2x-dig) shows which sub-columns gets selected for different
values of *SFA\_ID*.

![_images/tcgen05-mma-scale-factor-a-2x-dig.png](_images/tcgen05-mma-scale-factor-a-2x-dig.png)


Figure 232 Layout of scale factor A matrix with scale\_vec::2X/block32 with K=64/K=128[](#tcgen05-mma-scale-factor-a-2x-dig "Permalink to this image")

For example, if *SFA\_ID* is 0, then all the green columns are selected to form the scale factor
matrix. Similarly, if *SFA\_ID* is 2, then all of the blue columns are selected to form the scale
factor matrix.

###### 9.7.16.10.7.2.3. [Layout of the Scale Factor A Matrix for scale\_vec::4X/block16 with K=64/K=128](#tcgen05-mma-scale-factor-a-layout-4x)[](#tcgen05-mma-scale-factor-a-layout-4x "Permalink to this headline")

There are four scale factors per row of the `A` matrix with block size as 16 and the scale factor must be provided in
4-byte aligned sub-column of the Tensor Memory. The *SFA\_ID* value must be 0 and this specifies
that all of the columns (in green) will be used for the scale factor matrix.
[Figure 233](#tcgen05-mma-scale-factor-a-4x-dig) shows which sub-columns gets selected for different
values of *SFA\_ID*.

![_images/tcgen05-mma-scale-factor-a-4x-dig.png](_images/tcgen05-mma-scale-factor-a-4x-dig.png)


Figure 233 Layout of scale factor A matrix with scale\_vec::4X/block16 with K=64/K=128[](#tcgen05-mma-scale-factor-a-4x-dig "Permalink to this image")

###### 9.7.16.10.7.2.4. [Layout of the Scale Factor A Matrix for block32 with K=96 (Semantically equivalent to scale\_vec::3X)](#tcgen05-mma-scale-factor-a-layout-block32-k96)[](#tcgen05-mma-scale-factor-a-layout-block32-k96 "Permalink to this headline")

There are three scale factors per row of the `A` matrix with block size as 32 and the scale
factor must be provided in 4-byte aligned sub-column of the Tensor Memory. *SFA\_ID* specifies
the byte offset in the Tensor Memory word that must be used for the scale factor matrix.
[Figure 234](#tcgen05-mma-scale-factor-a-block32-k96-dig1), [Figure 235](#tcgen05-mma-scale-factor-a-block32-k96-dig2),
[Figure 236](#tcgen05-mma-scale-factor-a-block32-k96-dig3) and [Figure 237](#tcgen05-mma-scale-factor-a-block32-k96-dig4)
show which sub-columns get selected for different values of *SFA\_ID*.

![_images/tcgen05-mma-scale-factor-a-block32-k96-dig1.png](_images/tcgen05-mma-scale-factor-a-block32-k96-dig1.png)


Figure 234 Layout of scale factor A matrix with block32 with K=96 with SFA\_ID=00[](#tcgen05-mma-scale-factor-a-block32-k96-dig1 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-a-block32-k96-dig2.png](_images/tcgen05-mma-scale-factor-a-block32-k96-dig2.png)


Figure 235 Layout of scale factor A matrix with block32 with K=96 with SFA\_ID=01[](#tcgen05-mma-scale-factor-a-block32-k96-dig2 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-a-block32-k96-dig3.png](_images/tcgen05-mma-scale-factor-a-block32-k96-dig3.png)


Figure 236 Layout of scale factor A matrix with block32 with K=96 with SFA\_ID=10[](#tcgen05-mma-scale-factor-a-block32-k96-dig3 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-a-block32-k96-dig4.png](_images/tcgen05-mma-scale-factor-a-block32-k96-dig4.png)


Figure 237 Layout of scale factor A matrix with block32 with K=96 with SFA\_ID=11[](#tcgen05-mma-scale-factor-a-block32-k96-dig4 "Permalink to this image")

For example, if *SFA\_ID* is 0, then all the green columns are selected to form the scale factor
matrix. Similarly, *SFA\_ID* values of 1, 2 and 3 would select the blue, yellow, and red columns,
respectively.

###### 9.7.16.10.7.2.5. [Layout of the Scale Factor A Matrix for block16 with K=96 (Semantically equivalent to scale\_vec::6X)](#tcgen05-mma-scale-factor-a-layout-block16-k96)[](#tcgen05-mma-scale-factor-a-layout-block16-k96 "Permalink to this headline")

There are six scale factors per row of the `A` matrix with block size as 16 and the scale
factor must be provided in 4-byte aligned sub-column of the Tensor Memory. *SFA\_ID* specifies
the byte offset in the Tensor Memory word that must be used for the scale factor matrix.
[Figure 238](#tcgen05-mma-scale-factor-a-block16-k96-dig1) and [Figure 239](#tcgen05-mma-scale-factor-a-block16-k96-dig2)
show which sub-columns get selected for different values of *SFA\_ID*.

![_images/tcgen05-mma-scale-factor-a-block16-k96-dig1.png](_images/tcgen05-mma-scale-factor-a-block16-k96-dig1.png)


Figure 238 Layout of scale factor A matrix with block16 with K=96 with SFA\_ID=00[](#tcgen05-mma-scale-factor-a-block16-k96-dig1 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-a-block16-k96-dig2.png](_images/tcgen05-mma-scale-factor-a-block16-k96-dig2.png)


Figure 239 Layout of scale factor A matrix with block16 with K=96 with SFA\_ID=10[](#tcgen05-mma-scale-factor-a-block16-k96-dig2 "Permalink to this image")

For example, if *SFA\_ID* is 0, then all the green columns are selected to form the scale factor
matrix. Similarly, if *SFA\_ID* is 2, then all of the blue columns are selected to form the scale
factor matrix.

###### 9.7.16.10.7.3. [Scale Factor B ID](#tcgen05-mma-scale-factor-b)[](#tcgen05-mma-scale-factor-b "Permalink to this headline")

The value of the scale factor `B ID` selects the sub-columns in the Tensor Memory to
form the scale factor `B` matrix, which is used to scale the matrix `B`.

The following shows the scale factor matrix layout for various scale vector sizes:

###### 9.7.16.10.7.3.1. [Layout of the Scale Factor B Matrix for scale\_vec::1X/block32 with K=32/K=64](#tcgen05-mma-scale-factor-b-layout-1x)[](#tcgen05-mma-scale-factor-b-layout-1x "Permalink to this headline")

There is one scale factor per row of the `B` matrix with block size as 32 and the scale factor must be provided in
1-byte aligned sub-column of the Tensor Memory. *SFB\_ID* specifies the byte offset in the
Tensor Memory word that must be used for the scale factor matrix.
[Figure 240](#tcgen05-mma-scale-factor-b-1x-dig) shows which sub-columns get selected for
different values of *SFB\_ID*.

![_images/tcgen05-mma-scale-factor-b-1x-dig.png](_images/tcgen05-mma-scale-factor-b-1x-dig.png)


Figure 240 Layout of scale factor B matrix with scale\_vec::1X/block32 with K=32/K=64[](#tcgen05-mma-scale-factor-b-1x-dig "Permalink to this image")

For example, if *SFB\_ID* is 0, then all the green columns are selected to form the scale factor
matrix. Similarly, *SFB\_ID* values of 1, 2 and 3 would select the blue, yellow, and red columns, respectively.

###### 9.7.16.10.7.3.2. [Layout of the Scale Factor B Matrix for scale\_vec::2X/block32 with K=64/K=128](#tcgen05-mma-scale-factor-b-layout-2x)[](#tcgen05-mma-scale-factor-b-layout-2x "Permalink to this headline")

There are two scale factors per row of the `B` matrix with block size as 32 and the scale factor must be provided in
2-byte aligned sub-column of the Tensor Memory. *SFB\_ID* specifies the half word offset in the
Tensor Memory word that must be used for the scale factor matrix.
[Figure 241](#tcgen05-mma-scale-factor-b-2x-dig) shows which sub-columns get selected for
different values of *SFB\_ID*.

![_images/tcgen05-mma-scale-factor-b-2x-dig.png](_images/tcgen05-mma-scale-factor-b-2x-dig.png)


Figure 241 Layout of scale factor B matrix with scale\_vec::2X/block32 with K=64/K=128[](#tcgen05-mma-scale-factor-b-2x-dig "Permalink to this image")

For example, if *SFB\_ID* is 0, then all the green columns are selected to form the scale factor
matrix. Similarly, if *SFB\_ID* is 2, then all of the blue columns are selected to form the scale
factor matrix.

###### 9.7.16.10.7.3.3. [Layout of the Scale Factor B Matrix for scale\_vec::4X/block16 with K=64/K=128](#tcgen05-mma-scale-factor-b-layout-4x)[](#tcgen05-mma-scale-factor-b-layout-4x "Permalink to this headline")

There are four scale factors per row of the `B` matrix with block size as 16 and the scale factor must be provided in
4-byte aligned sub-column of the Tensor Memory. The *SFB\_ID* value must be 0 and this specifies
that all of the columns (in green) will be used for the scale factor matrix.
[Figure 242](#tcgen05-mma-scale-factor-b-4x-dig) shows which sub-columns get selected for
different values of *SFB\_ID*.

![_images/tcgen05-mma-scale-factor-b-4x-dig.png](_images/tcgen05-mma-scale-factor-b-4x-dig.png)


Figure 242 Layout of scale factor B matrix with scale\_vec::4X/block16 with K=64/K=128[](#tcgen05-mma-scale-factor-b-4x-dig "Permalink to this image")

###### 9.7.16.10.7.3.4. [Layout of the Scale Factor B Matrix for block32 with K=96 (Semantically equivalent to scale\_vec::3X)](#tcgen05-mma-scale-factor-b-layout-block32-k96)[](#tcgen05-mma-scale-factor-b-layout-block32-k96 "Permalink to this headline")

There are three scale factors per row of the `B` matrix with block size as 32 and the scale factor
must be provided in 4-byte aligned sub-column of the Tensor Memory. *SFB\_ID* specifies the byte
offset in the Tensor Memory word that must be used for the scale factor matrix.

For N<=128, [Figure 243](#tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig1),
[Figure 244](#tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig2),
[Figure 245](#tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig3) and
[Figure 246](#tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig4) show which
sub-columns get selected for different values of *SFB\_ID*.

![_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig1.png](_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig1.png)


Figure 243 Layout of scale factor B matrix with block32 with K=96 and N<=128 with SFA\_ID=00[](#tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig1 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig2.png](_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig2.png)


Figure 244 Layout of scale factor B matrix with block32 with K=96 and N<=128 with SFA\_ID=01[](#tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig2 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig3.png](_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig3.png)


Figure 245 Layout of scale factor B matrix with block32 with K=96 and N<=128 with SFA\_ID=10[](#tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig3 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig4.png](_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig4.png)


Figure 246 Layout of scale factor B matrix with block32 with K=96 and N<=128 with SFA\_ID=11[](#tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig4 "Permalink to this image")

For N>128, [Figure 247](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig1),
[Figure 248](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig2),
[Figure 249](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig3),
[Figure 250](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig4),
[Figure 251](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig5) and
[Figure 252](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig6) show which
sub-columns get selected for different values of *SFB\_ID*.

![_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig1.png](_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig1.png)


Figure 247 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA\_ID=00[](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig1 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig2.png](_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig2.png)


Figure 248 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA\_ID=01[](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig2 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig3.png](_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig3.png)


Figure 249 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA\_ID=10[](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig3 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig4.png](_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig4.png)


Figure 250 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA\_ID=10[](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig4 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig5.png](_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig5.png)


Figure 251 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA\_ID=11[](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig5 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig6.png](_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig6.png)


Figure 252 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA\_ID=11[](#tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig6 "Permalink to this image")

For example, if *SFB\_ID* is 0, then all the green columns are selected to form the
scale factor matrix. Similarly, *SFB\_ID* values of 1, 2 and 3 would select the blue,
yellow, and red columns, respectively.

###### 9.7.16.10.7.3.5. [Layout of the Scale Factor B Matrix for block16 with K=96 (Semantically equivalent to scale\_vec::6X)](#tcgen05-mma-scale-factor-b-layout-block16-k96)[](#tcgen05-mma-scale-factor-b-layout-block16-k96 "Permalink to this headline")

There are six scale factors per row of the `B` matrix with block size as 16 and the scale factor
must be provided in 4-byte aligned sub-column of the Tensor Memory. *SFB\_ID* specifies the byte
offset in the Tensor Memory word that must be used for the scale factor matrix.

For N<=128, [Figure 253](#tcgen05-mma-scale-factor-b-block16-k96-nlt128-dig1) and
[Figure 254](#tcgen05-mma-scale-factor-b-block16-k96-nlt128-dig2) show which sub-columns
get selected for different values of *SFB\_ID*.

![_images/tcgen05-mma-scale-factor-b-block16-k96-nlt128-dig1.png](_images/tcgen05-mma-scale-factor-b-block16-k96-nlt128-dig1.png)


Figure 253 Layout of scale factor B matrix with block16 with K=96 and N<=128 with SFA\_ID=00[](#tcgen05-mma-scale-factor-b-block16-k96-nlt128-dig1 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block16-k96-nlt128-dig2.png](_images/tcgen05-mma-scale-factor-b-block16-k96-nlt128-dig2.png)


Figure 254 Layout of scale factor B matrix with block16 with K=96 and N<=128 with SFA\_ID=10[](#tcgen05-mma-scale-factor-b-block16-k96-nlt128-dig2 "Permalink to this image")

For N>128, [Figure 255](#tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig1),
[Figure 256](#tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig2),
[Figure 257](#tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig3) and
[Figure 258](#tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig4) show which sub-columns
get selected for different values of *SFB\_ID*.

![_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig1.png](_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig1.png)


Figure 255 Layout of scale factor B matrix with block16 with K=96 and N>128 with SFA\_ID=00[](#tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig1 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig2.png](_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig2.png)


Figure 256 Layout of scale factor B matrix with block16 with K=96 and N>128 with SFA\_ID=00[](#tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig2 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig3.png](_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig3.png)


Figure 257 Layout of scale factor B matrix with block16 with K=96 and N>128 with SFA\_ID=10[](#tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig3 "Permalink to this image")


![_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig4.png](_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig4.png)


Figure 258 Layout of scale factor B matrix with block16 with K=96 and N>128 with SFA\_ID=10[](#tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig4 "Permalink to this image")

For example, if *SFB\_ID* is 0, then all the green columns are selected to form the
scale factor matrix. Similarly, if *SFB\_ID* is 2, then all of the blue columns are
selected to form the scale factor matrix.

##### 9.7.16.10.8. [Sparse Matrices](#tcgen05-sparse-matrices)[](#tcgen05-sparse-matrices "Permalink to this headline")

This instruction `tcgen05.mma.sp` can be used when the matrix `A` is a structured
sparse matrix with 50% zeros in each row distributed as per its sparse granularity.

In a *MxNxK* sparse `tcgen05.mma.sp` operation, the matrix `A` of shape *MxK* is
stored in a packed form as *Mx(K/2)* in memory. For each *K-wide* row of matrix `A`,
50% of elements are zeros and the remaining *K/2* non-zero elements are stored in
memory. The metadata specifies the mapping of the *K/2* non-zero elements to the *K*
elements before performing the MMA operation.

Granularity of sparse matrix `A` is defined as the ratio of the number of non-zero
elements in a sub-chunk of the matrix row to the total number of elements in that
sub-chunk where the size of the sub-chunk is shape-specific. The following table lists
the granularity of different `tcgen05.mma.sp` variants:

| .kind of tcgen05.mma | Sparse Granularity |
| --- | --- |
| `.kind::tf32` | 1:2 |
| `.kind::f16` | 2:4 |
| `.kind::f8f6f4` |
| `.kind::mxf8f6f4` |
| `.kind::i8` |
| `.kind::mxf4` | 4:8 (in pairs) |

###### 9.7.16.10.8.1. [Sparse `tcgen05.mma.sp` with `.kind::tf32`](#tcgen05-sparse-matrices-kind-tf32)[](#tcgen05-sparse-matrices-kind-tf32 "Permalink to this headline")

For `.kind::tf32`, matrix `A` is structured sparse at a granularity of `1:2`.
In other words, each chunk of two adjacent elements in a row of matrix `A` has one
zero and one non-zero element. Only the non-zero element is stored in memory and the
4-bit index in the metadata indicates the position of the non-zero element in the
two-wide chunk. The only meaningful values of the index are:

* `0b1110`
* `0b0100`

Rest of the values result in undefined behavior.

![_images/tcgen05-sparse-mma-metadata-tf32.png](_images/tcgen05-sparse-mma-metadata-tf32.png)


Figure 259 Sparse tcgen05.mma metadata example for tf32 kind[](#tcgen05-sparse-mma-metadata-tf32 "Permalink to this image")

###### 9.7.16.10.8.2. [Sparse `tcgen05.mma.sp` with `.kind::f16`, `.kind::f8f6f4`, `.kind::mxf8f6f4`, `.kind::i8`](#tcgen05-sparse-matrices-kind-f16-f8f8f4-mxf8f6f4)[](#tcgen05-sparse-matrices-kind-f16-f8f8f4-mxf8f6f4 "Permalink to this headline")

For the following `.kind` variants of `tcgen05.mma`:

* `.kind::f16`
* `.kind::f8f8f4`
* `.kind::mxf8f6f4`
* `.kind::i8`

matrix `A` is structured sparse at a granularity of `2:4`. In other words, each chunk
of four adjacent elements in a row of matrix `A` has two zero and two non-zero elements.
Only the non-zero elements are stored in memory and the two 2-bit indices in the metadata
indicates the position of the two non-zero elements in the four-wide chunk. The only
meaningful values of the index are:

* `0b0100`
* `0b1000`
* `0b1100`
* `0b1001`
* `0b1101`
* `0b0110`
* `0b1110`

![_images/tcgen05-sparse-mma-metadata-f16-f8f6f4-mxf8f6f4.png](_images/tcgen05-sparse-mma-metadata-f16-f8f6f4-mxf8f6f4.png)


Figure 260 Sparse tcgen05.mma metadata example for f16/f8f6f4/mxf8f6f4 kind[](#tcgen05-sparse-mma-metadata-f16-f8f6f4-mxf8f6f4 "Permalink to this image")

###### 9.7.16.10.8.3. [Sparse `tcgen05.mma.sp` with `.kind::mxf4` and `.kind::mxf4nvf4`](#tcgen05-sparse-matrices-kind-mxf4)[](#tcgen05-sparse-matrices-kind-mxf4 "Permalink to this headline")

For `.kind::mxf4` and `.kind::mxf4nvf4`, matrix `A` is pair-wise structured
sparse at a granularity of `4:8`. In other words, each chunk of eight adjacent
elements in a row of matrix `A` has four zero and four non-zero elements. The
zero and non-zero elements are clustered in sub-chunks of two elements each within
the eight-wide chunk, so each two-wide sub-chunk within the eight-wide chunk must be
all zeros or all non-zeros. Only the four non-zero elements are stored in memory and
the two 2-bit indices in the metadata indicates the position of the two two-wide
sub-chunks with non-zero values in the eight-wide chunk of a row of matrix `A`.
The only meaningful values of the index are:

* `0b0100`
* `0b1000`
* `0b1100`
* `0b1001`
* `0b1101`
* `0b0110`
* `0b1110`

Rest of the values result in undefined behavior.

![_images/tcgen05-sparse-mma-metadata-mxf4.png](_images/tcgen05-sparse-mma-metadata-mxf4.png)


Figure 261 Sparse tcgen05.mma metadata example for mxf4 kind[](#tcgen05-sparse-mma-metadata-mxf4 "Permalink to this image")

###### 9.7.16.10.8.4. [Sparsity selector](#tcgen05-sparse-matrices-sparsity-selector)[](#tcgen05-sparse-matrices-sparsity-selector "Permalink to this headline")

The value of the sparsity selector selects the sub-columns in the Tensor Memory
to form the sparsity metadata matrix, which is used with matrix `A` to form the
multiplicand matrix.

The following shows the sparse metadata matrix layout in Tensor Memory for various MMA variants:

###### 9.7.16.10.8.4.1. [Layout of the Sparsity Metadata Matrix for M = 64 for `.kind::f16`](#tcgen05-sparse-matrices-sparsity-selector-kind-f16-m64)[](#tcgen05-sparse-matrices-sparsity-selector-kind-f16-m64 "Permalink to this headline")

[Figure 262](#tcgen05-sparse-matrices-sparsity-selector-kind-f16-m64-dig) shows which sub-columns gets
selected for different values of Sparsity Selector.

![_images/tcgen05-sparse-matrices-sparsity-selector-kind-f16-m64.png](_images/tcgen05-sparse-matrices-sparsity-selector-kind-f16-m64.png)


Figure 262 Sparsity Metadata Layout for M = 64 for `.kind::f16`[](#tcgen05-sparse-matrices-sparsity-selector-kind-f16-m64-dig "Permalink to this image")

###### 9.7.16.10.8.4.2. [Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for `.kind::f16`](#tcgen05-sparse-matrices-sparsity-selector-kind-f16-m128-256)[](#tcgen05-sparse-matrices-sparsity-selector-kind-f16-m128-256 "Permalink to this headline")

[Figure 263](#tcgen05-sparse-matrices-sparsity-selector-kind-f16-m128-256-dig) shows which sub-columns gets
selected for different values of Sparsity Selector.

![_images/tcgen05-sparse-matrices-sparsity-selector-kind-f16-m128-256.png](_images/tcgen05-sparse-matrices-sparsity-selector-kind-f16-m128-256.png)


Figure 263 Sparsity Metadata Layout for M = 128 / M = 256 for `.kind::f16`[](#tcgen05-sparse-matrices-sparsity-selector-kind-f16-m128-256-dig "Permalink to this image")

###### 9.7.16.10.8.4.3. [Layout of the Sparsity Metadata Matrix for M = 64 for `.kind::tf32`](#tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m64)[](#tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m64 "Permalink to this headline")

[Figure 264](#tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m64-dig) shows which sub-columns gets
selected for different values of Sparsity Selector.

![_images/tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m64.png](_images/tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m64.png)


Figure 264 Sparsity Metadata Layout for M = 64 for `.kind::tf32`[](#tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m64-dig "Permalink to this image")

###### 9.7.16.10.8.4.4. [Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for `.kind::tf32`](#tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m128-256)[](#tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m128-256 "Permalink to this headline")

[Figure 265](#tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m128-256-dig) shows which sub-columns gets
selected for different values of Sparsity Selector.

![_images/tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m128-256.png](_images/tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m128-256.png)


Figure 265 Sparsity Metadata Layout for M = 128 / M = 256 for `.kind::tf32`[](#tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m128-256-dig "Permalink to this image")

###### 9.7.16.10.8.4.5. [Layout of the Sparsity Metadata Matrix for M = 64 for `.kind::f8f6f4`, `.kind::mxf8f6f4`, `.kind::i8`, `.kind::mxf4`, `.kind::mxf4nvf4`](#tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m64)[](#tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m64 "Permalink to this headline")

The value of the sparsity selector:

* must be 0 for `.kind::i8` and `.kind::f8f6f4`
* is assumed to be 0 for `.kind::mxf8f6f4`, `.kind::mxf4` and `.kind::mxf4nvf4`

and all of the columns are selected as
shown in [Figure 266](#tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m64-dig)

![_images/tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m64.png](_images/tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m64.png)


Figure 266 Sparsity Metadata Layout for M = 64 for `.kind::f8f6f4`, `.kind::mxf8f6f4`, `.kind::i8`, `.kind::mxf4`, `.kind::mxf4nvf4`[](#tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m64-dig "Permalink to this image")

###### 9.7.16.10.8.4.6. [Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for `.kind::f8f6f4`, `.kind::mxf8f6f4`, `.kind::i8`, `.kind::mxf4`, `.kind::mxf4nvf4`](#tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m128-256)[](#tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m128-256 "Permalink to this headline")

The value of the sparsity selector:

* must be 0 for `.kind::i8` and `.kind::f8f6f4`
* is assumed to be 0 for `.kind::mxf8f6f4`, `.kind::mxf4` and `.kind::mxf4nvf4`

and all of the columns are selected as
shown in [Figure 267](#tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m128-256-dig)

![_images/tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m128-256.png](_images/tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m128-256.png)


Figure 267 Sparsity Metadata Layout for M = 128 / M = 256 for `.kind::f8f6f4`, `.kind::mxf8f6f4`, `.kind::i8`, `.kind::mxf4`, `.kind::mxf4nvf4`[](#tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m128-256-dig "Permalink to this image")

###### 9.7.16.10.8.5. [Alignment restriction](#tcgen05-sparse-matrices-alignment-restriction)[](#tcgen05-sparse-matrices-alignment-restriction "Permalink to this headline")

The layouts which utilize only half the datapath lanes as specified in
[Data Path Layout Organization](#tcgen05-data-path-layout-organization),
i.e. [Layout F](#tcgen05-data-path-layout-f) and
[Layout C](#tcgen05-data-path-layout-c), must use the same alignment
across matrices A, D and the sparsity metadata matrix.

##### 9.7.16.10.9. [TensorCore 5th Generation of MMA Instructions](#tcgen05-mma-instructions)[](#tcgen05-mma-instructions "Permalink to this headline")

###### 9.7.16.10.9.1. [TensorCore 5th Generation Instructions: `tcgen05.mma`](#tcgen05-mma-instructions-mma)[](#tcgen05-mma-instructions-mma "Permalink to this headline")

`tcgen05.mma`

Perform the 5th generation of matrix multiply and accumulate operation.

Syntax

```
// 1. Floating-point type without block scaling:



tcgen05.mma.cta_group.kind   [d-tmem],  a-desc,  b-desc, idesc,

                             { disable-output-lane }, enable-input-d {, scale-input-d};



tcgen05.mma.cta_group.kind   [d-tmem], [a-tmem], b-desc, idesc,

                             { disable-output-lane }, enable-input-d {, scale-input-d};



.kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4 }

.cta_group = { .cta_group::1, .cta_group::2 }



----------------------------------------------------------------------------------



// 2. Floating-point type with block scaling:



tcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}

                                        [d-tmem],  a-desc,  b-desc, idesc,

                                        [scale-A-tmem], [scale-B-tmem], enable-input-d;



tcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}

                                        [d-tmem], [a-tmem], b-desc, idesc,

                                        [scale-A-tmem], [scale-B-tmem], enable-input-d;



.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }

.cta_group      = { .cta_group::1,   .cta_group::2 }

.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }



----------------------------------------------------------------------------------



// 3. Convolution MMA for floating-point type without block scaling:



tcgen05.mma.cta_group.kind.collector_usage [d-tmem],  a-desc,  b-desc, idesc,

                                           { disable-output-lane }, enable-input-d {, scale-input-d};



tcgen05.mma.cta_group.kind{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc, idesc,

                                                    { disable-output-lane }, enable-input-d {, scale-input-d};



tcgen05.mma.cta_group.kind.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,

                                                    { disable-output-lane }, enable-input-d {, scale-input-d};



.kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4 }

.cta_group = { .cta_group::1,   .cta_group::2 }

.collector_usage = { .collector::buffer::op }

::buffer         = { ::a }

::op             = { ::fill, ::use, ::lastuse, ::discard* }



----------------------------------------------------------------------------------



// 4. Activation Stationary MMA for floating-point type with block scaling:



tcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage

                                            [d-tmem],  a-desc,  b-desc, idesc,

                                            [scale-A-tmem], [scale-B-tmem], enable-input-d;



tcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage

                                            [d-tmem], [a-tmem], b-desc, idesc,

                                            [scale-A-tmem], [scale-B-tmem], enable-input-d;



.cta_group       = { .cta_group::1,   .cta_group::2 }

.scale_vectorsize  = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }

.kind            = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }

.collector_usage = { .collector::buffer::op }

::buffer         = { ::a }

::op             = { ::fill, ::use, ::lastuse, ::discard* }



----------------------------------------------------------------------------------



// 5. Integer type:



tcgen05.mma.cta_group.kind::i8  [d-tmem],  a-desc,  b-desc, idesc,

                                { disable-output-lane }, enable-input-d;



tcgen05.mma.cta_group.kind::i8  [d-tmem], [a-tmem], b-desc, idesc,

                                { disable-output-lane }, enable-input-d;



.cta_group = { .cta_group::1,   .cta_group::2  }



----------------------------------------------------------------------------------



// 6. Convolution MMA for integer type:



tcgen05.mma.cta_group.kind::i8.collector_usage          [d-tmem],  a-desc,  b-desc, idesc,

                                                        { disable-output-lane }, enable-input-d;



tcgen05.mma.cta_group.kind::i8.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,

                                                        { disable-output-lane }, enable-input-d;



tcgen05.mma.cta_group.kind::i8{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc, idesc,

                                                        { disable-output-lane }, enable-input-d;



.cta_group       = { .cta_group::1,   .cta_group::2  }

.collector_usage = { .collector::buffer::op }

::buffer         = { ::a }

::op             = { ::fill, ::use, ::lastuse, ::discard* }
```

Description

Instruction `tcgen05.mma` is an asynchronous instruction which initiates an *MxNxK* matrix
multiply and accumulate operation,
`D = A*B+D`
where the `A` matrix is *MxK*, the `B` matrix is *KxN*, and the `D` matrix is *MxN*.

The operation of the form
`D = A*B`
is issued when the input predicate argument `enable-input-d` is false.

The optional immediate argument `scale-input-d` can be specified to scale the input
matrix `D` as follows:
`D = A*B+D * (2 ^ - scale-input-d)`

The valid range of values for argument `scale-input-d` is [0, 15]. The argument
`scale-input-d` is only valid for `.kind::tf32` and `.kind::f16`.

The 32-bit register operand `idesc` is the instruction descriptor as described
in [Instruction descriptor](#tcgen05-instruction-descriptor), specifies
the shapes, exact types, sparsity and other details of the input matrices,
output matrix and the matrix multiply and accumulate operation.

The qualifier `.cta_group::1` specifies that the matrix multiply and
accumulate operation is performed on the [Tensor Memory](#tensor-memory) of the
executing thread’s CTA only. The qualifier `.cta_group::2` specifies that the matrix
multiply and accumulate operation is performed on the [Tensor Memory](#tensor-memory)
of the executing thread’s CTA and its [peer CTA](#tcgen05-peer-cta).

All `tcgen05` instructions within a kernel must specify the same value for the `.cta_group`
qualifier.

The instruction `tcgen05.mma` has single thread semantics, unlike the collective
instructions `mma.sync` or `wgmma.mma_async`. So, a single thread issuing the
`tcgen05.mma` will result in the initiation of the whole matrix multiply and
accumulate operation. Refer to the section [Issue Granularity](#tcgen05-issue-granularity).

The qualifier `.kind` specifies the general kind of the element types of the multiplicand
matrices. The exact types of the elements of the input and output matrices for each MMA-kind
are specified in the [Instruction descriptor](#tcgen05-instruction-descriptor).

The address operand `d-tmem` specifies the address of the destination and the accumulation
matrix `D` in the [Tensor Memory](#tensor-memory). The address operand `a-tmem`
specifies the address of the matrix `A` in the [Tensor Memory](#tensor-memory).
The 64-bit register operand `a-desc` and `b-desc` are the matrix descriptors which
represent the matrices `A` and `B` in shared memory respectively. The format of the
matrix descriptor is described in [Matrix Descriptors](#tcgen05-matrix-descriptors).

The vector operand `disable-output-lane` specifies the lane(s) in the
[Tensor Memory](#tensor-memory) that should be not be updated with the resultant
matrix `D`. Elements of the vector operand `disable-output-lane` forms a mask where
each bit corresponds to a lane of the [Tensor Memory](#tensor-memory), with least
significant bit of the first element of the vector (leftmost in syntax) corresponding
to the lane 0 of the [Tensor Memory](#tensor-memory). If a bit in the mask is 1,
then the corresponding lane in the Tensor Memory for the resultant matrix `D` will not
be updated. The size of the vector is as follows:

| .cta\_group | Size of the vector disable-output-lane |
| --- | --- |
| ::1 | 4 |
| ::2 | 8 |

Qualifier `.block_scale` specifies that the matrices `A` and `B` are scaled with
`scale_A` and `scale_B` matrices respectively before performing the matrix multiply
and accumulate operation as specified in the section [Block Scaling](#tcgen05-block-scaling).
The address operand `scale-A-tmem` and `scale-B-tmem` specify the base address the
matrices `scale_A` and `scale_B` respectively in the [Tensor Memory](#tensor-memory).

For qualifier `.scale_vectorsize`,

* If `.scale_vec::NX` is specified: N specifies the number of columns in `scale_A`
  matrix and number of rows in `scale_B` matrix.
* If `.blockN` is specified: N specifies the block size for which single scale factor
  will be applied. In this form, value of N is same as the K-dimension / (N of `.scale_vec::NX`).

Aliased `.scale_vectorsize` variants:

1. `.block16` is aliased with:

   1. `.scale_vec::4X` when `.kind = .kind::mxf4nvf4` and K = 64 or 128
2. `.block32` is aliased with:

   1. `.scale_vec::1X` when `.kind = .kind::mxf8f6f4` for all supported values of K
   2. `.scale_vec::2X` when `.kind = .kind::mxf4` or `.kind::mxf4nvf4` and K = 64 or 128

The valid combinations of MMA-kind and `.scale_vectorsize` are
described in [Table 54](#tcgen05-mma-scale-valid-comb). For `.kind::mxf4` when the qualifier
`.scale_vectorsize` is not specified, then it defaults to `.block32`. For `.kind::mxf4nvf4`,
the qualifier `.scale_vectorsize` must be explicitly specified.

The qualifier `.ashift` shifts the rows of the `A` matrix down by one row, except for
the last row in the [Tensor Memory](#tensor-memory). Qualifier `.ashift` is only allowed
with *M* = 128 or *M* = 256.

The qualifier `.collector_usage` specifies the usage of collector buffer for matrix `A`.
Following collector buffer operations can be specified:

| .collector\_usage | Semantics |
| --- | --- |
| `.collector::a::fill` | Specifies that the `A` matrix read from the memory should be filled in collector buffer. |
| `.collector::a::use` | Specifies that the `A` matrix can be read from the collector buffer. This requires a previous fill to the collector buffer to be still valid. |
| `.collector::a::lastuse` | Specifies that the `A` matrix can be read from the collector buffer and the contents of the collector buffer can be discarded. This requires a previous fill to the collector buffer to be valid till the collector buffer is read. |
| `.collector::a::discard` | Specifies that the contents of the collector buffer for `A` can be discarded. |

If no `.collector_usage` qualifier is specified, then it defaults to `.collector::a::discard`.
It is illegal to specify either of `.collector::a::use` or `.collector::a::fill` along with
`.ashift`.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Qualifier `.kind::mxf4nvf4` introduced in PTX ISA version 8.7.

Qualifiers `.block16` and `.block32` introduced in PTX ISA version 8.8.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8 except `.kind::i8`:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Qualifier `.kind::i8` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_110a`

Argument `scale-input-d` requires `sm_100a` and is supported on `sm_100f` or higher in the same family from PTX ISA version 8.8.

For `.scale_vectorsize`,

* `.scale_vec::1X`, `.scale_vec::2X`, `.scale_vec::4X` requires `sm_100a`.
* `.block16`, `.block32` requires `sm_100f` or `sm_110f`.

For Target ISA details on matrix shape, check [Target ISA Note](#tcgen05-matrix-shape-target-isa-note).

For Target ISA details on shared memory descriptor, check [Target ISA Note](#tcgen05-shared-memory-descriptor-target-isa-note).

Examples

```
tcgen05.mma.cta_group::1.kind::tf32      [taddr0],  adesc,  bdesc, idesc, {m0, m1, m2, m3}, p;

tcgen05.mma.cta_group::1.kind::mxf8f6f4  [taddr2],  [taddr1],  bdesc, idesc,

                                         [tmem_scaleA], [tmem_scaleB], p;



tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];



loop:

mbarrier.try_wait.parity.b64 p, [mbarObj0], 0;

@!p bra loop;
```

###### 9.7.16.10.9.2. [TensorCore 5th Generation Instructions: `tcgen05.mma.sp`](#tcgen05-mma-instructions-mma-sp)[](#tcgen05-mma-instructions-mma-sp "Permalink to this headline")

`tcgen05.mma.sp`

Perform the 5th generation of matrix multiply and accumulate operation with sparse `A` matrix.

Syntax

```
// 1. Floating-point type without block scaling:



tcgen05.mma.sp.cta_group.kind  [d-tmem],  a-desc,  b-desc, [sp-meta-tmem] ,  idesc,

                               { disable-output-lane }, enable-input-d{, scale-input-d};



tcgen05.mma.sp.cta_group.kind  [d-tmem], [a-tmem], b-desc, [sp-meta-tmem] , idesc,

                               { disable-output-lane }, enable-input-d{, scale-input-d};



.kind       = { .kind::f16, , .kind::tf32, .kind::f8f6f4 }

.cta_group  = { .cta_group::1,  .cta_group::2 }



----------------------------------------------------------------------------------



// 2. Floating-point type with block scaling:



tcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}

                                         [d-tmem],  a-desc,  b-desc , [sp-meta-tmem] , idesc,

                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;



tcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}

                                         [d-tmem], [a-tmem], b-desc , [sp-meta-tmem] , idesc,

                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;



.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }

.cta_group      = { .cta_group::1,  .cta_group::2 }

.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }



----------------------------------------------------------------------------------



// 3. Convolution MMA with floating-point type without block scaling:



tcgen05.mma.sp.cta_group.kind.collector_usage           [d-tmem],  a-desc,  b-desc,

                                                        [sp-meta-tmem] ,  idesc,

                                                        { disable-output-lane }, enable-input-d

                                                        {, scale-input-d};



tcgen05.mma.sp.cta_group.kind.ashift{.collector_usage}  [d-tmem], [a-tmem], b-desc,

                                                        [sp-meta-tmem] , idesc,

                                                        { disable-output-lane }, enable-input-d

                                                        {, scale-input-d};



tcgen05.mma.sp.cta_group.kind{.ashift}.collector_usage  [d-tmem], [a-tmem], b-desc,

                                                        [sp-meta-tmem] , idesc,

                                                        { disable-output-lane }, enable-input-d

                                                        {, scale-input-d};



.kind            = { .kind::f16, .kind::tf32, .kind::f8f6f4 }

.collector_usage = { .collector::buffer::op }

::buffer         = { ::a }

::op             = { ::fill, ::use, ::lastuse, ::discard* }



----------------------------------------------------------------------------------



// 4. Activation Stationary MMA with floating-point type with block scaling:



tcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage

                                         [d-tmem],  a-desc,  b-desc , [sp-meta-tmem] , idesc,

                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;



tcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage

                                         [d-tmem], [a-tmem], b-desc , [sp-meta-tmem] , idesc,

                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;



.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }

.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }

.collector_usage = { .collector::buffer::op }

::buffer         = { ::a }

::op             = { ::fill, ::use, ::lastuse, ::discard* }



----------------------------------------------------------------------------------



// 5. Integer type:



tcgen05.mma.sp.cta_group.kind::i8 [d-tmem],  a-desc,  b-desc, [sp-meta-tmem] , idesc,

                                  { disable-output-lane }, enable-input-d;



tcgen05.mma.sp.cta_group.kind::i8 [d-tmem], [a-tmem], b-desc, [sp-meta-tmem] , idesc,

                                  { disable-output-lane }, enable-input-d;



.cta_group      = { .cta_group::1,  .cta_group::2 }



----------------------------------------------------------------------------------



// 6. Convolution MMA with Integer type:



tcgen05.mma.sp.cta_group.kind::i8.collector_usage          [d-tmem],  a-desc,  b-desc,

                                                           [sp-meta-tmem] , idesc,

                                                           { disable-output-lane }, enable-input-d;



tcgen05.mma.sp.cta_group.kind::i8.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc,

                                                           [sp-meta-tmem], idesc ,

                                                           { disable-output-lane }, enable-input-d;



tcgen05.mma.sp.cta_group.kind::i8{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc,

                                                           [sp-meta-tmem], idesc ,

                                                           { disable-output-lane }, enable-input-d;



.collector_usage = { .collector::buffer::op }

::buffer         = { ::a }

::op             = { ::fill, ::use, ::lastuse, ::discard* }
```

Description

Instruction `tcgen05.mma.sp` is an asynchronous instruction which initiates an
*MxNxK* matrix multiply and accumulate operation of the form
`D = A*B+D`
where the `A` matrix is *Mx(K/2)*, the `B` matrix is *KxN*, and the `D` matrix is *MxN*.
[Sparse Matrices](#tcgen05-sparse-matrices) describes the details of the sparsity.

The operation of the form
`D = A*B`
is issued when the input predicate argument `enable-input-d` is false.

The optional immediate argument `scale-input-d` can be specified to scale the
input matrix `D` as follows:
`D = A*B+D * (2 ^ - scale-input-d)`

The valid range of values for argument `scale-input-d` is [0, 15]. The argument
`scale-input-d` is only valid for `.kind::tf32` and `.kind::f16`.

The 32-bit register operand `idesc` is the instruction descriptor as described in
[Instruction descriptor](#tcgen05-instruction-descriptor), specifies the shapes,
exact types, sparsity and other details of the input matrices, output matrix and the
matrix multiply and accumulate operation.

The qualifier `.cta_group::1` specifies that the matrix multiply and accumulate
operation is performed on the [Tensor Memory](#tensor-memory) of the executing
thread’s CTA only. The qualifier `.cta_group::2` specifies that the matrix
multiply and accumulate operation is performed on the [Tensor Memory](#tensor-memory)
of the executing thread’s CTA and its [peer CTA](#tcgen05-peer-cta).

All `tcgen05` instructions within a kernel must specify the same value for the `.cta_group`
qualifier.

The instruction `tcgen05.mma.sp` has single thread semantics, unlike the collective
instructions `mma.sync` or `wgmma.mma_async`. So, a single thread issuing the
`tcgen05.mma.sp` will result in the initiation of the whole matrix multiply and
accumulate operation. Refer to the section [Issue Granularity](#tcgen05-issue-granularity).

The qualifier `.kind` specifies the general kind of the element types of the multiplicand
matrices. The exact types of the elements of the input and output matrices for each MMA-kind
are specified in the [Instruction descriptor](#tcgen05-instruction-descriptor).

The address operand `d-tmem` specifies the address of the destination and the accumulation
matrix `D` in the [Tensor Memory](#tensor-memory). The address operand `a-tmem`
specifies the address of the matrix `A` in the [Tensor Memory](#tensor-memory). The
64-bit register operand `a-desc` and `b-desc` are the matrix descriptors which represent
the matrices `A` and `B` in shared memory respectively. The format of the matrix descriptor
is described in [Matrix Descriptors](#tcgen05-matrix-descriptors).

The vector operand `disable-output-lane` specifies the lane(s) in the [Tensor Memory](#tensor-memory)
that should be not be updated with the resultant matrix `D`. Elements of the vector operand
`disable-output-lane` forms a mask where each bit corresponds to a lane of the
[Tensor Memory](#tensor-memory). with least significant bit of the first element of
the vector (leftmost in syntax) corresponding to the lane 0 of the Tensor Memory. If a bit in
the mask is 1, then the corresponding lane in the Tensor Memory for the resultant matrix `D`
will not be updated. The size of the vector is as follows:

| .cta\_group | Size of the vector disable-output-lane |
| --- | --- |
| ::1 | 4 |
| ::2 | 8 |

Qualifier `.block_scale` specifies that the matrices `A` and `B` are scaled with
`scale_A` and `scale_B` matrices respectively before performing the matrix multiply
and accumulate operation as specified in the section [Block Scaling](#tcgen05-block-scaling).
The address operand `scale-A-tmem` and `scale-B-tmem` specify the base address the
matrices `scale_A` and `scale_B` respectively in the [Tensor Memory](#tensor-memory).

For qualifier `.scale_vectorsize`,

* If `.scale_vec::NX` is specified: N specifies the number of columns in `scale_A`
  matrix and number of rows in `scale_B` matrix.
* If `.blockN` is specified: N specifies the block size for which single scale factor
  will be applied. In this form, value of N is same as the K-dimension / (N of `.scale_vec::NX`).

Aliased `.scale_vectorsize` variants:

1. `.block16` is aliased with:

   1. `.scale_vec::4X` when `.kind = .kind::mxf4nvf4` and K = 64 or 128
2. `.block32` is aliased with:

   1. `.scale_vec::1X` when `.kind = .kind::mxf8f6f4` for all supported values of K
   2. `.scale_vec::2X` when `.kind = .kind::mxf4` or `.kind::mxf4nvf4` and K = 64 or 128

The valid combinations of MMA-kind and `.scale_vectorsize` are
described in [Table 54](#tcgen05-mma-scale-valid-comb). For `.kind::mxf4` when the qualifier
`.scale_vectorsize` is not specified, then it defaults to `.block32`. For `.kind::mxf4nvf4`,
the qualifier `.scale_vectorsize` must be explicitly specified.

The qualifier `.ashift` shifts the rows of the `A` matrix down by one row, except for
the last row in the [Tensor Memory](#tensor-memory). Qualifier `.ashift` is only allowed
with *M* = 128 or *M* = 256.

The qualifier `.collector_usage` specifies the usage of collector buffer for matrix `A`.
Following collector buffer operations can be specified:

| .collector\_usage | Semantics |
| --- | --- |
| `.collector::a::fill` | Specifies that the `A` matrix read from the memory should be filled in collector buffer. |
| `.collector::a::use` | Specifies that the `A` matrix can be read from the collector buffer. This requires a previous fill to the collector buffer to be still valid. |
| `.collector::a::lastuse` | Specifies that the `A` matrix can be read from the collector buffer and the contents of the collector buffer can be discarded. This requires a previous fill to the collector buffer to be valid till the collector buffer is read. |
| `.collector::a::discard` | Specifies that the contents of the collector buffer for `A` can be discarded. |

If no `.collector_usage` qualifier is specified, then it defaults to `.collector::a::discard`.
It is illegal to specify either of `.collector::a::use` or `.collector::a::fill` along with
`.ashift`.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Qualifier `.kind::mxf4nvf4` introduced in PTX ISA version 8.7.

Qualifiers `.block16` and `.block32` introduced in PTX ISA version 8.8.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8 except `.kind::i8`/`.kind::mxf4nvf4`/`.kind::mxf4`:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Qualifier `.kind::i8` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_110a`

Qualifiers `.kind::mxf4nvf4` and `.kind::mxf4` are supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_103a`
* `sm_110a`

Argument `scale-input-d` requires `sm_100a` and is supported on `sm_100f` or higher in the same family from PTX ISA version 8.8.

For `.scale_vectorsize`,

* `.scale_vec::1X`, `.scale_vec::2X`, `.scale_vec::4X` requires `sm_100a`.
* `.block16`, `.block32` requires `sm_100f` or `sm_110f`.

For Target ISA details on matrix shape, check [Target ISA Note](#tcgen05-matrix-shape-target-isa-note).

For Target ISA details on shared memory descriptor, check [Target ISA Note](#tcgen05-shared-memory-descriptor-target-isa-note).

Examples

```
tcgen05.mma.sp.cta_group::1.kind::f16      [taddr0],  adesc,  bdesc, [tmem_spmeta0], idesc, p;



tcgen05.mma.sp.cta_group::1.kind::mxf8f6f4.collector::a:fill

                                           [taddr2],  [taddr1],  bdesc, [tmem_spmeta1], idesc,

                                           [tmem_scaleA], [tmem_scaleB], p;



tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];



loop:

mbarrier.try_wait.parity.b64 p, [mbarObj0], 0;

@!p bra loop;
```

###### 9.7.16.10.9.3. [TensorCore 5th Generation Instructions: `tcgen05.mma.ws`](#tcgen05-mma-instructions-mma-ws)[](#tcgen05-mma-instructions-mma-ws "Permalink to this headline")

`tcgen05.mma.ws`

Perform the 5th generation of weight stationary convolution matrix multiply and accumulate
operation.

Syntax

```
// 1. Floating-point type without block scaling:



tcgen05.mma.ws.cta_group::1.kind{.collector_usage}    [d-tmem],  a-desc,  b-desc,  idesc,

                                                      enable-input-d {, zero-column-mask-desc };



tcgen05.mma.ws.cta_group::1.kind{.collector_usage}    [d-tmem], [a-tmem], b-desc, idesc,

                                                      enable-input-d {, zero-column-mask-desc };



.kind = { .kind::f16, .kind::tf32, .kind::f8f6f4 }



----------------------------------------------------------------------------------



// 2. Integer type:



tcgen05.mma.ws.cta_group::1.kind::i8{.collector_usage} [d-tmem],  a-desc,  b-desc, idesc,

                                                       enable-input-d {, zero-column-mask-desc};



tcgen05.mma.ws.cta_group::1.kind::i8{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,

                                                       enable-input-d {, zero-column-mask-desc};



.collector_usage = { .collector::buffer::op }

::buffer = { ::b0, ::b1, ::b2, ::b3 }

::op   = { ::fill, ::use, ::lastuse, ::discard}
```

Description

Instruction `tcgen05.mma.ws` is an asynchronous instruction which initiates an *MxNxK*
matrix multiply and accumulate operation,
`D = A*B+D`
where the `A` matrix is *MxK*, the `B` matrix is *KxN*, and the `D` matrix is *MxN*.

The operation of the form
`D = A*B`
is issued when the input predicate argument `enable-input-d` is false.

The 32-bit register operand `idesc` is the instruction descriptor as described in
[Instruction descriptor](#tcgen05-instruction-descriptor), specifies the shapes, exact
types, sparsity and other details of the input matrices, output matrix and the matrix
multiply and accumulate operation.

The qualifier `.cta_group::1` specifies that the matrix multiply and accumulate operation
is performed on the [Tensor Memory](#tensor-memory) of the executing thread’s CTA only.

All `tcgen05` instructions within a kernel must specify the same value for the `.cta_group`
qualifier.

The instruction `tcgen05.mma.ws` has single thread semantics, unlike the collective
instructions `mma.sync` or `wgmma.mma_async`. So, a single thread issuing the
`tcgen05.mma.ws` will result in the initiation of the whole matrix multiply and accumulate
operation. Refer to the section [Issue Granularity](#tcgen05-issue-granularity).

The qualifier `.kind` specifies the general kind of the element types of the multiplicand
matrices. The exact types of the elements of the input and output matrices for each MMA-kind
are specified in the [Instruction descriptor](#tcgen05-instruction-descriptor).

The address operand `d-tmem` specifies the address of the destination and the accumulation
matrix `D` in the [Tensor Memory](#tensor-memory). The address operand `a-tmem`
specifies the address of the matrix `A` in the [Tensor Memory](#tensor-memory). The
64-bit register operand `a-desc` and `b-desc` are the matrix descriptors which represent
the matrices `A` and `B` in shared memory respectively. The format of the matrix descriptor
is described in [Matrix Descriptors](#tcgen05-matrix-descriptors).

The optional operand `zero-column-mask-desc` is a 64-bit register which specifies the
[Zero-Column Mask Descriptor](#tcgen05-zero-column-mask-descriptor). The zero-column
mask descriptor is used to generate a mask that specifies which columns of `B` matrix
will have zero value for the matrix multiply and accumulate operation regardless of the
values present in the shared memory.

The qualifier `.collector_usage` specifies the usage of collector buffer for Matrix `B`.
Following collector buffer operations can be specified:

| .collector\_usage | Semantics |
| --- | --- |
| `.collector::bN::fill` | Specifies that the `B` matrix read from the memory should be filled in collector buffer #N. |
| `.collector::bN::use` | Specifies that the `B` matrix can be read from the collector buffer #N. This requires a previous fill to the collector buffer #N to be still valid. |
| `.collector::bN::lastuse` | Specifies that the `B` matrix can be read from the collector buffer #N after which the contents of the collector buffer #N can be discarded. This requires a previous fill to the collector buffer #N to be valid till the collector buffer #N is read. |
| `.collector::bN::discard` | Specifies that the contents of the collector buffer #N can be discarded. |

If no `.collector_usage` qualifier is specified, then it defaults to `.collector::b0::discard`.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8 except `.kind::i8`:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Qualifier `.kind::i8` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_110a`

Examples

```
tcgen05.mma.ws.cta_group::1.kind::i8.collector::b2:use [taddr2], [taddr1], bdesc, idesc, p;

tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];



loop:

mbarrier.try_wait.parity.b64 p, [mbarObj0], 0;

@!p bra loop;
```

###### 9.7.16.10.9.4. [TensorCore 5th Generation Instructions: `tcgen05.mma.ws.sp`](#tcgen05-mma-instructions-mma-ws-sp)[](#tcgen05-mma-instructions-mma-ws-sp "Permalink to this headline")

`tcgen05.mma.ws.sp`

Perform the 5th generation of weight stationary convolution matrix multiply and accumulate
operation with sparse `A` matrix.

Syntax

```
// 1. Floating-point type without block scaling:



tcgen05.mma.ws.sp.cta_group::1.kind{.collector_usage} [d-tmem],  a-desc,  b-desc,

                                                      [sp-meta-tmem] ,  idesc,

                                                      enable-input-d {, zero-column-mask-desc};



tcgen05.mma.ws.sp.cta_group::1.kind{.collector_usage} [d-tmem], [a-tmem], b-desc,

                                                      [sp-meta-tmem] , idesc,

                                                      enable-input-d {, zero-column-mask-desc};



.kind = { .kind::f16, .kind::tf32, .kind::f8f6f4 }



----------------------------------------------------------------------------------



// 2. Integer type:



tcgen05.mma.ws.sp.cta_group::1.kind::i8{.collector_usage} [d-tmem], a-desc, b-desc,

                                                          [sp-meta-tmem] , idesc,

                                                          enable-input-d {, zero-column-mask-desc};



tcgen05.mma.ws.sp.cta_group::1.kind::i8{.collector_usage} [d-tmem], [a-tmem], b-desc,

                                                          [sp-meta-tmem] , idesc,

                                                          enable-input-d {, zero-column-mask-desc};



.collector_usage = { .collector::buffer::op }

::buffer = { ::b0, ::b1, ::b2, ::b3 }

::op   = { ::fill, ::use, ::lastuse, ::discard}
```

Description

Instruction `tcgen05.mma.ws.sp` is an asynchronous instruction which initiates
an *MxNxK* matrix multiply and accumulate operation,
`D = A*B+D`
where the `A` matrix is *Mx(K/2)*, the `B` matrix is *KxN*, and the `D` matrix
is *MxN*. [Sparse Matrices](#tcgen05-sparse-matrices) describes the details of the
sparsity.

The operation of the form
`D = A*B`
is issued when the input predicate argument `enable-input-d` is false.

The 32-bit register operand `idesc` is the instruction descriptor as described in
[Instruction descriptor](#tcgen05-instruction-descriptor), specifies the shapes, exact
types, sparsity and other details of the input matrices, output matrix and the matrix
multiply and accumulate operation.

The qualifier `.cta_group::1` specifies that the matrix multiply and accumulate
operation is performed on the Tensor Memory of the executing thread’s CTA only.

All `tcgen05` instructions within a kernel must specify the same value for the `.cta_group`
qualifier.

The instruction `tcgen05.mma.ws.sp` has single thread semantics, unlike the collective
instructions `mma.sync` or `wgmma.mma_async`. So, a single thread issuing the
`tcgen05.mma.ws.sp` will result in the initiation of the whole matrix multiply and
accumulate operation. Refer to the section [Issue Granularity](#tcgen05-issue-granularity).

The qualifier `.kind` specifies the general kind of the element types of the multiplicand
matrices. The exact types of the elements of the input and output matrices for each MMA-kind are
specified in the [Instruction descriptor](#tcgen05-instruction-descriptor).

The address operand `d-tmem` specifies the address of the destination and the accumulation
matrix `D` in the [Tensor Memory](#tensor-memory). The address operand `a-tmem` specifies
the address of the matrix `A` in the [Tensor Memory](#tensor-memory). The 64-bit register
operand `a-desc` and `b-desc` are the matrix descriptors which represent the matrices `A`
and `B` in shared memory respectively. The format of the matrix descriptor is described in
[Matrix Descriptors](#tcgen05-matrix-descriptors).

The optional operand `zero-column-mask-desc` is a 64-bit register which specifies the
[Zero-Column Mask Descriptor](#tcgen05-zero-column-mask-descriptor). The zero-column
mask descriptor is used to generate a mask that specifies which columns of `B` matrix
will have zero value for the matrix multiply and accumulate operation regardless of the
values present in the shared memory.

The qualifier `.collector_usage` specifies the usage of collector buffer for Matrix `B`.
Following collector buffer operations can be specified:

| .collector\_usage | Semantics |
| --- | --- |
| `.collector::bN::fill` | Specifies that the `B` matrix read from the memory should be filled in collector buffer #N. |
| `.collector::bN::use` | Specifies that the `B` matrix can be read from the collector buffer #N. This requires a previous fill to the collector buffer #N to be still valid. |
| `.collector::bN::lastuse` | Specifies that the `B` matrix can be read from the collector buffer #N after which the contents of the collector buffer #N can be discarded. This requires a previous fill to the collector buffer #N to be valid till the collector buffer #N is read. |
| `.collector::bN::discard` | Specifies that the contents of the collector buffer #N can be discarded. |

If no `.collector_usage` qualifier is specified, then it defaults to `.collector::b0::discard`.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8 except `.kind::i8`:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Qualifier `.kind::i8` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_110a`

Examples

```
tcgen05.mma.ws.sp.cta_group::1.kind::tf32.collector::b1::fill  [taddr1], [taddr0], bdesc,

                                                               [tmem_spmeta0], idesc, p;



tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];



loop:

mbarrier.try_wait.parity.b64 p, [mbarObj0], 0;

@!p bra loop;
```

#### 9.7.16.11. [TensorCore 5th Generation Specialized Synchronization Operations](#tcgen05-special-sync-operations)[](#tcgen05-special-sync-operations "Permalink to this headline")

##### 9.7.16.11.1. [TensorCore 5th Generation Instructions: `tcgen05.fence`](#tcgen05-special-sync-operations-fence)[](#tcgen05-special-sync-operations-fence "Permalink to this headline")

`tcgen05.fence`

Specialized fence for the asynchronous tcgen05 operations.

Syntax

```
tcgen05.fence::before_thread_sync ;

tcgen05.fence::after_thread_sync  ;
```

Description

The instruction `tcgen05.fence::before_thread_sync` orders all the prior asynchronous
`tcgen05` operations with respect to the subsequent `tcgen05` and the execution
ordering operations.

The instruction `tcgen05.fence::after_thread_sync` orders all the subsequent asynchronous
`tcgen05` operations with respect to the prior `tcgen05` and the execution ordering
operations.

The `tcgen05.fence::*` instructions compose with execution ordering instructions across
a thread scope and provide ordering between `tcgen05` instructions across the same scope.

The `tcgen05.fence::before_thread_sync` instructions behave as code motion fence for prior
`tcgen05` instructions as they cannot be hoisted across. The `tcgen05.fence::after_thread_sync`
instructions behave as code motion fence for subsequent `tcgen05` instructions as they cannot
be hoisted across.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Examples

```
// Producer thread:



tcgen05.cp.cta_group::1.128x256b  [taddr0], sdesc0;



tcgen05.fence::before_thread_sync;

st.relaxed.b32 [flag], 1;



// Consumer thread:



loop:

ld.relaxed.b32 r, [flag];

setp.eq.u32 p, r, 1;

@!p bra loop;



tcgen05.fence::after_thread_sync;

tcgen05.mma.cta_group.kind   [taddr0], adesc, bdesc, idesc, p;
```

#### 9.7.16.12. [TensorCore 5th Generation Async Synchronization Operations](#tcgen-async-sync-operations)[](#tcgen-async-sync-operations "Permalink to this headline")

##### 9.7.16.12.1. [TensorCore 5th Generation Instructions: `tcgen05.commit`](#tcgen-async-sync-operations-commit)[](#tcgen-async-sync-operations-commit "Permalink to this headline")

`tcgen05.commit`

Makes the mbarrier object track the completion of all prior async-tcgen05 operations initiated
by the executing thread.

Syntax

```
tcgen05.commit.cta_group.completion_mechanism{.shared::cluster}{.multicast}.b64

                                                            [mbar] {, ctaMask};



.completion_mechanism = { .mbarrier::arrive::one }

.cta_group            = { .cta_group::1, .cta_group::2 }

.multicast            = { .multicast::cluster }
```

Description

The instruction `tcgen05.commit` is an asynchronous instruction which makes the mbarrier object,
specified by the address operand `mbar`, track the completion of all the prior asynchronous
`tcgen05` operations, as listed in
[mbarrier based completion mechanism](#tcgen05-memory-consistency-model-mbarrier-completion),
initiated by the executing thread. Upon the completion of the tracked asynchronous `tcgen05`
operations, the signal specified by the `.completion_mechanism` is triggered by the system
on the mbarrier object.

The instruction `tcgen05.commit.cta_group::1` tracks for the completion of all prior
asynchronous `tcgen05` operations with `.cta_group::1` issued by the current thread.
Similarly, the instruction `tcgen05.commit.cta_group::2` tracks for the completion of all
prior asynchronous `tcgen05` operations with `.cta_group::2` issued by the current thread.

All `tcgen05` instructions within a kernel must specify the same value for the `.cta_group`
qualifier.

The qualifier `.mbarrier::arrive::one` indicates that upon the completion of the prior
asynchronous `tcgen05` operation issued by the current thread, an arrive-on operation, with
the count argument of 1, is signaled on the mbarrier object. The scope of the arrive-on operation
is the cluster scope.

The optional qualifier `.multicast::cluster` allows signaling on the mbarrier objects of multiple
CTAs in the cluster. Operand `ctaMask` specifies the CTAs in the cluster such that each bit
position in the 16-bit `ctaMask` operand corresponds to the `%cluster_ctarank` of the destination
CTA. The mbarrier signal is multicast to the same offset as `mbar` in the shared memory of each
destination CTA.

If no state space is specified then [Generic Addressing](#generic-addressing) is used. If the
address specified by `mbar` does not fall within the address window of `.shared::cluster` state
space then the behavior is undefined.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Examples

```
Example 1:

tcgen05.cp.cta_group::1.128x256b                      [taddr0], sdesc0;

tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj1];



loop:

mbarrier.try_wait.parity.b64 p, [mbarObj1], 0;

@!p bra loop;



Example 2:

tcgen05.mma.cta_group::2.kind::tf32    [taddr0],  adesc,  bdesc, idesc, p;

tcgen05.commit.cta_group::2.mbarrier::arrive::one.b64 [mbarObj2];



loop:

mbarrier.try_wait.parity.b64 p, [mbarObj2], 0;

@!p bra loop;
```