# 3. PTX Machine Model 

3.1. [A Set of SIMT Multiprocessors](#set-of-simt-multiprocessors)[](#set-of-simt-multiprocessors "Permalink to this headline")
--------------------------------------------------------------------------------------------------------------------------------

The NVIDIA GPU architecture is built around a scalable array of multithreaded *Streaming
Multiprocessors (SMs)*. When a host program invokes a kernel grid, the blocks of the grid are
enumerated and distributed to multiprocessors with available execution capacity. The threads of a
thread block execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are
launched on the vacated multiprocessors.

A multiprocessor consists of multiple *Scalar Processor (SP)* cores, a multithreaded instruction
unit, and on-chip shared memory. The multiprocessor creates, manages, and executes concurrent
threads in hardware with zero scheduling overhead. It implements a single-instruction barrier
synchronization. Fast barrier synchronization together with lightweight thread creation and
zero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for
example, a low granularity decomposition of problems by assigning one thread to each data element
(such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation).

To manage hundreds of threads running several different programs, the multiprocessor employs an
architecture we call *SIMT (single-instruction, multiple-thread)*. The multiprocessor maps each
thread to one scalar processor core, and each scalar thread executes independently with its own
instruction address and register state. The multiprocessor SIMT unit creates, manages, schedules,
and executes threads in groups of parallel threads called *warps*. (This term originates from
weaving, the first parallel thread technology.) Individual threads composing a SIMT warp start
together at the same program address but are otherwise free to branch and execute independently.

When a multiprocessor is given one or more thread blocks to execute, it splits them into warps that
get scheduled by the SIMT unit. The way a block is split into warps is always the same; each warp
contains threads of consecutive, increasing thread IDs with the first warp containing thread 0.

At every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues
the next instruction to the active threads of the warp. A warp executes one common instruction at a
time, so full efficiency is realized when all threads of a warp agree on their execution path. If
threads of a warp diverge via a data-dependent conditional branch, the warp serially executes each
branch path taken, disabling threads that are not on that path, and when all paths complete, the
threads converge back to the same execution path. Branch divergence occurs only within a warp;
different warps execute independently regardless of whether they are executing common or disjointed
code paths.

SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a
single instruction controls multiple processing elements. A key difference is that SIMD vector
organizations expose the SIMD width to the software, whereas SIMT instructions specify the execution
and branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables
programmers to write thread-level parallel code for independent, scalar threads, as well as
data-parallel code for coordinated threads. For the purposes of correctness, the programmer can
essentially ignore the SIMT behavior; however, substantial performance improvements can be realized
by taking care that the code seldom requires threads in a warp to diverge. In practice, this is
analogous to the role of cache lines in traditional code: Cache line size can be safely ignored when
designing for correctness but must be considered in the code structure when designing for peak
performance. Vector architectures, on the other hand, require the software to coalesce loads into
vectors and manage divergence manually.

How many blocks a multiprocessor can process at once depends on how many registers per thread and
how much shared memory per block are required for a given kernel since the multiprocessor’s
registers and shared memory are split among all the threads of the batch of blocks. If there are not
enough registers or shared memory available per multiprocessor to process at least one block, the
kernel will fail to launch.

![_images/hardware-model.png](_images/hardware-model.png)


Figure 4 Hardware Model[](#set-of-simt-multiprocessors-hardware-model "Permalink to this image")

A set of SIMT multiprocessors with on-chip shared memory.

3.2. [Independent Thread Scheduling](#independent-thread-scheduling)[](#independent-thread-scheduling "Permalink to this headline")
------------------------------------------------------------------------------------------------------------------------------------

On architectures prior to Volta, warps used a single program counter shared amongst all 32 threads
in the warp together with an active mask specifying the active threads of the warp. As a result,
threads from the same warp in divergent regions or different states of execution cannot signal each
other or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or
mutexes can easily lead to deadlock, depending on which warp the contending threads come from.

Starting with the Volta architecture, *Independent Thread Scheduling* allows full concurrency
between threads, regardless of warp. With *Independent Thread Scheduling*, the GPU maintains
execution state per thread, including a program counter and call stack, and can yield execution at a
per-thread granularity, either to make better use of execution resources or to allow one thread to
wait for data to be produced by another. A schedule optimizer determines how to group active threads
from the same warp together into SIMT units. This retains the high throughput of SIMT execution as
in prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at
sub-warp granularity.

*Independent Thread Scheduling* can lead to a rather different set of threads participating in the
executed code than intended if the developer made assumptions about warp-synchronicity of previous
hardware architectures. In particular, any warp-synchronous code (such as synchronization-free,
intra-warp reductions) should be revisited to ensure compatibility with Volta and beyond. See the
section on Compute Capability 7.x in the *Cuda Programming Guide* for further details.

3.3. [On-chip Shared Memory](#on-chip-shared-memory)[](#on-chip-shared-memory "Permalink to this headline")
------------------------------------------------------------------------------------------------------------

As illustrated by [Figure 4](#set-of-simt-multiprocessors-hardware-model), each multiprocessor has
on-chip memory of the four following types:

* One set of local 32-bit *registers* per processor,
* A parallel data cache or *shared memory* that is shared by all scalar processor cores and is where
  the shared memory space resides,
* A read-only *constant cache* that is shared by all scalar processor cores and speeds up reads from
  the constant memory space, which is a read-only region of device memory,
* A read-only *texture cache* that is shared by all scalar processor cores and speeds up reads from
  the texture memory space, which is a read-only region of device memory; each multiprocessor
  accesses the texture cache via a *texture unit* that implements the various addressing modes and
  data filtering.

The local and global memory spaces are read-write regions of device memory.