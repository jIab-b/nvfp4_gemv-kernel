### 9.7.9. Data Movement and Conversion Instructions 

These instructions copy data from place to place, and from state space to state space, possibly
converting it from one format to another. `mov`, `ld`, `ldu`, and `st` operate on both
scalar and vector types. The `isspacep` instruction is provided to query whether a generic address
falls within a particular state space window. The `cvta` instruction converts addresses between
`generic` and `const`, `global`, `local`, or `shared` state spaces.

Instructions `ld`, `st`, `suld`, and `sust` support optional cache operations.

The Data Movement and Conversion Instructions are:

* `mov`
* `shfl.sync`
* `prmt`
* `ld`
* `ldu`
* `st`
* `st.async`
* `st.bulk`
* `multimem.ld_reduce`, `multimem.st`, `multimem.red`
* `prefetch`, `prefetchu`
* `isspacep`
* `cvta`
* `cvt`
* `cvt.pack`
* `cp.async`
* `cp.async.commit_group`
* `cp.async.wait_group`, `cp.async.wait_all`
* `cp.async.bulk`
* `cp.reduce.async.bulk`
* `cp.async.bulk.prefetch`
* `cp.async.bulk.tensor`
* `cp.reduce.async.bulk.tensor`
* `cp.async.bulk.prefetch.tensor`
* `cp.async.bulk.commit_group`
* `cp.async.bulk.wait_group`
* `tensormap.replace`

#### 9.7.9.1. [Cache Operators](#cache-operators)[](#cache-operators "Permalink to this headline")

PTX ISA version 2.0 introduced optional cache operators on load and store instructions. The cache
operators require a target architecture of `sm_20` or higher.

Cache operators on load or store instructions are treated as performance hints only. The use of a
cache operator on an `ld` or `st` instruction does not change the memory consistency behavior of
the program.

For `sm_20` and higher, the cache operators have the following definitions and behavior.

Table 30 Cache Operators for Memory Load Instructions[](#id675 "Permalink to this table")




| Operator | Meaning |
| --- | --- |
| `.ca` | Cache at all levels, likely to be accessed again.  The default load instruction cache operation is ld.ca, which allocates cache lines in all levels (L1 and L2) with normal eviction policy. Global data is coherent at the L2 level, but multiple L1 caches are not coherent for global data. If one thread stores to global memory via one L1 cache, and a second thread loads that address via a second L1 cache with `ld.ca`, the second thread may get stale L1 cache data, rather than the data stored by the first thread. The driver must invalidate global L1 cache lines between dependent grids of parallel threads. Stores by the first grid program are then correctly fetched by the second grid program issuing default `ld.ca` loads cached in L1. |
| `.cg` | Cache at global level (cache in L2 and below, not L1).  Use `ld.cg` to cache loads only globally, bypassing the L1 cache, and cache only in the L2 cache. |
| `.cs` | Cache streaming, likely to be accessed once.  The `ld.cs` load cached streaming operation allocates global lines with evict-first policy in L1 and L2 to limit cache pollution by temporary streaming data that may be accessed once or twice. When `ld.cs` is applied to a Local window address, it performs the `ld.lu` operation. |
| `.lu` | Last use.  The compiler/programmer may use `ld.lu` when restoring spilled registers and popping function stack frames to avoid needless write-backs of lines that will not be used again. The `ld.lu` instruction performs a load cached streaming operation (`ld.cs`) on global addresses. |
| `.cv` | Don’t cache and fetch again (consider cached system memory lines stale, fetch again).  The ld.cv load operation applied to a global System Memory address invalidates (discards) a matching L2 line and re-fetches the line on each new load. |

Table 31 Cache Operators for Memory Store Instructions[](#id676 "Permalink to this table")




| Operator | Meaning |
| --- | --- |
| `.wb` | Cache write-back all coherent levels.  The default store instruction cache operation is `st.wb`, which writes back cache lines of coherent cache levels with normal eviction policy.  If one thread stores to global memory, bypassing its L1 cache, and a second thread in a different SM later loads from that address via a different L1 cache with `ld.ca`, the second thread may get a hit on stale L1 cache data, rather than get the data from L2 or memory stored by the first thread.  The driver must invalidate global L1 cache lines between dependent grids of thread arrays. Stores by the first grid program are then correctly missed in L1 and fetched by the second grid program issuing default `ld.ca` loads. |
| `.cg` | Cache at global level (cache in L2 and below, not L1).  Use `st.cg` to cache global store data only globally, bypassing the L1 cache, and cache only in the L2 cache. |
| `.cs` | Cache streaming, likely to be accessed once.  The `st.cs` store cached-streaming operation allocates cache lines with evict-first policy to limit cache pollution by streaming output data. |
| `.wt` | Cache write-through (to system memory).  The `st.wt` store write-through operation applied to a global System Memory address writes through the L2 cache. |

#### 9.7.9.2. [Cache Eviction Priority Hints](#cache-eviction-priority-hints)[](#cache-eviction-priority-hints "Permalink to this headline")

PTX ISA version 7.4 adds optional cache eviction priority hints on load and store
instructions. Cache eviction priority requires target architecture `sm_70` or higher.

Cache eviction priority on load or store instructions is treated as a performance hint. It is
supported for `.global` state space and generic addresses where the address points to `.global`
state space.

Table 32 Cache Eviction Priority Hints for Memory Load and Store Instructions[](#id677 "Permalink to this table")




| Cache Eviction Priority | Meaning |
| --- | --- |
| `evict_normal` | Cache data with normal eviction priority. This is the default eviction priority. |
| `evict_first` | Data cached with this priority will be first in the eviction priority order and will likely be evicted when cache eviction is required. This priority is suitable for streaming data. |
| `evict_last` | Data cached with this priority will be last in the eviction priority order and will likely be evicted only after other data with `evict_normal` or `evict_first` eviction priotity is already evicted. This priority is suitable for data that should remain persistent in cache. |
| `evict_unchanged` | Do not change eviction priority order as part of this operation. |
| `no_allocate` | Do not allocate data to cache. This priority is suitable for streaming data. |

#### 9.7.9.3. [Data Movement and Conversion Instructions: `mov`](#data-movement-and-conversion-instructions-mov)[](#data-movement-and-conversion-instructions-mov "Permalink to this headline")

`mov`

Set a register variable with the value of a register variable or an immediate value. Take the
non-generic address of a variable in global, local, or shared state space.

Syntax

```
mov.type  d, a;

mov.type  d, sreg;

mov.type  d, avar;       // get address of variable

mov.type  d, avar+imm;   // get address of variable with offset

mov.u32   d, fname;      // get address of device function

mov.u64   d, fname;      // get address of device function

mov.u32   d, kernel;     // get address of entry function

mov.u64   d, kernel;     // get address of entry function



.type = { .pred,

          .b16, .b32, .b64,

          .u16, .u32, .u64,

          .s16, .s32, .s64,

                .f32, .f64 };
```

Description

Write register `d` with the value of `a`.

Operand `a` may be a register, special register, variable with optional offset in an addressable
memory space, or function name.

For variables declared in `.const`, `.global`, `.local`, and `.shared` state spaces, `mov`
places the non-generic address of the variable (i.e., the address of the variable in its state
space) into the destination register. The generic address of a variable in `const`, `global`,
`local`, or `shared` state space may be generated by first taking the address within the state
space with `mov` and then converting it to a generic address using the `cvta` instruction;
alternately, the generic address of a variable declared in `const`, `global`, `local`, or
`shared` state space may be taken directly using the `cvta` instruction.

Note that if the address of a device function parameter is moved to a register, the parameter will
be copied onto the stack and the address will be in the local state space.

Semantics

```
d = a;

d = sreg;

d = &avar;        // address is non-generic; i.e., within the variable's declared state space

d = &avar+imm;
```

Notes

* Although only predicate and bit-size types are required, we include the arithmetic types for the
  programmer’s convenience: their use enhances program readability and allows additional type
  checking.
* When moving address of a kernel or a device function, only `.u32` or `.u64` instruction types
  are allowed. However, if a signed type is used, it is not treated as a compilation error. The
  compiler issues a warning in this case.

PTX ISA Notes

Introduced in PTX ISA version 1.0.

Taking the address of kernel entry functions requires PTX ISA version 3.1 or later. Kernel function
addresses should only be used in the context of CUDA Dynamic Parallelism system calls. See the *CUDA
Dynamic Parallelism Programming Guide* for details.

Target ISA Notes

`mov.f64` requires `sm_13` or higher.

Taking the address of kernel entry functions requires `sm_35` or higher.

Examples

```
mov.f32  d,a;

mov.u16  u,v;

mov.f32  k,0.1;

mov.u32  ptr, A;        // move address of A into ptr

mov.u32  ptr, A[5];     // move address of A[5] into ptr

mov.u32  ptr, A+20;     // move address with offset into ptr

mov.u32  addr, myFunc;  // get address of device function 'myFunc'

mov.u64  kptr, main;    // get address of entry function 'main'
```

#### 9.7.9.4. [Data Movement and Conversion Instructions: `mov`](#data-movement-and-conversion-instructions-mov-2)[](#data-movement-and-conversion-instructions-mov-2 "Permalink to this headline")

`mov`

Move vector-to-scalar (pack) or scalar-to-vector (unpack).

Syntax

```
mov.type  d, a;



.type = { .b16, .b32, .b64, .b128 };
```

Description

Write scalar register `d` with the packed value of vector register `a`, or write vector register
`d` with the unpacked values from scalar register `a`.

When destination operand `d` is a vector register, the sink symbol `'_'` may be used for one or
more elements provided that at least one element is a scalar register.

For bit-size types, `mov` may be used to pack vector elements into a scalar register or unpack
sub-fields of a scalar register into a vector. Both the overall size of the vector and the size of
the scalar must match the size of the instruction type.

Semantics

```
// pack two 8-bit elements into .b16

d = a.x | (a.y << 8)

// pack four 8-bit elements into .b32

d = a.x | (a.y << 8)  | (a.z << 16) | (a.w << 24)

// pack two 16-bit elements into .b32

d = a.x | (a.y << 16)

// pack four 16-bit elements into .b64

d = a.x | (a.y << 16)  | (a.z << 32) | (a.w << 48)

// pack two 32-bit elements into .b64

d = a.x | (a.y << 32)

// pack four 32-bit elements into .b128

d = a.x | (a.y << 32)  | (a.z << 64) | (a.w << 96)

// pack two 64-bit elements into .b128

d = a.x | (a.y << 64)



// unpack 8-bit elements from .b16

{ d.x, d.y } = { a[0..7], a[8..15] }

// unpack 8-bit elements from .b32

{ d.x, d.y, d.z, d.w }

        { a[0..7], a[8..15], a[16..23], a[24..31] }



// unpack 16-bit elements from .b32

{ d.x, d.y }  = { a[0..15], a[16..31] }

// unpack 16-bit elements from .b64

{ d.x, d.y, d.z, d.w } =

        { a[0..15], a[16..31], a[32..47], a[48..63] }



// unpack 32-bit elements from .b64

{ d.x, d.y } = { a[0..31], a[32..63] }



// unpack 32-bit elements from .b128

{ d.x, d.y, d.z, d.w } =

        { a[0..31], a[32..63], a[64..95], a[96..127] }

// unpack 64-bit elements from .b128

{ d.x, d.y } = { a[0..63], a[64..127] }
```

PTX ISA Notes

Introduced in PTX ISA version 1.0.

Support for `.b128` type introduced in PTX ISA version 8.3.

Target ISA Notes

Supported on all target architectures.

Support for `.b128` type requires `sm_70` or higher.

Examples

```
mov.b32 %r1,{a,b};      // a,b have type .u16

mov.b64 {lo,hi}, %x;    // %x is a double; lo,hi are .u32

mov.b32 %r1,{x,y,z,w};  // x,y,z,w have type .b8

mov.b32 {r,g,b,a},%r1;  // r,g,b,a have type .u8

mov.b64 {%r1, _}, %x;   // %x is.b64, %r1 is .b32

mov.b128 {%b1, %b2}, %y;   // %y is.b128, %b1 and % b2 are .b64

mov.b128 %y, {%b1, %b2};   // %y is.b128, %b1 and % b2 are .b64
```

#### 9.7.9.5. [Data Movement and Conversion Instructions: `shfl` (deprecated)](#data-movement-and-conversion-instructions-shfl)[](#data-movement-and-conversion-instructions-shfl "Permalink to this headline")

`shfl` (deprecated)

Register data shuffle within threads of a warp.

Syntax

```
shfl.mode.b32  d[|p], a, b, c;



.mode = { .up, .down, .bfly, .idx };
```

Deprecation Note

The `shfl` instruction without a `.sync` qualifier is deprecated in PTX ISA version 6.0.

* Support for this instruction with `.target` lower than `sm_70` may be removed in a future PTX ISA version.

Removal Note

Support for `shfl` instruction without a `.sync` qualifier is removed in PTX ISA version 6.4 for `.target` `sm_70` or higher.

Description

Exchange register data between threads of a warp.

Each thread in the currently executing warp will compute a source lane index *j* based on input
operands `b` and `c` and the *mode*. If the computed source lane index *j* is in range, the
thread will copy the input operand `a` from lane *j* into its own destination register `d`;
otherwise, the thread will simply copy its own input `a` to destination `d`. The optional
destination predicate `p` is set to `True` if the computed source lane is in range, and
otherwise set to `False`.

Note that an out of range value of `b` may still result in a valid computed source lane index
*j*. In this case, a data transfer occurs and the destination predicate `p` is True.

Note that results are undefined in divergent control flow within a warp, if an active thread sources
a register from an inactive thread.

Operand `b` specifies a source lane or source lane offset, depending on the mode.

Operand `c` contains two packed values specifying a mask for logically splitting warps into
sub-segments and an upper bound for clamping the source lane index.

Semantics

```
lane[4:0]  = [Thread].laneid;  // position of thread in warp

bval[4:0] = b[4:0];            // source lane or lane offset (0..31)

cval[4:0] = c[4:0];            // clamp value

mask[4:0] = c[12:8];



// get value of source register a if thread is active and

// guard predicate true, else unpredictable

if (isActive(Thread) && isGuardPredicateTrue(Thread)) {

    SourceA[lane] = a;

} else {

    // Value of SourceA[lane] is unpredictable for

    // inactive/predicated-off threads in warp

}

maxLane = (lane[4:0] & mask[4:0]) | (cval[4:0] & ~mask[4:0]);

minLane = (lane[4:0] & mask[4:0]);



switch (.mode) {

    case .up:    j = lane - bval; pval = (j >= maxLane); break;

    case .down:  j = lane + bval; pval = (j <= maxLane); break;

    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;

    case .idx:   j = minLane  | (bval[4:0] & ~mask[4:0]);

                                 pval = (j <= maxLane); break;

}

if (!pval) j = lane;  // copy from own lane

d = SourceA[j];       // copy input a from lane j

if (dest predicate selected)

    p = pval;
```

PTX ISA Notes

Introduced in PTX ISA version 3.0.

Deprecated in PTX ISA version 6.0 in favor of `shfl.sync`.

Not supported in PTX ISA version 6.4 for .target `sm_70` or higher.

Target ISA Notes

`shfl` requires `sm_30` or higher.

`shfl` is not supported on `sm_70` or higher starting PTX ISA version 6.4.

Examples

```
    // Warp-level INCLUSIVE PLUS SCAN:

    //

    // Assumes input in following registers:

    //     - Rx  = sequence value for this thread

    //

    shfl.up.b32  Ry|p, Rx, 0x1,  0x0;

@p  add.f32      Rx, Ry, Rx;

    shfl.up.b32  Ry|p, Rx, 0x2,  0x0;

@p  add.f32      Rx, Ry, Rx;

    shfl.up.b32  Ry|p, Rx, 0x4,  0x0;

@p  add.f32      Rx, Ry, Rx;

    shfl.up.b32  Ry|p, Rx, 0x8,  0x0;

@p  add.f32      Rx, Ry, Rx;

    shfl.up.b32  Ry|p, Rx, 0x10, 0x0;

@p  add.f32      Rx, Ry, Rx;





    // Warp-level INCLUSIVE PLUS REVERSE-SCAN:

    //

    // Assumes input in following registers:

    //     - Rx  = sequence value for this thread

    //

    shfl.down.b32  Ry|p, Rx, 0x1,  0x1f;

@p  add.f32        Rx, Ry, Rx;

    shfl.down.b32  Ry|p, Rx, 0x2,  0x1f;

@p  add.f32        Rx, Ry, Rx;

    shfl.down.b32  Ry|p, Rx, 0x4,  0x1f;

@p  add.f32        Rx, Ry, Rx;

    shfl.down.b32  Ry|p, Rx, 0x8,  0x1f;

@p  add.f32        Rx, Ry, Rx;

    shfl.down.b32  Ry|p, Rx, 0x10, 0x1f;

@p  add.f32        Rx, Ry, Rx;





    // BUTTERFLY REDUCTION:

    //

    // Assumes input in following registers:

    //     - Rx  = sequence value for this thread

    //

    shfl.bfly.b32  Ry, Rx, 0x10, 0x1f;   // no predicate dest

    add.f32        Rx, Ry, Rx;

    shfl.bfly.b32  Ry, Rx, 0x8,  0x1f;

    add.f32        Rx, Ry, Rx;

    shfl.bfly.b32  Ry, Rx, 0x4,  0x1f;

    add.f32        Rx, Ry, Rx;

    shfl.bfly.b32  Ry, Rx, 0x2,  0x1f;

    add.f32        Rx, Ry, Rx;

    shfl.bfly.b32  Ry, Rx, 0x1,  0x1f;

    add.f32        Rx, Ry, Rx;

    //

    // All threads now hold sum in Rx
```

#### 9.7.9.6. [Data Movement and Conversion Instructions: `shfl.sync`](#data-movement-and-conversion-instructions-shfl-sync)[](#data-movement-and-conversion-instructions-shfl-sync "Permalink to this headline")

`shfl.sync`

Register data shuffle within threads of a warp.

Syntax

```
shfl.sync.mode.b32  d[|p], a, b, c, membermask;



.mode = { .up, .down, .bfly, .idx };
```

Description

Exchange register data between threads of a warp.

`shfl.sync` will cause executing thread to wait until all non-exited threads corresponding to
`membermask` have executed `shfl.sync` with the same qualifiers and same `membermask` value
before resuming execution.

Operand `membermask` specifies a 32-bit integer which is a mask indicating threads participating
in barrier where the bit position corresponds to thread’s `laneid`.

`shfl.sync` exchanges register data between threads in `membermask`.

Each thread in the currently executing warp will compute a source lane index *j* based on input
operands `b` and `c` and the *mode*. If the computed source lane index *j* is in range, the
thread will copy the input operand `a` from lane *j* into its own destination register `d`;
otherwise, the thread will simply copy its own input `a` to destination `d`. The optional
destination predicate `p` is set to `True` if the computed source lane is in range, and
otherwise set to `False`.

Note that an out of range value of `b` may still result in a valid computed source lane index
*j*. In this case, a data transfer occurs and the destination predicate `p` is True.

Note that results are undefined if a thread sources a register from an inactive thread or a thread
that is not in `membermask`.

Operand `b` specifies a source lane or source lane offset, depending on the mode.

Operand `c` contains two packed values specifying a mask for logically splitting warps into
sub-segments and an upper bound for clamping the source lane index.

The behavior of `shfl.sync` is undefined if the executing thread is not in the `membermask`.

Note

For .target `sm_6x` or below, all threads in `membermask` must execute the same `shfl.sync`
instruction in convergence, and only threads belonging to some `membermask` can be active when
the `shfl.sync` instruction is executed. Otherwise, the behavior is undefined.

Semantics

```
// wait for all threads in membermask to arrive

wait_for_specified_threads(membermask);



lane[4:0]  = [Thread].laneid;  // position of thread in warp

bval[4:0] = b[4:0];            // source lane or lane offset (0..31)

cval[4:0] = c[4:0];            // clamp value

segmask[4:0] = c[12:8];



// get value of source register a if thread is active and

// guard predicate true, else unpredictable

if (isActive(Thread) && isGuardPredicateTrue(Thread)) {

    SourceA[lane] = a;

} else {

    // Value of SourceA[lane] is unpredictable for

    // inactive/predicated-off threads in warp

}

maxLane = (lane[4:0] & segmask[4:0]) | (cval[4:0] & ~segmask[4:0]);

minLane = (lane[4:0] & segmask[4:0]);



switch (.mode) {

    case .up:    j = lane - bval; pval = (j >= maxLane); break;

    case .down:  j = lane + bval; pval = (j <= maxLane); break;

    case .bfly:  j = lane ^ bval; pval = (j <= maxLane); break;

    case .idx:   j = minLane  | (bval[4:0] & ~segmask[4:0]);

                                 pval = (j <= maxLane); break;

}

if (!pval) j = lane;  // copy from own lane

d = SourceA[j];       // copy input a from lane j

if (dest predicate selected)

    p = pval;
```

PTX ISA Notes

Introduced in PTX ISA version 6.0.

Target ISA Notes

Requires `sm_30` or higher.

Examples

```
shfl.sync.up.b32  Ry|p, Rx, 0x1,  0x0, 0xffffffff;
```

#### 9.7.9.7. [Data Movement and Conversion Instructions: `prmt`](#data-movement-and-conversion-instructions-prmt)[](#data-movement-and-conversion-instructions-prmt "Permalink to this headline")

`prmt`

Permute bytes from register pair.

Syntax

```
prmt.b32{.mode}  d, a, b, c;



.mode = { .f4e, .b4e, .rc8, .ecl, .ecr, .rc16 };
```

Description

Pick four arbitrary bytes from two 32-bit registers, and reassemble them into a 32-bit destination
register.

In the generic form (no mode specified), the permute control consists of four 4-bit selection
values. The bytes in the two source registers are numbered from 0 to 7: `{b, a} = {{b7, b6, b5,
b4}, {b3, b2, b1, b0}}`. For each byte in the target register, a 4-bit selection value is defined.

The 3 lsbs of the selection value specify which of the 8 source bytes should be moved into the
target position. The msb defines if the byte value should be copied, or if the sign (msb of the
byte) should be replicated over all 8 bits of the target position (sign extend of the byte value);
`msb=0` means copy the literal value; `msb=1` means replicate the sign. Note that the sign
extension is only performed as part of generic form.

Thus, the four 4-bit values fully specify an arbitrary byte permute, as a `16b` permute code.

| default mode | `d.b3`  source select | `d.b2`  source select | `d.b1`  source select | `d.b0`  source select |
| --- | --- | --- | --- | --- |
| index | `c[15:12]` | `c[11:8]` | `c[7:4]` | `c[3:0]` |

The more specialized form of the permute control uses the two lsb’s of operand `c` (which is
typically an address pointer) to control the byte extraction.

| mode | selector  `c[1:0]` | `d.b3`  source | `d.b2`  source | `d.b1`  source | `d.b0`  source |
| --- | --- | --- | --- | --- | --- |
| `f4e` (forward 4 extract) | 0 | 3 | 2 | 1 | 0 |
|  | 1 | 4 | 3 | 2 | 1 |
|  | 2 | 5 | 4 | 3 | 2 |
|  | 3 | 6 | 5 | 4 | 3 |
| `b4e` (backward 4 extract) | 0 | 5 | 6 | 7 | 0 |
|  | 1 | 6 | 7 | 0 | 1 |
|  | 2 | 7 | 0 | 1 | 2 |
|  | 3 | 0 | 1 | 2 | 3 |
| `rc8` (replicate 8) | 0 | 0 | 0 | 0 | 0 |
|  | 1 | 1 | 1 | 1 | 1 |
|  | 2 | 2 | 2 | 2 | 2 |
|  | 3 | 3 | 3 | 3 | 3 |
| `ecl` (edge clamp left) | 0 | 3 | 2 | 1 | 0 |
|  | 1 | 3 | 2 | 1 | 1 |
|  | 2 | 3 | 2 | 2 | 2 |
|  | 3 | 3 | 3 | 3 | 3 |
| `ecr` (edge clamp right) | 0 | 0 | 0 | 0 | 0 |
|  | 1 | 1 | 1 | 1 | 0 |
|  | 2 | 2 | 2 | 1 | 0 |
|  | 3 | 3 | 2 | 1 | 0 |
| `rc16` (replicate 16) | 0 | 1 | 0 | 1 | 0 |
|  | 1 | 3 | 2 | 3 | 2 |
|  | 2 | 1 | 0 | 1 | 0 |
|  | 3 | 3 | 2 | 3 | 2 |

Semantics

```
tmp64 = (b<<32) | a;  // create 8 byte source



if ( ! mode ) {

   ctl[0] = (c >>  0) & 0xf;

   ctl[1] = (c >>  4) & 0xf;

   ctl[2] = (c >>  8) & 0xf;

   ctl[3] = (c >> 12) & 0xf;

} else {

   ctl[0] = ctl[1] = ctl[2] = ctl[3] = (c >>  0) & 0x3;

}



tmp[07:00] = ReadByte( mode, ctl[0], tmp64 );

tmp[15:08] = ReadByte( mode, ctl[1], tmp64 );

tmp[23:16] = ReadByte( mode, ctl[2], tmp64 );

tmp[31:24] = ReadByte( mode, ctl[3], tmp64 );
```

PTX ISA Notes

Introduced in PTX ISA version 2.0.

Target ISA Notes

`prmt` requires `sm_20` or higher.

Examples

```
prmt.b32      r1, r2, r3, r4;

prmt.b32.f4e  r1, r2, r3, r4;
```

#### 9.7.9.8. [Data Movement and Conversion Instructions: `ld`](#data-movement-and-conversion-instructions-ld)[](#data-movement-and-conversion-instructions-ld "Permalink to this headline")

`ld`

Load a register variable from an addressable state space variable.

Syntax

```
ld{.weak}{.ss}{.cop}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};



ld{.weak}{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{.unified}{, cache-policy};



ld.volatile{.ss}{.level::prefetch_size}{.vec}.type  d, [a];



ld.relaxed.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};



ld.acquire.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type  d, [a]{, cache-policy};



ld.mmio.relaxed.sys{.global}.type  d, [a];



.ss =                       { .const, .global, .local, .param{::entry, ::func}, .shared{::cta, ::cluster} };

.cop =                      { .ca, .cg, .cs, .lu, .cv };

.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,

                               .L1::evict_first, .L1::evict_last, .L1::no_allocate };

.level2::eviction_priority = {.L2::evict_normal, .L2::evict_first, .L2::evict_last};

.level::cache_hint =        { .L2::cache_hint };

.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }

.scope =                    { .cta, .cluster, .gpu, .sys };

.vec =                      { .v2, .v4, .v8 };

.type =                     { .b8, .b16, .b32, .b64, .b128,

                              .u8, .u16, .u32, .u64,

                              .s8, .s16, .s32, .s64,

                              .f32, .f64 };
```

Description

Load register variable `d` from the location specified by the source address operand `a` in
specified state space. If no state space is given, perform the load using [Generic Addressing](#generic-addressing).

If no sub-qualifier is specified with `.shared` state space, then `::cta` is assumed by default.

Supported addressing modes for operand `a` and alignment requirements are described in
[Addresses as Operands](#addresses-as-operands)

If no sub-qualifier is specified with `.param` state space, then:

* `::func` is assumed when access is inside a device function.
* `::entry` is assumed when accessing kernel function parameters from entry function. Otherwise, when
  accessing device function parameters or any other `.param` variables from entry function `::func`
  is assumed by default.

For `ld.param::entry` instruction, operand a must be a kernel parameter address, otherwise behavior
is undefined. For `ld.param::func` instruction, operand a must be a device function parameter address,
otherwise behavior is undefined.

Instruction `ld.param{::func}` used for reading value returned from device function call cannot be
predicated. See [Parameter State Space](#parameter-state-space) and
[Function Declarations and Definitions](#function-declarations-and-definitions) for descriptions
of the proper use of `ld.param`.

The `.relaxed` and `.acquire` qualifiers indicate memory synchronization as described in the
[Memory Consistency Model](#memory-consistency-model). The `.scope` qualifier
indicates the set of threads with which an `ld.relaxed` or `ld.acquire` instruction can directly
synchronize1. The `.weak` qualifier indicates a memory instruction with no synchronization.
The effects of this instruction become visible to other threads only when synchronization is established
by other means.

The semantic details of `.mmio` qualifier are described in the [Memory Consistency Model](#memory-consistency-model).
Only `.sys` thread scope is valid for `ld.mmio` operation. The
qualifiers `.mmio` and `.relaxed` must be specified together.

The semantic details of `.volatile` qualifier are described in the [Memory Consistency Model](#memory-consistency-model).

The `.weak`, `.volatile`, `.relaxed` and `.acquire` qualifiers are mutually exclusive. When
none of these is specified, the `.weak` qualifier is assumed by default.

The qualifiers `.volatile`, `.relaxed` and `.acquire` may be used only with `.global` and
`.shared` spaces and with generic addressing, where the address points to `.global` or
`.shared` space. Cache operations are not permitted with these qualifiers. The qualifier `.mmio`
may be used only with `.global` space and with generic addressing, where the address points to
`.global` space.

The optional qualifier `.unified` must be specified on operand `a` if `a` is the address of a
variable declared with `.unified` attribute as described in [Variable and Function Attribute Directive: .attribute](#variable-and-function-attribute-directive-attribute).

The `.v8` (`.vec`) qualifier is supported if:

* `.type` is `.b32` or `.s32` or `.u32` or `.f32` AND
* State space is `.global` or with generic addressing where address points to `.global` state space

The `.v4` (`.vec`) qualifier with type `.b64` or `.s64` or `.u64` or `.f64` is supported if:

* State space is `.global` or with generic addressing where address points to `.global` state space

Qualifiers `.level1::eviction_priority` and `.level2::eviction_priority` specify the eviction policy
for L1 and L2 cache respectively which may be applied during memory access.

Qualifier `.level2::eviction_priority` is supported if:

* `.vec` is `.v8` and `.type` is `.b32` or `.s32` or `.u32` or `.f32`

  + AND Operand `d` is vector of 8 registers with type specified with `.type`
* OR `.vec` is `.v4` and `.type` is `.b64` or `.s64` or `.u64` or `.f64`

  + AND Operand `d` is vector of 4 registers with type specified with `.type`

Optionally, sink symbol ‘\_’ can be used in vector expression `d` when:

* `.vec` is `.v8` and `.type` is `.b32` or `.s32` or `.u32` or `.f32` OR
* `.vec` is `.v4` and `.type` is `.b64` or `.s64` or `.u64` or `.f64`

which indicates that data from corresponding memory location is not read.

The `.level::prefetch_size` qualifier is a hint to fetch additional data of the specified size
into the respective cache level.The sub-qualifier `prefetch_size` can be set to either of `64B`,
`128B`, `256B` thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes
respectively.

The qualifier `.level::prefetch_size` may only be used with `.global` state space and with
generic addressing where the address points to `.global` state space. If the generic address does
not fall within the address window of the global memory, then the prefetching behavior is undefined.

The `.level::prefetch_size` qualifier is treated as a performance hint only.

When the optional argument `cache-policy` is specified, the qualifier `.level::cache_hint` is
required. The 64-bit operand `cache-policy` specifies the cache eviction policy that may be used
during the memory access.

The qualifiers `.unified` and `.level::cache_hint` are only supported for `.global` state
space and for generic addressing where the address points to the `.global` state space.

`cache-policy` is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.

1 This synchronization is further extended to other threads through the transitive nature of
*causality order*, as described in the memory consistency model.

Semantics

```
d = a;             // named variable a

d = *(&a+immOff)   // variable-plus-offset

d = *a;            // register

d = *(a+immOff);   // register-plus-offset

d = *(immAddr);    // immediate address
```

Notes

Destination `d` must be in the `.reg` state space.

A destination register wider than the specified type may be used. The value loaded is sign-extended
to the destination register width for signed integers, and is zero-extended to the destination
register width for unsigned and bit-size types. See
[Table 28](#operand-size-exceeding-instruction-type-size-relaxed-type-checking-rules-destination-operands)
for a description of these relaxed type-checking rules.

`.f16` data may be loaded using `ld.b16`, and then converted to `.f32` or `.f64` using
`cvt` or can be used in half precision floating point instructions.

`.f16x2` data may be loaded using `ld.b32` and then used in half precision floating point
instructions.

PTX ISA Notes

ld introduced in PTX ISA version 1.0. `ld.volatile` introduced in PTX ISA version 1.1.

Generic addressing and cache operations introduced in PTX ISA version 2.0.

Support for scope qualifier, `.relaxed`, `.acquire`, `.weak` qualifiers introduced in PTX ISA
version 6.0.

Support for generic addressing of .const space added in PTX ISA version 3.1.

Support for `.level1::eviction_priority`, `.level::prefetch_size` and `.level::cache_hint`
qualifiers introduced in PTX ISA version 7.4.

Support for `.cluster` scope qualifier introduced in PTX ISA version 7.8.

Support for `::cta` and `::cluster` sub-qualifiers introduced in PTX ISA version 7.8.

Support for `.unified` qualifier introduced in PTX ISA version 8.0.

Support for `.mmio` qualifier introduced in PTX ISA version 8.2.

Support for `::entry` and `::func` sub-qualifiers on `.param` space introduced in PTX ISA
version 8.3.

Support for `.b128` type introduced in PTX ISA version 8.3.

Support for `.sys` scope with `.b128` type introduced in PTX ISA version 8.4.

Support for `.level2::eviction_priority` qualifier and `.v8.b32`/`.v4.b64` introduced in PTX ISA version 8.8.

Target ISA Notes

`ld.f64` requires `sm_13` or higher.

Support for scope qualifier, `.relaxed`, `.acquire`, `.weak` qualifiers require `sm_70` or
higher.

Generic addressing requires `sm_20` or higher.

Cache operations require `sm_20` or higher.

Support for `.level::eviction_priority` qualifier requires `sm_70` or higher.

Support for `.level::prefetch_size` qualifier requires `sm_75` or higher.

Support for `.L2::256B` and `.L2::cache_hint` qualifiers requires `sm_80` or higher.

Support for `.cluster` scope qualifier requires `sm_90` or higher.

Sub-qualifier `::cta` requires `sm_30` or higher.

Sub-qualifier `::cluster` requires `sm_90` or higher.

Support for `.unified` qualifier requires `sm_90` or higher.

Support for `.mmio` qualifier requires `sm_70` or higher.

Support for `.b128` type requires `sm_70` or higher.

Support for `.level2::eviction_priority` qualifier and `.v8.b32`/`.v4.b64` require `sm_100` or higher.

Examples

```
ld.global.f32    d,[a];

ld.shared.v4.b32 Q,[p];

ld.const.s32     d,[p+4];

ld.local.b32     x,[p+-8]; // negative offset

ld.local.b64     x,[240];  // immediate address



ld.global.b16    %r,[fs];  // load .f16 data into 32-bit reg

cvt.f32.f16      %r,%r;    // up-convert f16 data to f32



ld.global.b32    %r0, [fs];     // load .f16x2 data in 32-bit reg

ld.global.b32    %r1, [fs + 4]; // load .f16x2 data in 32-bit reg

add.rn.f16x2     %d0, %r0, %r1; // addition of f16x2 data

ld.global.relaxed.gpu.u32 %r0, [gbl];

ld.shared.acquire.gpu.u32 %r1, [sh];

ld.global.relaxed.cluster.u32 %r2, [gbl];

ld.shared::cta.acquire.gpu.u32 %r2, [sh + 4];

ld.shared::cluster.u32 %r3, [sh + 8];

ld.global.mmio.relaxed.sys.u32 %r3, [gbl];



ld.global.f32    d,[ugbl].unified;

ld.b32           %r0, [%r1].unified;



ld.global.L1::evict_last.u32  d, [p];



ld.global.L2::64B.b32   %r0, [gbl]; // Prefetch 64B to L2

ld.L2::128B.f64         %r1, [gbl]; // Prefetch 128B to L2

ld.global.L2::256B.f64  %r2, [gbl]; // Prefetch 256B to L2



createpolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 1;

ld.global.L2::cache_hint.b64  x, [p], cache-policy;

ld.param::entry.b32 %rp1, [kparam1];



ld.global.b128   %r0, [gbl];   // 128-bit load



// 256-bit load

ld.global.L2::evict_last.v8.f32 { %reg0, _, %reg2, %reg3, %reg4, %reg5, %reg6, %reg7}, [addr];

ld.global.L2::evict_last.L1::evict_last.v4.u64 { %reg0, %reg1, %reg2, %reg3}, [addr];
```

#### 9.7.9.9. [Data Movement and Conversion Instructions: `ld.global.nc`](#data-movement-and-conversion-instructions-ld-global-nc)[](#data-movement-and-conversion-instructions-ld-global-nc "Permalink to this headline")

`ld.global.nc`

Load a register variable from global state space via non-coherent cache.

Syntax

```
ld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.type                 d, [a]{, cache-policy};

ld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.vec.type             d, [a]{, cache-policy};



ld.global.nc{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.type      d, [a]{, cache-policy};

ld.global.nc{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.vec.type  d, [a]{, cache-policy};



.cop  =                     { .ca, .cg, .cs };     // cache operation

.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,

                               .L1::evict_first, .L1::evict_last, .L1::no_allocate};

.level2::eviction_priority = {.L2::evict_normal, .L2::evict_first, .L2::evict_last};

.level::cache_hint =        { .L2::cache_hint };

.level::prefetch_size =     { .L2::64B, .L2::128B, .L2::256B }

.vec  =                     { .v2, .v4, .v8 };

.type =                     { .b8, .b16, .b32, .b64, .b128,

                              .u8, .u16, .u32, .u64,

                              .s8, .s16, .s32, .s64,

                              .f32, .f64 };
```

Description

Load register variable `d` from the location specified by the source address operand `a` in the
global state space, and optionally cache in non-coherent read-only cache.

Note

On some architectures, the texture cache is larger, has higher bandwidth, and longer latency than
the global memory cache. For applications with sufficient parallelism to cover the longer
latency, `ld.global.nc` should offer better performance than `ld.global` on such
architectures.

The address operand `a` shall contain a global address.
Supported addressing modes for operand `a` and alignment requirements are
described in [Addresses as Operands](#addresses-as-operands).

The `.v8` (`.vec`) qualifier is supported if:

* `.type` is `.b32`, `.s32`, `.u32`, or `.f32` AND
* State space is `.global` or with generic addressing where address points to `.global` state space

The `.v4` (`.vec`) qualifier with type `.b64` or `.s64` or `.u64` or `.f64` is supported if:

* State space is `.global` or with generic addressing where address points to `.global` state space

Qualifiers `.level1::eviction_priority` and `.level2::eviction_priority` specify the eviction policy
for L1 and L2 cache respectively which may be applied during memory access.

Qualifier `.level2::eviction_priority` is supported if:

* `.vec` is `.v8` and `.type` is `.b32` or `.s32` or `.u32` or `.f32`

  + AND Operand `d` is vector of 8 registers with type specified with `.type`
* OR `.vec` is `.v4` and `.type` is `.b64` or `.s64` or `.u64` or `.f64`

  + AND Operand `d` is vector of 4 registers with type specified with `.type`

Optionally, sink symbol ‘\_’ can be used in vector expression `d` when:

* `.vec` is `.v8` and `.type` is `.b32` or `.s32` or `.u32` or `.f32` OR
* `.vec` is `.v4` and `.type` is `.b64` or `.s64` or `.u64` or `.f64`

which indicates that data from corresponding memory location is not read.

The `.level::prefetch_size` qualifier is a hint to fetch additional data of the specified size
into the respective cache level.The sub-qualifier `prefetch_size` can be set to either of `64B`,
`128B`, `256B` thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes
respectively.

The `.level::prefetch_size` qualifier is treated as a performance hint only.

When the optional argument `cache-policy` is specified, the qualifier `.level::cache_hint` is
required. The 64-bit operand `cache-policy` specifies the cache eviction policy that may be used
during the memory access.

`cache-policy` is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.

Semantics

```
d = a;             // named variable a

d = *(&a+immOff)   // variable-plus-offset

d = *a;            // register

d = *(a+immOff);   // register-plus-offset

d = *(immAddr);    // immediate address
```

Notes

Destination `d` must be in the `.reg` state space.

A destination register wider than the specified type may be used. The value loaded is sign-extended
to the destination register width for signed integers, and is zero-extended to the destination
register width for unsigned and bit-size types.

`.f16` data may be loaded using `ld.b16`, and then converted to `.f32` or `.f64` using `cvt`.

PTX ISA Notes

Introduced in PTX ISA version 3.1.

Support for `.level::eviction_priority`, `.level::prefetch_size` and `.level::cache_hint`
qualifiers introduced in PTX ISA version 7.4.

Support for `.b128` type introduced in PTX ISA version 8.3.

Support for `.level2::eviction_priority` qualifier and `.v8.b32`/`.v4.b64` introduced in PTX ISA version 8.8.

Target ISA Notes

Requires `sm_32` or higher.

Support for `.level1::eviction_priority` qualifier requires `sm_70` or higher.

Support for `.level::prefetch_size` qualifier requires `sm_75` or higher.

Support for `.level::cache_hint` qualifier requires `sm_80` or higher.

Support for `.b128` type requires `sm_70` or higher.

Support for `.level2::eviction_priority` qualifier and `.v8.b32`/`.v4.b64` require `sm_100` or higher.

Examples

```
ld.global.nc.f32           d, [a];

ld.gloal.nc.L1::evict_last.u32 d, [a];



createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.5;

ld.global.nc.L2::cache_hint.f32  d, [a], cache-policy;



ld.global.nc.L2::64B.b32      d,  [a];     // Prefetch 64B to L2

ld.global.nc.L2::256B.f64     d,  [a];     // Prefetch 256B to L2



ld.global.nc.b128             d,  [a];



ld.global.nc.L2::evict_first.v4.f64 {%reg0, %reg1. %reg2, %reg3}. [a]; // 256-bit load
```

#### 9.7.9.10. [Data Movement and Conversion Instructions: `ldu`](#data-movement-and-conversion-instructions-ldu)[](#data-movement-and-conversion-instructions-ldu "Permalink to this headline")

`ldu`

Load read-only data from an address that is common across threads in the warp.

Syntax

```
ldu{.ss}.type      d, [a];       // load from address

ldu{.ss}.vec.type  d, [a];       // vec load from address



.ss   = { .global };             // state space

.vec  = { .v2, .v4 };

.type = { .b8, .b16, .b32, .b64, .b128,

          .u8, .u16, .u32, .u64,

          .s8, .s16, .s32, .s64,

                     .f32, .f64 };
```

Description

Load *read-only* data into register variable `d` from the location specified by the source address
operand `a` in the global state space, where the address is guaranteed to be the same across all
threads in the warp. If no state space is given, perform the load using [Generic Addressing](#generic-addressing).

Supported addressing modes for operand `a` and alignment requirements are described in
[Addresses as Operands](#addresses-as-operands).

Semantics

```
d = a;             // named variable a

d = *(&a+immOff)   // variable-plus-offset

d = *a;            // register

d = *(a+immOff);   // register-plus-offset

d = *(immAddr);    // immediate address
```

Notes

Destination `d` must be in the `.reg` state space.

A destination register wider than the specified type may be used. The value loaded is sign-extended
to the destination register width for signed integers, and is zero-extended to the destination
register width for unsigned and bit-size types. See
[Table 28](#operand-size-exceeding-instruction-type-size-relaxed-type-checking-rules-destination-operands)
for a description of these relaxed type-checking rules.

`.f16` data may be loaded using `ldu.b16`, and then converted to `.f32` or `.f64` using
`cvt` or can be used in half precision floating point instructions.

`.f16x2` data may be loaded using `ldu.b32` and then used in half precision floating point
instructions.

PTX ISA Notes

Introduced in PTX ISA version 2.0.

Support for `.b128` type introduced in PTX ISA version 8.3.

Target ISA Notes

`ldu.f64` requires `sm_13` or higher.

Support for `.b128` type requires `sm_70` or higher.

Examples

```
ldu.global.f32    d,[a];

ldu.global.b32    d,[p+4];

ldu.global.v4.f32 Q,[p];

ldu.global.b128   d,[a];
```

#### 9.7.9.11. [Data Movement and Conversion Instructions: `st`](#data-movement-and-conversion-instructions-st)[](#data-movement-and-conversion-instructions-st "Permalink to this headline")

`st`

Store data to an addressable state space variable.

Syntax

```
st{.weak}{.ss}{.cop}{.level::cache_hint}{.vec}.type   [a], b{, cache-policy};

st{.weak}{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type

                                                      [a], b{, cache-policy};

st.volatile{.ss}{.vec}.type                           [a], b;

st.relaxed.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type

                                                      [a], b{, cache-policy};

st.release.scope{.ss}{.level1::eviction_priority}{.level2::eviction_priority}{.level::cache_hint}{.vec}.type

                                                      [a], b{, cache-policy};

st.mmio.relaxed.sys{.global}.type         [a], b;



.ss =                       { .global, .local, .param{::func}, .shared{::cta, ::cluster} };

.level1::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged,

                               .L1::evict_first, .L1::evict_last, .L1::no_allocate };

.level2::eviction_priority = { .L2::evict_normal, .L2::evict_first, .L2::evict_last };

.level::cache_hint =        { .L2::cache_hint };

.cop =                      { .wb, .cg, .cs, .wt };

.sem =                      { .relaxed, .release };

.scope =                    { .cta, .cluster, .gpu, .sys };

.vec =                      { .v2, .v4, .v8 };

.type =                     { .b8, .b16, .b32, .b64, .b128,

                              .u8, .u16, .u32, .u64,

                              .s8, .s16, .s32, .s64,

                              .f32, .f64 };
```

Description

Store the value of operand `b` in the location specified by the destination address
operand `a` in specified state space. If no state space is given, perform the store using
[Generic Addressing](#generic-addressing). Stores to const memory are illegal.

If no sub-qualifier is specified with `.shared` state space, then `::cta` is assumed by default.

Supported addressing modes for operand `a` and alignment requirements are described in
[Addresses as Operands](#addresses-as-operands).

If `.param` is specified without any sub-qualifiers then it defaults to `.param::func`.

Instruction `st.param{::func}` used for passing arguments to device function cannot be predicated.
See [Parameter State Space](#parameter-state-space) and [Function Declarations and Definitions](#function-declarations-and-definitions)
for descriptions of the proper use
of `st.param`.

The qualifiers `.relaxed` and `.release` indicate memory synchronization as described in the
[Memory Consistency Model](#memory-consistency-model). The `.scope` qualifier
indicates the set of threads with which an `st.relaxed` or `st.release` instruction can directly
synchronize1. The `.weak` qualifier indicates a memory instruction with no synchronization.
The effects of this instruction become visible to other threads only when synchronization is established
by other means.

The semantic details of `.mmio` qualifier are described in the [Memory Consistency Model](#memory-consistency-model).
Only `.sys` thread scope is valid for `st.mmio` operation. The
qualifiers `.mmio` and `.relaxed` must be specified together.

The semantic details of `.volatile` qualifier are described in the
[Memory Consistency Model](#memory-consistency-model).

The `.weak`, `.volatile`, `.relaxed` and `.release` qualifiers are mutually exclusive. When
none of these is specified, the `.weak` qualifier is assumed by default.

The qualifiers `.volatile`, `.relaxed` and `.release` may be used only with `.global` and
`.shared` spaces and with generic addressing, where the address points to `.global` or
`.shared` space. Cache operations are not permitted with these qualifiers. The qualifier `.mmio`
may be used only with `.global` space and with generic addressing, where the address points to
`.global` space.

The `.v8` (`.vec`) qualifier is supported if:

* `.type` is `.b32`, `.s32`, `.u32`, or `.f32` AND
* State space is `.global` or with generic addressing where address points to `.global` state space

The `.v4` (`.vec`) qualifier with type `.b64` or `.s64` or `.u64` or `.f64` is supported if:

* State space is `.global` or with generic addressing where address points to `.global` state space

Qualifiers `.level1::eviction_priority` and `.level2::eviction_priority` specify the eviction policy
for L1 and L2 cache respectively which may be applied during memory access.

Qualifier `.level2::eviction_priority` is supported if:

* `.vec` is `.v8` and `.type` is `.b32` or `.s32` or `.u32` or `.f32`

  + AND Operand `d` is vector of 8 registers with type specified with `.type`
* OR `.vec` is `.v4` and `.type` is `.b64` or `.s64` or `.u64` or `.f64`

  + AND Operand `d` is vector of 4 registers with type specified with `.type`

Optionally, sink symbol ‘\_’ can be used in vector expression `b` when:

* `.vec` is `.v8` and `.type` is `.b32` or `.s32` or `.u32` or `.f32` OR
* `.vec` is `.v4` and `.type` is `.b64` or `.s64` or `.u64` or `.f64`

which indicates that no data is being written at the corresponding destination address.

When the optional argument `cache-policy` is specified, the qualifier `.level::cache_hint` is
required. The 64-bit operand `cache-policy` specifies the cache eviction policy that may be used
during the memory access.

The qualifier `.level::cache_hint` is only supported for `.global` state space and for generic
addressing where the address points to the `.global` state space.

`cache-policy` is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.

1 This synchronization is further extended to other threads through the transitive nature of
*causality order*, as described in the memory consistency model.

Semantics

```
d = a;                // named variable d

*(&a+immOffset) = b;            // variable-plus-offset

*a = b;               // register

*(a+immOffset) = b;   // register-plus-offset

*(immAddr) = b;       // immediate address
```

Notes

Operand `b` must be in the `.reg` state space.

A source register wider than the specified type may be used. The lower `n` bits corresponding to
the instruction-type width are stored to memory. See
[Table 27](#operand-size-exceeding-instruction-type-size-relaxed-type-checking-rules-source-operands)
for a description of these relaxed type-checking rules.

`.f16` data resulting from a `cvt` instruction may be stored using `st.b16`.

`.f16x2` data may be stored using `st.b32`.

PTX ISA Notes

st introduced in PTX ISA version 1.0. `st.volatile` introduced in PTX ISA version 1.1.

Generic addressing and cache operations introduced in PTX ISA version 2.0.

Support for scope qualifier, `.relaxed`, `.release`, `.weak` qualifiers introduced in PTX ISA
version 6.0.

Support for `.level1::eviction_priority` and `.level::cache_hint` qualifiers introduced in PTX
ISA version 7.4.

Support for `.cluster` scope qualifier introduced in PTX ISA version 7.8.

Support for `::cta` and `::cluster` sub-qualifiers introduced in PTX ISA version 7.8.

Support for `.mmio` qualifier introduced in PTX ISA version 8.2.

Support for `::func` sub-qualifier on `.param` space introduced in PTX ISA version 8.3.

Support for `.b128` type introduced in PTX ISA version 8.3.

Support for `.sys` scope with `.b128` type introduced in PTX ISA version 8.4.

Support for `.level2::eviction_priority` qualifier and `.v8.b32`/`.v4.b64` introduced in PTX ISA version 8.8.

Target ISA Notes

`st.f64` requires `sm_13` or higher.

Support for scope qualifier, `.relaxed`, `.release`, `.weak` qualifiers require `sm_70` or
higher.

Generic addressing requires `sm_20` or higher.

Cache operations require `sm_20` or higher.

Support for `.level1::eviction_priority` qualifier requires `sm_70` or higher.

Support for `.level::cache_hint` qualifier requires `sm_80` or higher.

Support for `.cluster` scope qualifier requires `sm_90` or higher.

Sub-qualifier `::cta` requires `sm_30` or higher.

Sub-qualifier `::cluster` requires `sm_90` or higher.

Support for `.mmio` qualifier requires `sm_70` or higher.

Support for `.b128` type requires `sm_70` or higher.

Support for `.level2::eviction_priority` qualifier and `.v8.b32`/`.v4.b64` require `sm_100` or higher.

Examples

```
st.global.f32    [a],b;

st.local.b32     [q+4],a;

st.global.v4.s32 [p],Q;

st.local.b32     [q+-8],a; // negative offset

st.local.s32     [100],r7; // immediate address



cvt.f16.f32      %r,%r;    // %r is 32-bit register

st.b16           [fs],%r;  // store lower

st.global.relaxed.sys.u32 [gbl], %r0;

st.shared.release.cta.u32 [sh], %r1;

st.global.relaxed.cluster.u32 [gbl], %r2;

st.shared::cta.release.cta.u32 [sh + 4], %r1;

st.shared::cluster.u32 [sh + 8], %r1;

st.global.mmio.relaxed.sys.u32 [gbl], %r1;



st.global.L1::no_allocate.f32 [p], a;



createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.25;

st.global.L2::cache_hint.b32  [a], b, cache-policy;



st.param::func.b64 [param1], %rp1;



st.global.b128  [a], b;  // 128-bit store



// 256-bit store

st.global.L2::evict_last.v8.f32 [addr], { %reg0, _, %reg2, %reg3, %reg4, %reg5, %reg6, %reg7};
```

#### 9.7.9.12. [Data Movement and Conversion Instructions: `st.async`](#data-movement-and-conversion-instructions-st-async)[](#data-movement-and-conversion-instructions-st-async "Permalink to this headline")

`st.async`

Asynchronous store operation.

Syntax

```
st.async{.sem}{.scope}{.ss}{.completion_mechanism}{.vec}.type [a], b, [mbar];



.sem  =                 { .weak };

.scope =                { .cluster };

.ss   =                 { .shared::cluster };

.type =                 { .b32, .b64,

                          .u32, .u64,

                          .s32, .s64,

                          .f32, .f64 };

.vec  =                 { .v2, .v4 };

.completion_mechanism = { .mbarrier::complete_tx::bytes };



st.async{.mmio}.sem.scope{.ss}{.completion_mechanism}.type [a], b;



.sem =                  { .release };

.scope =                { .gpu, .sys };

.ss =                   { .global };

.completion_mechanism = { };

.type =                 { .b8, .b16, .b32, .b64,

                          .u8, .u16, .u32, .u64,

                          .s8, .s16, .s32, .s64,

                                     .f32, .f64 };
```

Description

`st.async` is a non-blocking instruction which initiates an asynchronous store operation that
stores the value specified by source operand `b` to the destination memory location
specified by operand `a`.

Operands

* `a` is a destination address, and must be either a register, or of the form `register + immOff`,
  as described in [Addresses as Operands](#addresses-as-operands).
* `b` is a source value, of the type indicated by qualifier `.type`.
* `mbar` is an mbarrier object address.

Qualifiers

* `.mmio` indicates whether this is an [mmio Operation](#mmio-operation).
* `.sem` specifies the memory ordering semantics as described in the
  [Memory Consistency Model](#memory-consistency-model).

  + If `.sem` is not specified, it defaults to `.weak`.
* `.scope` specifies the set of threads with which this instruction can directly synchronize.
* `.ss` specifies the state space of the destination operand `a` and the mbarrier
  operand `mbar`.

  + If `.ss` is not specified, [Generic Addressing](#generic-addressing) is used.
* `.completion_mechanism` specifies the mechanism for observing the completion of the
  asynchronous operation.

  + When `.completion_mechanism` is `.mbarrier::complete_tx::bytes`: upon completion of the
    asynchronous operation, a
    [complete-tx](#parallel-synchronization-and-communication-instructions-mbarrier-complete-tx-operation)
    operation will be performed on the mbarrier object specified by the operand `mbar`, with
    `completeCount` argument equal to the amount of data stored in bytes.
  + When `.completion_mechanism` is not specified: the completion of the store synchronizes
    with the end of the CTA.
* `.type` specifies the type of the source operand `b`.

Conditions

When `.sem` is `.weak`:

* This is a weak store to shared memory, which signals its completion through an mbarrier object.
* The store operation is treated as a weak memory operation.
* The complete-tx operation on the mbarrier has `.release` semantics at `.cluster`
  scope.
* Requires:

  + The shared memory addresses of destination operand `a` and the *mbarrier object* `mbar` belong
    to the same CTA within the same cluster as the executing thread.
  + The number of CTAs within the cluster is strictly greater than one; `%cluster_nctarank > 1` is true.

  Otherwise, the behavior is undefined.
* `.mmio` must not be specified.
* If `.ss` is specified, it must be `.shared::cluster`.
* If `.ss` is not specified, generic addressing is used for operands `a` and `mbar`.
  If the generic addresses specified do not fall within the address window of
  `.shared::cluster` state space, the behavior is undefined.
* If `.completion_mechanism` is specified, it must be `.mbarrier::complete_tx::bytes`.
* If `.completion_mechanism` is not specified, it defaults to `.mbarrier::complete_tx::bytes`.

When `.sem` is `.release`:

* This is a release store to global memory.
* The store operation is a strong memory operation with `.release` semantics at the
  scope specified by `.scope`.
* If `.mmio` is specified, `.scope` must be `.sys`.
* If `.ss` is specified, it must be `.global`.
* If `.ss` is not specified, generic addressing is used for operand `a`.
  If the generic address specified does not fall within the address window of `.global`
  state space, the behavior is undefined.
* `.completion_mechanism` must not be specified.

PTX ISA Notes

Introduced in PTX ISA version 8.1.

Support for `.mmio` qualifier, `.release` semantics, `.global` state space, and
`.scope` qualifier introduced in PTX ISA version 8.7.

Target ISA Notes

Requires `sm_90` or higher.

`.mmio` qualifier, `.release` semantics, `.global` state space, and
`.scope` qualifier require `sm_100` or higher.

Examples

```
st.async.shared::cluster.mbarrier::complete_tx::bytes.u32 [addr], b, [mbar_addr];



st.async.release.global.u32 [addr], b;
```

#### 9.7.9.13. [Data Movement and Conversion Instructions: `st.bulk`](#data-movement-and-conversion-instructions-st-bulk)[](#data-movement-and-conversion-instructions-st-bulk "Permalink to this headline")

`st.bulk`

Initializes a region of memory as specified by state space.

Syntax

```
st.bulk{.weak}{.shared::cta}  [a], size, initval; // initval must be zero
```

Description

`st.bulk` instruction initializes a region of shared memory starting from the location specified
by destination address operand `a`.

The 32-bit or 64-bit integer operand `size` specifies the amount of memory to be initialized in terms of
number of bytes. `size` must be a multiple of 8. If the value is not a multiple of 8, then the
behavior is undefined. The maximum value of `size` operand can be 16777216.

The integer immediate operand `initval` specifies the initialization value for the memory
locations. The only numeric value allowed for operand `initval` is 0.

If no state space is specified then [Generic Addressing](#generic-addressing) is used. If the
address specified by `a` does not fall within the address window of `.shared` state space then
the behavior is undefined.

The optional qualifier `.weak` specify the memory synchronizing effect of the `st.bulk`
instruction as described in the [Memory Consistency Model](#memory-consistency-model).

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Support for `size` operand with 32-bit length is introduced in PTX ISA version 9.0.

Target ISA Notes

Requires `sm_100` or higher.

Examples

```
st.bulk.weak.shared::cta  [dst], n, 0;



st.bulk                   [gdst], 4096, 0;
```

#### 9.7.9.14. [Data Movement and Conversion Instructions: `multimem.ld_reduce`, `multimem.st`, `multimem.red`](#data-movement-and-conversion-instructions-multimem)[](#data-movement-and-conversion-instructions-multimem "Permalink to this headline")

The multimem.\* operations operate on multimem addresses and accesses all of the multiple memory
locations which the multimem address points to.

Multimem addresses can only be accessed only by multimem.\* operations. Accessing a multimem address
with `ld`, `st` or any other memory operations results in undefined behavior.

Refer to *CUDA programming guide* for creation and management of the multimem addresses.

`multimem.ld_reduce`, `multimem.st`, `multimem.red`

Perform memory operations on the multimem address.

Syntax

```
// Integer type:



multimem.ld_reduce{.ldsem}{.scope}{.ss}.op.type      d, [a];

multimem.ld_reduce.weak{.ss}.op.type                 d, [a];



multimem.st{.stsem}{.scope}{.ss}.type                [a], b;

multimem.st.weak{.ss}.type                           [a], b;



multimem.red{.redsem}{.scope}{.ss}.op.type           [a], b;



.ss =       { .global }

.ldsem =    { .relaxed, .acquire }

.stsem =    { .relaxed, .release }

.redsem =   { .relaxed, .release }

.scope =    { .cta, .cluster, .gpu, .sys }

.op  =      { .min, .max, .add, .and, .or, .xor }

.type =     { .b32, .b64,  .u32, .u64, .s32, .s64 }



// Floating point type:



multimem.ld_reduce{.ldsem}{.scope}{.ss}.op{.acc_prec}{.vec}.type    d, [a];

multimem.ld_reduce.weak{.ss}.op{.acc_prec}{.vec}.type               d, [a];



multimem.st{.stsem}{.scope}{.ss}{.vec}.type                         [a], b;

multimem.st.weak{.ss}{.vec}.type                                    [a], b;



multimem.red{.redsem}{.scope}{.ss}.redop{.vec}.redtype              [a], b;



.ss =       { .global }

.ldsem =    { .relaxed, .acquire }

.stsem =    { .relaxed, .release }

.redsem =   { .relaxed, .release }

.scope =    { .cta, .cluster, .gpu, .sys }

.op  =      { .min, .max, .add }

.redop  =   { .add }

.acc_prec = { .acc::f32, .acc::f16 }

.vec =      { .v2, .v4, .v8 }

.type=      { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64, .e5m2, .e5m2x2, .e5m2x4, .e4m3, .e4m3x2, .e4m3x4 }

.redtype =  { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64 }
```

Description

Instruction `multimem.ld_reduce` performs the following operations:

* load operation on the multimem address `a`, which involves loading of data from all of the
  multiple memory locations pointed to by the multimem address `a`,
* reduction operation specified by `.op` on the multiple data loaded from the multimem address
  `a`.

The result of the reduction operation in returned in register `d`.

Instruction `multimem.st` performs a store operation of the input operand `b` to all the memory
locations pointed to by the multimem address `a`.

Instruction `multimem.red` performs a reduction operation on all the memory locations pointed to
by the multimem address `a`, with operand `b`.

Instruction `multimem.ld_reduce` performs reduction on the values loaded from all the memory
locations that the multimem address points to. In contrast, the `multimem.red` perform reduction
on all the memory locations that the multimem address points to.

Address operand `a` must be a multimem address. Otherwise, the behavior is undefined. Supported
addressing modes for operand a and alignment requirements are described in
[Addresses as Operands](#addresses-as-operands).

If no state space is specified then [Generic Addressing](#generic-addressing) is
used. If the address specified by `a` does not fall within the address window of `.global` state
space then the behavior is undefined.

For floating-point type multi- operations, the size of the specified type along with `.vec` must
equal either 32-bits or 64-bits or 128-bits. No other combinations of `.vec` and type are
allowed. Type `.f64` cannot be used with `.vec` qualifier.
The following table describes the valid usage of `.vec` and base floating-point type:

| .vec | Base float-type supported |
| --- | --- |
| No `.vec` specified | `.f16x2`, `.bf16x2`, `.f32`, `.f64`, `.e5m2x4`, `.e4m3x4` |
| `.v2` | `.f16`, `.f16x2`, `.bf16`, `.bf16x2` `.f32`, `.e5m2x2`, `.e5m2x4`, `.e4m3x2`, `.e4m3x4` |
| `.v4` | `.f16`, `.f16x2`, `.bf16`, `.bf16x2` `.f32`, `.e5m2`, `.e5m2x2`, `.e5m2x4`, `.e4m3`, `.e4m3x2`, `.e4m3x4` |
| `.v8` | `.f16`, `.bf16`, `.e5m2`, `.e4m3`, `.e5m2x2`, `.e4m3x2` |

The following table describes the valid combinations of `.op` and base type:

| op | Base type |
| --- | --- |
| `.add` | `.u32`, `.u64`, `.s32` `.f16`, `.f16x2`, `.bf16`, `.bf16x2` `.f32`, `.f64`, `.e5m2`, `.e5m2x2`, `.e5m2x4`, `.e4m3`, `.e4m3x2`, `.e4m3x4` |
| `.and`, `.or`, `.xor` | `.b32`, `.b64` |
| `.min`, `.max` | `.u32`, `.s32`, `.u64`, `.s64` `.f16`, `.f16x2`, `.bf16`, `.bf16x2` `.e5m2`, `.e5m2x2`, `.e5m2x4`, `.e4m3`, `.e4m3x2`, `.e4m3x4` |

For `multimem.ld_reduce`, the default precision of the intermediate accumulation is same as the
specified type.

Optionally, `.acc_prec` qualifier can be specified to change the precision of intermediate
accumulation as follows:

| .type | .acc::prec | Changes precision to |
| --- | --- | --- |
| `.f16`, `.f16x2`, `.bf16`, `.bf16x2` | `.acc::f32` | `.f32` |
| `.e5m2`, `.e4m3`, `.e5m2x2`, `.e4m3x2`, `.e4m3x4`, `.e5m2x4` | `.acc::f16` | `.f16` |

Optional qualifiers `.ldsem`, `.stsem` and `.redsem` specify the memory synchronizing effect
of the `multimem.ld_reduce`, `multimem.st` and `multimem.red` respectively, as described in
[Memory Consistency Model](#memory-consistency-model). If explicit semantics qualifiers
are not specified, then `multimem.ld_reduce` and `multimem.st` default to `.weak` and
`multimem.red` defaults to `.relaxed`.

The optional `.scope` qualifier specifies the set of threads that can directly observe the memory
synchronizing effect of this operation, as described in
[Memory Consistency Model](#memory-consistency-model). If the `.scope` qualifier is not specified for
`multimem.red` then `.sys` scope is assumed by default.

PTX ISA Notes

Introduced in PTX ISA version 8.1.

Support for `.acc::f32` qualifier introduced in PTX ISA version 8.2.

Support for types `.e5m2`, `.e5m2x2`, `.e5m2x4`, `.e4m3`, `.e4m3x2`, `.e4m3x4`
introduced in PTX ISA version 8.6.

Support for `.acc::f16` qualifier introduced in PTX ISA version 8.6.

Target ISA Notes

Requires `sm_90` or higher.

Types `.e5m2`, `.e5m2x2`, `.e5m2x4`, `.e4m3`, `.e4m3x2`, `.e4m3x4`
are supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_120a`
* `sm_121a`
* And are supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Qualifier `.acc::f16` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_120a`
* `sm_121a`
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  > + `sm_100f` or higher in the same family
  > + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Examples

```
multimem.ld_reduce.and.b32                    val1_b32, [addr1];

multimem.ld_reduce.acquire.gpu.global.add.u32 val2_u32, [addr2];



multimem.st.relaxed.gpu.b32                [addr3], val3_b32;

multimem.st.release.cta.global.u32         [addr4], val4_u32;



multimem.red.relaxed.gpu.max.f64           [addr5], val5_f64;

multimem.red.release.cta.global.add.v4.f32 [addr6], {val6, val7, val8, val9};

multimem.ld_reduce.add.acc::f32.v2.f16x2   {val_10, val_11}, [addr7];



multimem.ld_reduce.relaxed.cta.min.v2.e4m3x2 {val_12, val_13}, [addr8];

multimem.ld_reduce.relaxed.cta.add.v4.e4m3   {val_14, val_15, val_16, val_17}, [addr9];

multimem.ld_reduce.add.acc::f16.v4.e5m2      {val_18, val_19, val_20, val_21}, [addr10];
```

#### 9.7.9.15. [Data Movement and Conversion Instructions: `prefetch`, `prefetchu`](#data-movement-and-conversion-instructions-prefetch-prefetchu)[](#data-movement-and-conversion-instructions-prefetch-prefetchu "Permalink to this headline")

`prefetch`, `prefetchu`

Prefetch line containing a generic address at a specified level of memory hierarchy, in specified
state space.

Syntax

```
prefetch{.space}.level                    [a];   // prefetch to data cache

prefetch.global.level::eviction_priority  [a];   // prefetch to data cache



prefetchu.L1  [a];             // prefetch to uniform cache



prefetch{.tensormap_space}.tensormap [a];  // prefetch the tensormap



.space =                    { .global, .local };

.level =                    { .L1, .L2 };

.level::eviction_priority = { .L2::evict_last, .L2::evict_normal };

.tensormap_space =          { .const, .param };
```

Description

The `prefetch` instruction brings the cache line containing the specified address in global or
local memory state space into the specified cache level.

If the `.tensormap` qualifier is specified then the `prefetch` instruction brings the cache line
containing the specified address in the `.const` or `.param` memory state space for subsequent
use by the `cp.async.bulk.tensor` instruction.

If no state space is given, the `prefetch` uses [Generic Addressing](#generic-addressing).

Optionally, the eviction priority to be applied on the prefetched cache line can be specified by the
modifier `.level::eviction_priority`.

Supported addressing modes for operand `a` and alignment requirements are described in
[Addresses as Operands](#addresses-as-operands)

The `prefetchu` instruction brings the cache line containing the specified generic address into
the specified uniform cache level.

A `prefetch` to a shared memory location performs no operation.

A `prefetch` into the uniform cache requires a generic address, and no operation occurs if the
address maps to a `const`, `local`, or `shared` memory location.

PTX ISA Notes

Introduced in PTX ISA version 2.0.

Support for `.level::eviction_priority` qualifier introduced in PTX ISA version 7.4.

Support for the `.tensormap` qualifier is introduced in PTX ISA version 8.0.

Target ISA Notes

`prefetch` and `prefetchu` require `sm_20` or higher.

Support for `.level::eviction_priority` qualifier requires `sm_80` or higher.

Support for the `.tensormap` qualifier requires `sm_90` or higher.

Examples

```
prefetch.global.L1             [ptr];

prefetch.global.L2::evict_last [ptr];

prefetchu.L1  [addr];

prefetch.const.tensormap       [ptr];
```

#### 9.7.9.16. [Data Movement and Conversion Instructions: `applypriority`](#data-movement-and-conversion-instructions-applypriority)[](#data-movement-and-conversion-instructions-applypriority "Permalink to this headline")

`applypriority`

Apply the cache eviction priority to the specified address in the specified cache level.

Syntax

```
applypriority{.global}.level::eviction_priority  [a], size;



.level::eviction_priority = { .L2::evict_normal };
```

Description

The `applypriority` instruction applies the cache eviction priority specified by the
`.level::eviction_priority` qualifier to the address range `[a..a+size)` in the specified cache
level.

If no state space is specified then [Generic Addressing](#generic-addressing) is
used. If the specified address does not fall within the address window of `.global` state space
then the behavior is undefined.

The operand `size` is an integer constant that specifies the amount of data, in bytes, in the
specified cache level on which the priority is to be applied. The only supported value for the
`size` operand is 128.

Supported addressing modes for operand `a` are described in [Addresses as Operands](#addresses-as-operands).
`a` must be aligned to 128 bytes.

PTX ISA Notes

Introduced in PTX ISA version 7.4.

Target ISA Notes

Requires `sm_80` or higher.

Examples

```
applypriority.global.L2::evict_normal [ptr], 128;
```

#### 9.7.9.17. [Data Movement and Conversion Instructions: `discard`](#data-movement-and-conversion-instructions-discard)[](#data-movement-and-conversion-instructions-discard "Permalink to this headline")

`discard`

Discard the data at the specified address range and cache level.

Syntax

```
discard{.global}.level  [a], size;



.level = { .L2 };
```

Description

Semantically, this behaves like a weak write of an *unstable indeterminate value*:
reads of memory locations with *unstable indeterminate values* may return different
bit patterns each time until the memory is overwritten.
This operation *hints* to the implementation that data in the specified cache `.level`
can be destructively discarded without writing it back to memory.

The operand `size` is an integer constant that specifies the length in bytes of the
address range `[a, a + size)` to write *unstable indeterminate values* into.
The only supported value for the `size` operand is `128`.

If no state space is specified then [Generic Addressing](#generic-addressing) is used.
If the specified address does not fall within the address window of `.global` state space
then the behavior is undefined.

Supported addressing modes for address operand `a` are described in [Addresses as Operands](#addresses-as-operands).
`a` must be aligned to 128 bytes.

PTX ISA Notes

Introduced in PTX ISA version 7.4.

Target ISA Notes

Requires `sm_80` or higher.

Examples

```
discard.global.L2 [ptr], 128;

ld.weak.u32 r0, [ptr];

ld.weak.u32 r1, [ptr];

// The values in r0 and r1 may differ!
```

#### 9.7.9.18. [Data Movement and Conversion Instructions: `createpolicy`](#data-movement-and-conversion-instructions-createpolicy)[](#data-movement-and-conversion-instructions-createpolicy "Permalink to this headline")

`createpolicy`

Create a cache eviction policy for the specified cache level.

Syntax

```
// Range-based policy

createpolicy.range{.global}.level::primary_priority{.level::secondary_priority}.b64

                                   cache-policy, [a], primary-size, total-size;



// Fraction-based policy

createpolicy.fractional.level::primary_priority{.level::secondary_priority}.b64

                                   cache-policy{, fraction};



// Converting the access property from CUDA APIs

createpolicy.cvt.L2.b64            cache-policy, access-property;



.level::primary_priority =   { .L2::evict_last, .L2::evict_normal,

                               .L2::evict_first, .L2::evict_unchanged };

.level::secondary_priority = { .L2::evict_first, .L2::evict_unchanged };
```

Description

The `createpolicy` instruction creates a cache eviction policy for the specified cache level in an
opaque 64-bit register specified by the destination operand `cache-policy`. The cache eviction
policy specifies how cache eviction priorities are applied to global memory addresses used in memory
operations with `.level::cache_hint` qualifier.

There are two types of cache eviction policies:

* Range-based policy

  The cache eviction policy created using `createpolicy.range` specifies the cache eviction
  behaviors for the following three address ranges:

  + `[a .. a + (primary-size - 1)]` referred to as primary range.
  + `[a + primary-size .. a + (total-size - 1)]` referred to as trailing secondary range.
  + `[a - (total-size - primary-size) .. (a - 1)]` referred to as preceding secondary range.

  When a range-based cache eviction policy is used in a memory operation with
  `.level::cache_hint` qualifier, the eviction priorities are applied as follows:

  + If the memory address falls in the primary range, the eviction priority specified by
    `.L2::primary_priority` is applied.
  + If the memory address falls in any of the secondary ranges, the eviction priority specified by
    `.L2::secondary_priority` is applied.
  + If the memory address does not fall in either of the above ranges, then the applied eviction
    priority is unspecified.

  The 32-bit operand `primary-size` specifies the size, in bytes, of the primary range. The
  32-bit operand `total-size` specifies the combined size, in bytes, of the address range
  including primary and secondary ranges. The value of `primary-size` must be less than or equal
  to the value of `total-size`. Maximum allowed value of `total-size` is 4GB.

  If `.L2::secondary_priority` is not specified, then it defaults to `.L2::evict_unchanged`.

  If no state space is specified then [Generic Addressing](#generic-addressing) is
  used. If the specified address does not fall within the address window of `.global` state space
  then the behavior is undefined.
* Fraction-based policy

  A memory operation with `.level::cache_hint` qualifier can use the fraction-based cache
  eviction policy to request the cache eviction priority specified by `.L2:primary_priority` to
  be applied to a fraction of cache accesses specified by the 32-bit floating point operand
  `fraction`. The remainder of the cache accesses get the eviction priority specified by
  `.L2::secondary_priority`. This implies that in a memory operation that uses a fraction-based
  cache policy, the memory access has a probability specified by the operand `fraction` of
  getting the cache eviction priority specified by `.L2::primary_priority`.

  The valid range of values for the operand `fraction` is `(0.0,.., 1.0]`. If the operand
  `fraction` is not specified, it defaults to 1.0.

  If `.L2::secondary_priority` is not specified, then it defaults to `.L2::evict_unchanged`.

The access property created using the CUDA APIs can be converted into cache eviction policy by the
instruction `createpolicy.cvt`. The source operand `access-property` is a 64-bit opaque
register. Refer to *CUDA programming guide* for more details.

PTX ISA Notes

Introduced in PTX ISA version 7.4.

Target ISA Notes

Requires `sm_80` or higher.

Examples

```
createpolicy.fractional.L2::evict_last.b64                      policy, 1.0;

createpolicy.fractional.L2::evict_last.L2::evict_unchanged.b64  policy, 0.5;



createpolicy.range.L2::evict_last.L2::evict_first.b64

                                            policy, [ptr], 0x100000, 0x200000;



// access-prop is created by CUDA APIs.

createpolicy.cvt.L2.b64 policy, access-prop;
```

#### 9.7.9.19. [Data Movement and Conversion Instructions: `isspacep`](#data-movement-and-conversion-instructions-isspacep)[](#data-movement-and-conversion-instructions-isspacep "Permalink to this headline")

`isspacep`

Query whether a generic address falls within a specified state space window.

Syntax

```
isspacep.space  p, a;    // result is .pred



.space = { const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };
```

Description

Write predicate register `p` with `1` if generic address a falls within the specified state
space window and with `0` otherwise. Destination `p` has type `.pred`; the source address
operand must be of type `.u32` or `.u64`.

`isspacep.param{::entry}` returns `1` if the generic address falls within the window of
[Kernel Function Parameters](#kernel-function-parameters), otherwise returns `0`. If `.param`
is specified without any sub-qualifiers then it defaults to `.param::entry`.

`isspacep.global` returns `1` for [Kernel Function Parameters](#kernel-function-parameters)
as `.param` window is contained within the `.global`
window.

If no sub-qualifier is specified with `.shared` state space, then `::cta` is assumed by default.

Note

`ispacep.shared::cluster` will return 1 for every shared memory address that is accessible to
the threads in the cluster, whereas `ispacep.shared::cta` will return 1 only if the address is
of a variable declared in the executing CTA.

PTX ISA Notes

Introduced in PTX ISA version 2.0.

`isspacep.const` introduced in PTX ISA version 3.1.

`isspacep.param` introduced in PTX ISA version 7.7.

Support for `::cta` and `::cluster` sub-qualifiers introduced in PTX ISA version 7.8.

Support for sub-qualifier `::entry` on `.param` space introduced in PTX ISA version 8.3.

Target ISA Notes

`isspacep` requires `sm_20` or higher.

`isspacep.param{::entry}` requires `sm_70` or higher.

Sub-qualifier `::cta` requires `sm_30` or higher.

Sub-qualifier `::cluster` requires `sm_90` or higher.

Examples

```
isspacep.const           iscnst, cptr;

isspacep.global          isglbl, gptr;

isspacep.local           islcl,  lptr;

isspacep.shared          isshrd, sptr;

isspacep.param::entry    isparam, pptr;

isspacep.shared::cta     isshrdcta, sptr;

isspacep.shared::cluster ishrdany sptr;
```

#### 9.7.9.20. [Data Movement and Conversion Instructions: `cvta`](#data-movement-and-conversion-instructions-cvta)[](#data-movement-and-conversion-instructions-cvta "Permalink to this headline")

`cvta`

Convert address from `.const`,
[Kernel Function Parameters](#kernel-function-parameters) (`.param`), `.global`, `.local`, or `.shared`
state space to generic, or vice-versa. Take the generic address of a variable declared in
`.const`, [Kernel Function Parameters](#kernel-function-parameters) (`.param`),
`.global`, `.local`, or `.shared` state space.

Syntax

```
// convert const, global, local, or shared address to generic address

cvta.space.size  p, a;        // source address in register a

cvta.space.size  p, var;      // get generic address of var

cvta.space.size  p, var+imm;  // generic address of var+offset



// convert generic address to const, global, local, or shared address

cvta.to.space.size  p, a;



.space = { .const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} };

.size  = { .u32, .u64 };
```

Description

Convert a `const`, [Kernel Function Parameters](#kernel-function-parameters)
(`.param`), `global`, `local`, or `shared` address to a generic address, or vice-versa. The
source and destination addresses must be the same size. Use `cvt.u32.u64` or `cvt.u64.u32` to
truncate or zero-extend addresses.

For variables declared in `.const`,
[Kernel Function Parameters](#kernel-function-parameters) (`.param`), `.global`, `.local`, or `.shared`
state space, the generic address of the variable may be taken using `cvta`. The source is either a
register or a variable defined in `const`,
[Kernel Function Parameters](#kernel-function-parameters) (`.param`), `global`, `local`, or `shared` memory
with an optional offset.

When converting a generic address into a `const`,
[Kernel Function Parameters](#kernel-function-parameters) (`.param`), `global`, `local`, or `shared`
address, the resulting address is undefined in cases where the generic address does not fall within
the address window of the specified state space. A program may use `isspacep` to guard against
such incorrect behavior.

For `cvta` with `.shared` state space, the address must belong to the space specified by
`::cta` or `::cluster` sub-qualifier, otherwise the behavior is undefined. If no sub-qualifier
is specified with `.shared` state space, then `::cta` is assumed by default.

If `.param` is specified without any sub-qualifiers then it defaults to `.param::entry`. For
`.param{::entry}` state space, operand `a` must be a kernel parameter address, otherwise
behavior is undefined.

PTX ISA Notes

Introduced in PTX ISA version 2.0.

`cvta.const` and `cvta.to.const` introduced in PTX ISA version 3.1.

`cvta.param` and `cvta.to.param` introduced in PTX ISA version 7.7.

**Note:** The current implementation does not allow generic pointers to `const` space variables in
programs that contain pointers to constant buffers passed as kernel parameters.

Support for `::cta` and `::cluster` sub-qualifiers introduced in PTX ISA version 7.8.

Support for sub-qualifier `::entry` on `.param` space introduced in PTX ISA version 8.3.

Target ISA Notes

`cvta` requires `sm_20` or higher.

`cvta.param{::entry}` and `cvta.to.param{::entry}` requires `sm_70` or higher.

Sub-qualifier `::cta` requires `sm_30` or higher.

Sub-qualifier `::cluster` requires `sm_90` or higher.

Examples

```
cvta.const.u32   ptr,cvar;

cvta.local.u32   ptr,lptr;

cvta.shared::cta.u32  p,As+4;

cvta.shared::cluster.u32 ptr, As;

cvta.to.global.u32  p,gptr;

cvta.param.u64   ptr,pvar;

cvta.to.param::entry.u64  epptr, ptr;
```

#### 9.7.9.21. [Data Movement and Conversion Instructions: `cvt`](#data-movement-and-conversion-instructions-cvt)[](#data-movement-and-conversion-instructions-cvt "Permalink to this headline")

`cvt`

Convert a value from one type to another.

Syntax

```
cvt{.irnd}{.ftz}{.sat}.dtype.atype         d, a;  // integer rounding

cvt{.frnd}{.ftz}{.sat}.dtype.atype         d, a;  // fp rounding



cvt.frnd2{.relu}{.satfinite}.f16.f32       d, a;

cvt.frnd2{.relu}{.satfinite}.f16x2.f32     d, a, b;

cvt.rs{.relu}{.satfinite}.f16x2.f32        d, a, b, rbits;



cvt.frnd2{.relu}{.satfinite}.bf16.f32      d, a;

cvt.frnd2{.relu}{.satfinite}.bf16x2.f32    d, a, b;

cvt.rs{.relu}{.satfinite}.bf16x2.f32       d, a, b, rbits;



cvt.rna{.satfinite}.tf32.f32               d, a;

cvt.frnd2{.satfinite}{.relu}.tf32.f32      d, a;



cvt.rn.satfinite{.relu}.f8x2type.f32       d, a, b;

cvt.rn.satfinite{.relu}.f8x2type.f16x2     d, a;

cvt.rn.{.relu}.f16x2.f8x2type              d, a;

cvt.rs{.relu}.satfinite.f8x4type.f32       d, {a, b, e, f}, rbits;



cvt.rn.satfinite{.relu}.f4x2type.f32       d, a, b;

cvt.rn{.relu}.f16x2.f4x2type               d, a;

cvt.rs{.relu}.satfinite.f4x4type.f32       d, {a, b, e, f}, rbits;



cvt.rn.satfinite{.relu}.f6x2type.f32       d, a, b;

cvt.rn{.relu}.f16x2.f6x2type               d, a;

cvt.rs{.relu}.satfinite.f6x4type.f32       d, {a, b, e, f}, rbits;



cvt.frnd3{.satfinite}.ue8m0x2.f32          d, a, b;

cvt.frnd3{.satfinite}.ue8m0x2.bf16x2       d, a;

cvt.rn.bf16x2.ue8m0x2                      d, a;



.irnd   = { .rni, .rzi, .rmi, .rpi };

.frnd   = { .rn,  .rz,  .rm,  .rp  };

.frnd2  = { .rn,  .rz };

.frnd2  = { .rn,  .rz };

.frnd3  = { .rz,  .rp };

.dtype = .atype = { .u8,   .u16, .u32, .u64,

                    .s8,   .s16, .s32, .s64,

                    .bf16, .f16, .f32, .f64 };

.f8x2type = { .e4m3x2, .e5m2x2 };

.f4x2type = { .e2m1x2 };

.f6x2type = { .e2m3x2, .e3m2x2 };

.f4x4type = { .e2m1x4 };

.f8x4type = { .e4m3x4, .e5m2x4 };

.f6x4type = { .e2m3x4, .e3m2x4 };
```

Description

Convert between different types and sizes.

For `.f16x2` and `.bf16x2` instruction type, two inputs `a` and `b` of `.f32` type are
converted into `.f16` or `.bf16` type and the converted values are packed in the destination
register `d`, such that the value converted from input `a` is stored in the upper half of `d`
and the value converted from input `b` is stored in the lower half of `d`

For `.f16x2` instruction type, destination operand `d` has `.f16x2` or `.b32` type. For
`.bf16` instruction type, operand `d` has `.b16` type. For `.bf16x2` instruction type,
operand `d` has `.b32` type. For `.tf32` instruction type, operand `d` has `.b32` type.

When converting to `.e4m3x2`/`.e5m2x2` data formats, the destination operand `d` has `.b16`
type. When converting two `.f32` inputs to `.e4m3x2`/`.e5m2x2`, each input is converted to the
specified format, and the converted values are packed in the destination operand `d` such that the
value converted from input `a` is stored in the upper 8 bits of `d` and the value converted from
input `b` is stored in the lower 8 bits of `d`. When converting an `.f16x2` input to
`.e4m3x2`/ `.e5m2x2`, each `.f16` input from operand `a` is converted to the specified
format. The converted values are packed in the destination operand `d` such that the value
converted from the upper 16 bits of input `a` is stored in the upper 8 bits of `d` and the value
converted from the lower 16 bits of input `a` is stored in the lower 8 bits of `d`.

When converting from `.e4m3x2`/`.e5m2x2` to `.f16x2`, source operand `a` has `.b16`
type. Each 8-bit input value in operand `a` is converted to `.f16` type. The converted values
are packed in the destination operand `d` such that the value converted from the upper 8 bits of
`a` is stored in the upper 16 bits of `d` and the value converted from the lower 8 bits of `a`
is stored in the lower 16 bits of `d`.

When converting to `.e2m1x2` data formats, the destination operand `d` has `.b8` type.
When converting two `.f32` inputs to `.e2m1x2`, each input is converted to the specified format,
and the converted values are packed in the destination operand `d` such that the value converted
from input `a` is stored in the upper 4 bits of `d` and the value converted from input `b` is
stored in the lower 4 bits of `d`.

When converting from `.e2m1x2` to `.f16x2`, source operand `a` has `.b8` type. Each 4-bit
input value in operand `a` is converted to `.f16` type. The converted values are packed in the
destination operand `d` such that the value converted from the upper 4 bits of `a` is stored in
the upper 16 bits of `d` and the value converted from the lower 4 bits of `a` is stored in the
lower 16 bits of `d`.

When converting to `.e2m1x4` data format, the destination operand `d` has `.b16` type. When
converting four `.f32` inputs to `.e2m1x4`, each input is converted to the specified format,
and the converted values are packed in the destination operand `d` such that the value converted
from inputs `a`, `b`, `e`, `f` are stored in each 4 bits starting from upper bits of `d`.

When converting to `.e2m3x2`/`.e3m2x2` data formats, the destination operand `d` has `.b16`
type. When converting two `.f32` inputs to `.e2m3x2`/`.e3m2x2`, each input is converted to the
specified format, and the converted values are packed in the destination operand `d` such that the
value converted from input `a` is stored in the upper 8 bits of `d` with 2 MSB bits padded with
zeros and the value converted from input `b` is stored in the lower 8 bits of `d` with 2 MSB bits
padded with zeros.

When converting from `.e2m3x2`/`.e3m2x2` to `.f16x2`, source operand `a` has `.b16` type.
Each 8-bit input value with 2 MSB bits 0 in operand `a` is converted to `.f16` type. The converted
values are packed in the destination operand `d` such that the value converted from the upper 8 bits
of `a` is stored in the upper 16 bits of `d` and the value converted from the lower 8 bits of `a`
is stored in the lower 16 bits of `d`.

When converting to `.e5m2x4`/`.e4m3x4`/`.e3m2x4`/`.e2m3x4` data format, the destination
operand `d` has `.b32` type. When converting four `.f32` inputs to
`.e5m2x4`/`.e4m3x4`/`.e3m2x4`/`.e2m3x4`, each input is converted to the specified format,
and the converted values are packed in the destination operand `d` such that the value converted
from inputs `a`, `b`, `e`, `f` are stored in each 8 bits starting from upper bits of `d`.
For `.e3m2x4`/`.e2m3x4`, each 8-bit output will have 2 MSB bits padded with zeros.

When converting to `.ue8m0x2` data formats, the destination operand `d` has `.b16` type. When
converting two `.f32` or two packed `.bf16` inputs to `.ue8m0x2`, each input is converted to the
specified format, and the converted values are packed in the destination operand `d` such that the
value converted from input `a` is stored in the upper 8 bits of `d` and the value converted from
input `b` is stored in the lower 8 bits of `d`.

When converting from `.ue8m0x2` to `.bf16x2`, source operand `a` has `.b16` type. Each 8-bit
input value in operand `a` is converted to `.bf16` type. The converted values are packed in the
destination operand `d` such that the value converted from the upper 8 bits of `a` is stored in
the upper 16 bits of `d` and the value converted from the lower 8 bits of `a` is stored in the
lower 16 bits of `d`.

`rbits` is a `.b32` type register operand used for providing random bits for `.rs` rounding mode.

When converting to `.f16x2`, two 16-bit values are provided from `rbits` where 13 LSBs from
upper 16-bits are used as random bits for operand `a` with 3 MSBs are 0 and 13 LSBs from lower
16-bits are used as random bits for operand `b` with 3 MSBs are 0.

When converting to `.bf16x2`, two 16-bit values are provided from `rbits` where upper 16-bits
are used as random bits for operand `a` and lower 16-bits are used as random bits for operand `b`.

When converting to `.e4m3x4`/`.e5m2x4`/`.e2m3x4`/`.e3m2x4`, two 16-bit values are provided
from `rbits` where lower 16-bits are used for operands `e`, `f` and upper 16 bits are used
for operands `a`, `b`.

When converting to `.e2m1x4`, two 16-bit values are provided from `rbits` where lower 8-bits
from both 16-bits half of `rbits` are used for operands `e`, `f` and upper 8-bits from both
16-bits half of `rbits` are used for operands `a`, `b`.

Rounding modifier is mandatory in all of the following cases:

* float-to-float conversions, when destination type is smaller than source type
* All float-to-int conversions
* All int-to-float conversions
* All conversions involving `.f16x2`, `.e4m3x2, .e5m2x2,`, `.bf16x2`, `.tf32`, `.e2m1x2`,
  `.e2m3x2`, `.e3m2x2`, `.e4m3x4`, `.e5m2x4`, `.e2m1x4`, `.e2m3x4`, `.e3m2x4` and
  `.ue8m0x2` instruction types.

`.satfinite` modifier is only supported for conversions involving the following types:

* `.e4m3x2`, `.e5m2x2`, `.e2m1x2`, `.e2m3x2`, `.e3m2x2`, `.e4m3x4`, `.e5m2x4`,
  `.e2m1x4`, `.e2m3x4`, `.e3m2x4` destination types.
  `.satfinite` modifier is mandatory for such conversions.
* `.f16`, `.bf16`, `.f16x2`, `.bf16x2`, `.tf32`, `.ue8m0x2` as destination types.

Semantics

```
if (/* inst type is .f16x2 or .bf16x2 */) {

    d[31:16] = convert(a);

    d[15:0]  = convert(b);

} else if (/* inst destination type is .e5m2x2 or .e4m3x2 or .ue8m0x2 */) {

    d[15:8] = convert(a);

    d[7:0]  = convert(b);

} else if (/* inst destination type is .e2m1x2 */) {

    d[7:4] = convert(a);

    d[3:0] = convert(b);

} else if (/* inst destination type is .e2m3x2 or .e3m2x2 */) {

    d[15:14] = 0;

    d[13:8] = convert(a);

    d[7:6] = 0;

    d[5:0] = convert(b);

} else if (/* inst destination type is .e2m1x4 */) {

    d[15:12] = convert(a);

    d[11:8] = convert(b);

    d[7:4] = convert(e);

    d[3:0] = convert(f);

} else if (/* inst destination type is .e4m3x4 or .e5m2x4 */) {

    d[31:24] = convert(a);

    d[23:16] = convert(b);

    d[15:8] = convert(e);

    d[7:0] = convert(f);

} else if (/* inst destination type is .e2m3x4 or .e3m2x4 */) {

    d[31:30] = 0;

    d[29:24] = convert(a);

    d[23:22] = 0;

    d[21:16] = convert(b);

    d[15:14] = 0;

    d[13:8] = convert(e);

    d[7:6] = 0;

    d[5:0] = convert(f);

} else {

    d = convert(a);

}
```

// Random bits `rbits` semantics for `.rs` rounding:

1. Destination type `.f16`:
   Refer [Figure 38](#cvt-rs-rbits-layout-f16) for random bits layout details.

   ![_images/cvt-rs-rbits-layout-f16.png](_images/cvt-rs-rbits-layout-f16.png)


   Figure 38 Random bits layout for `.rs` rounding with `.f16` destination type[](#cvt-rs-rbits-layout-f16 "Permalink to this image")
2. Destination type `.bf16`:
   Refer [Figure 39](#cvt-rs-rbits-layout-bf16) for random bits layout details.

   ![_images/cvt-rs-rbits-layout-bf16.png](_images/cvt-rs-rbits-layout-bf16.png)


   Figure 39 Random bits layout for `.rs` rounding with `.bf16` destination type[](#cvt-rs-rbits-layout-bf16 "Permalink to this image")
3. Destination type `.e2m1x4`:
   Refer [Figure 40](#cvt-rs-rbits-layout-f4) for random bits layout details.

   ![_images/cvt-rs-rbits-layout-f4.png](_images/cvt-rs-rbits-layout-f4.png)


   Figure 40 Random bits layout for `.rs` rounding with `.e2m1x4` destination type[](#cvt-rs-rbits-layout-f4 "Permalink to this image")
4. Destination type `.e5m2x4`, `.e4m3x4`, `.e3m2x4`, `.e2m3x4`:
   Refer [Figure 41](#cvt-rs-rbits-layout-f8-f6) for random bits layout details.

   ![_images/cvt-rs-rbits-layout-f8-f6.png](_images/cvt-rs-rbits-layout-f8-f6.png)


   Figure 41 Random bits layout for `.rs` rounding with `.e5m2x4`/`.e4m3x4`/`.e3m2x4`/`.e2m3x4` destination type[](#cvt-rs-rbits-layout-f8-f6 "Permalink to this image")

Integer Notes

Integer rounding is required for float-to-integer conversions, and for same-size float-to-float
conversions where the value is rounded to an integer. Integer rounding is illegal in all other
instances.

Integer rounding modifiers:

`.rni`
:   round to nearest integer, choosing even integer if source is equidistant between two integers

`.rzi`
:   round to nearest integer in the direction of zero

`.rmi`
:   round to nearest integer in direction of negative infinity

`.rpi`
:   round to nearest integer in direction of positive infinity

In float-to-integer conversions, depending upon conversion types, `NaN` input results in following
value:

1. Zero if source is not `.f64` and destination is not `.s64`, `.u64`.
2. Otherwise 1 << (BitWidth(dst) - 1) corresponding to the value of (`MAXINT` >> 1) + 1 for unsigned type
   or `MININT` for signed type.

Subnormal numbers:

`sm_20+`
:   By default, subnormal numbers are supported.

    For `cvt.ftz.dtype.f32` float-to-integer conversions and `cvt.ftz.f32.f32` float-to-float
    conversions with integer rounding, subnormal inputs are flushed to sign-preserving zero. Modifier
    `.ftz` can only be specified when either `.dtype` or `.atype` is `.f32` and applies only
    to single precision (`.f32`) inputs and results.

`sm_1x`
:   For `cvt.ftz.dtype.f32` float-to-integer conversions and `cvt.ftz.f32.f32`
    float-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving
    zero. The optional `.ftz` modifier may be specified in these cases for clarity.

    **Note:** In PTX ISA versions 1.4 and earlier, the `cvt` instruction did not flush single-precision
    subnormal inputs or results to zero if the destination type size was 64-bits. The compiler will
    preserve this behavior for legacy PTX code.

Saturation modifier:

`.sat`
:   For integer destination types, `.sat` limits the result to `MININT..MAXINT` for the size of
    the operation. Note that saturation applies to both signed and unsigned integer types.

    The saturation modifier is allowed only in cases where the destination type’s value range is not
    a superset of the source type’s value range; i.e., the `.sat` modifier is illegal in cases
    where saturation is not possible based on the source and destination types.

    For float-to-integer conversions, the result is clamped to the destination range by default; i.e,
    `.sat` is redundant.

Floating Point Notes

Floating-point rounding is required for float-to-float conversions that result in loss of precision,
and for integer-to-float conversions. Floating-point rounding is illegal in all other instances.

Floating-point rounding modifiers:

`.rn`
:   rounding to nearest, with ties to even

`.rna`
:   rounding to nearest, with ties away from zero

`.rz`
:   rounding toward zero

`.rm`
:   rounding toward negative infinity

`.rp`
:   rounding toward positive infinity

`.rs`
:   Stochastic rounding is achieved through the use of the supplied random bits. Operation’s result
    is rounded in the direction toward zero or away from zero based on the carry out of the integer
    addition of the supplied random bits (`rbits`) to the truncated off (discarded) bits of
    mantissa from the input.

A floating-point value may be rounded to an integral value using the integer rounding modifiers (see
Integer Notes). The operands must be of the same size. The result is an integral value, stored in
floating-point format.

Subnormal numbers:

`sm_20+`
:   By default, subnormal numbers are supported. Modifier `.ftz` may be specified to flush
    single-precision subnormal inputs and results to sign-preserving zero. Modifier `.ftz` can only
    be specified when either `.dtype` or `.atype` is `.f32` and applies only to single
    precision (`.f32`) inputs and results.

`sm_1x`
:   Single-precision subnormal inputs and results are flushed to sign-preserving zero. The optional
    `.ftz` modifier may be specified in these cases for clarity.

**Note:** In PTX ISA versions 1.4 and earlier, the `cvt` instruction did not flush
single-precision subnormal inputs or results to zero if either source or destination type was
`.f64`. The compiler will preserve this behavior for legacy PTX code. Specifically, if the PTX
ISA version is 1.4 or earlier, single-precision subnormal inputs and results are flushed to
sign-preserving zero only for `cvt.f32.f16`, `cvt.f16.f32`, and `cvt.f32.f32` instructions.

Saturation modifier:

`.sat`:
:   For floating-point destination types, `.sat` limits the result to the range [0.0, 1.0]. `NaN`
    results are flushed to positive zero. Applies to `.f16`, `.f32`, and `.f64` types.

`.relu`:
:   For `.f16`, `.f16x2`, `.bf16`, `.bf16x2`, `.e4m3x2`, `.e5m2x2`, `.e2m1x2`, `.e2m3x2`,
    `.e3m2x2`, `.e4m3x4`, `.e5m2x4`, `.e2m1x4`, `.e2m3x4`, `.e3m2x4` and `.tf32`
    destination types, `.relu` clamps the result to 0 if negative. `NaN` results are converted
    to canonical `NaN`.

`.satfinite`:
:   For `.f16`, `.f16x2`, `.bf16`, `.bf16x2`, `.e4m3x2`, `.e5m2x2`, `.ue8m0x2`, `.e4m3x4`,
    `.e5m2x4` and `.tf32` destination formats, if the input value is `NaN`, then the result is
    `NaN` in the specified destination format. For `.e2m1x2`, `.e2m3x2`, `.e3m2x2`, `.e2m1x4`,
    `.e2m3x4`, `.e3m2x4` destination formats `NaN` results are converted to positive *MAX\_NORM*.
    If the absolute value of input (ignoring sign) is greater than *MAX\_NORM* of the specified destination
    format, then the result is sign-preserved *MAX\_NORM* of the destination format and a positive
    *MAX\_NORM* in `.ue8m0x2` for which the destination sign is not supported.

Notes

A source register wider than the specified type may be used, except when the source operand has
`.bf16` or `.bf16x2` format. The lower `n` bits corresponding to the instruction-type width
are used in the conversion. See
[Operand Size Exceeding Instruction-Type Size](#operand-size-exceeding-instruction-type-size) for a description of these relaxed
type-checking rules.

A destination register wider than the specified type may be used, except when the destination
operand has `.bf16`, `.bf16x2` or `.tf32` format. The result of conversion is sign-extended to
the destination register width for signed integers, and is zero-extended to the destination register
width for unsigned, bit-size, and floating-point types. See
[Operand Size Exceeding Instruction-Type Size](#operand-size-exceeding-instruction-type-size) for a description of these relaxed
type-checking rules.

For `cvt.f32.bf16`, `NaN` input yields unspecified `NaN`.

PTX ISA Notes

Introduced in PTX ISA version 1.0.

`.relu` modifier and {`.f16x2`, `.bf16`, `.bf16x2`, `.tf32`} destination formats
introduced in PTX ISA version 7.0.

`cvt.f32.bf16` introduced in PTX ISA version 7.1.

`cvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16}`,
`cvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16`, and `cvt.tf32.f32.{relu}.{rn/rz}` introduced
in PTX ISA version 7.8.

`.ftz` qualifier for `cvt.f32.bf16` introduced in PTX ISA version 7.8.

`cvt` with `.e4m3x2`/`.e5m2x2` for `sm_90` or higher introduced in PTX ISA version 7.8.

`cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2}` for `sm_90` or higher introduced in PTX ISA version 7.8.

`cvt` with `.e4m3x2`/`.e5m2x2` for `sm_89` introduced in PTX ISA version 8.1.

`cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2}` for `sm_89` introduced in PTX ISA version 8.1.

`cvt.satfinite.{f16, bf16, f16x2, bf16x2, tf32}.f32` introduced in PTX ISA version 8.1.

`cvt.{rn/rz}.satfinite.tf32.f32` introduced in PTX ISA version 8.6.

`cvt.rn.satfinite{.relu}.{e2m1x2/e2m3x2/e3m2x2/ue8m0x2}.f32` introduced in PTX ISA version 8.6.

`cvt.rn{.relu}.f16x2.{e2m1x2/e2m3x2/e3m2x2}` introduced in PTX ISA version 8.6.

`cvt.{rp/rz}{.satfinite}{.relu}.ue8m0x2.bf16x2` introduced in PTX ISA version 8.6.

`cvt.{rz/rp}.satfinite.ue8m0x2.f32` introduced in PTX ISA version 8.6.

`cvt.rn.bf16x2.ue8m0x2` introduced in PTX ISA version 8.6.

`.rs` rounding mode introduced in PTX ISA version 8.7.

`cvt.rs{.e2m1x4/.e4m3x4/.e5m2x4/.e3m2x4/.e2m3x4}.f32` introduced in PTX ISA version 8.7.

Target ISA Notes

`cvt` to or from `.f64` requires `sm_13` or higher.

`.relu` modifier and {`.f16x2`, `.bf16`, `.bf16x2`, `.tf32`} destination formats require
`sm_80` or higher.

`cvt.f32.bf16` requires `sm_80` or higher.

`cvt.bf16.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16}`,
`cvt.{u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16`, and `cvt.tf32.f32.{relu}.{rn/rz}` require
`sm_90` or higher.

`.ftz` qualifier for `cvt.f32.bf16` requires `sm_90` or higher.

`cvt` with `.e4m3x2`/`.e5m2x2` requires `sm89` or higher.

`cvt.satfinite.{e4m3x2, e5m2x2}.{f32, f16x2}` requires `sm_89` or higher.

`cvt.{rn/rz}.satfinite.tf32.f32` requires `sm_100` or higher.

`cvt.rn.satfinite{.relu}.{e2m1x2/e2m3x2/e3m2x2/ue8m0x2}.f32` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_120a`
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
  + `sm_120f` or higher in the same family
* `sm_110f` or higher in the same family

`cvt.rn{.relu}.f16x2.{e2m1x2/e2m3x2/e3m2x2}` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_120a`
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
  + `sm_120f` or higher in the same family
* `sm_110f` or higher in the same family

`cvt.{rz/rp}{.satfinite}{.relu}.ue8m0x2.bf16x2` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_120a`
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
  + `sm_120f` or higher in the same family
* `sm_110f` or higher in the same family

`cvt.{rz/rp}.satfinite.ue8m0x2.f32` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_120a`
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
  + `sm_120f` or higher in the same family
* `sm_110f` or higher in the same family

`cvt.rn.bf16x2.ue8m0x2` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_120a`
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
  + `sm_120f` or higher in the same family
* `sm_110f` or higher in the same family

`.rs` rounding mode is supported on following architectures:

* `sm_100a`
* `sm_103a`

`cvt.rs{.e2m1x4/.e4m3x4/.e5m2x4/.e3m2x4/.e2m3x4}.f32` is supported on following architectures:

* `sm_100a`
* `sm_103a`

Examples

```
cvt.f32.s32 f,i;

cvt.s32.f64 j,r;     // float-to-int saturates by default

cvt.rni.f32.f32 x,y; // round to nearest int, result is fp

cvt.f32.f32 x,y;     // note .ftz behavior for sm_1x targets

cvt.rn.relu.f16.f32      b, f;        // result is saturated with .relu saturation mode

cvt.rz.f16x2.f32         b1, f, f1;   // convert two fp32 values to packed fp16 outputs

cvt.rn.relu.satfinite.f16x2.f32    b1, f, f1;   // convert two fp32 values to packed fp16 outputs with .relu saturation on each output

cvt.rn.bf16.f32          b, f;        // convert fp32 to bf16

cvt.rz.relu.satfinite.bf16.f3 2    b, f;        // convert fp32 to bf16 with .relu and .satfinite saturation

cvt.rz.satfinite.bf16x2.f32        b1, f, f1;   // convert two fp32 values to packed bf16 outputs

cvt.rn.relu.bf16x2.f32   b1, f, f1;   // convert two fp32 values to packed bf16 outputs with .relu saturation on each output

cvt.rna.satfinite.tf32.f32         b1, f;       // convert fp32 to tf32 format

cvt.rn.relu.tf32.f32     d, a;        // convert fp32 to tf32 format

cvt.f64.bf16.rp          f, b;        // convert bf16 to f64 format

cvt.bf16.f16.rz          b, f         // convert f16 to bf16 format

cvt.bf16.u64.rz          b, u         // convert u64 to bf16 format

cvt.s8.bf16.rpi          s, b         // convert bf16 to s8 format

cvt.bf16.bf16.rpi        b1, b2       // convert bf16 to corresponding int represented in bf16 format

cvt.rn.satfinite.e4m3x2.f32 d, a, b;  // convert a, b to .e4m3 and pack as .e4m3x2 output

cvt.rn.relu.satfinite.e5m2x2.f16x2 d, a; // unpack a and convert the values to .e5m2 outputs with .relu

                                         // saturation on each output and pack as .e5m2x2

cvt.rn.f16x2.e4m3x2 d, a;             // unpack a, convert two .e4m3 values to packed f16x2 output

cvt.rn.satfinite.tf32.f32 d, a;       // convert fp32 to tf32 format

cvt.rn.relu.f16x2.e2m1x2 d, a;        // unpack a, convert two .e2m1 values to packed f16x2 output

cvt.rn.satfinite.e2m3x2.f32 d, a, b;  // convert a, b to .e2m3 and pack as .e2m3x2 output

cvt.rn.relu.f16x2.e3m2x2 d, a;        // unpack a, convert two .e3m2 values to packed f16x2 output



cvt.rs.f16x2.f32    d, a, b, rbits;  // convert 2 fp32 values to packed fp16 with applying .rs rounding

cvt.rs.satfinite.e2m1x4.f32  d, {a, b, e, f}, rbits; // convert 4 fp32 values to packed 4 e2m1 values with applying .rs rounding
```

#### 9.7.9.22. [Data Movement and Conversion Instructions: `cvt.pack`](#data-movement-and-conversion-instructions-cvt-pack)[](#data-movement-and-conversion-instructions-cvt-pack "Permalink to this headline")

`cvt.pack`

Convert two integer values from one integer type to another and pack the results.

Syntax

```
cvt.pack.sat.convertType.abType  d, a, b;

    .convertType  = { .u16, .s16 }

    .abType       = { .s32 }



cvt.pack.sat.convertType.abType.cType  d, a, b, c;

    .convertType  = { .u2, .s2, .u4, .s4, .u8, .s8 }

    .abType       = { .s32 }

    .cType        = { .b32 }
```

Description

Convert two 32-bit integers `a` and `b` into specified type and pack the results into `d`.

Destination `d` is an unsigned 32-bit integer. Source operands `a` and `b` are integers of
type `.abType` and the source operand `c` is an integer of type `.cType`.

The inputs `a` and `b` are converted to values of type specified by `.convertType` with
saturation and the results after conversion are packed into lower bits of `d`.

If operand `c` is specified then remaining bits of `d` are copied from lower bits of `c`.

Semantics

```
ta = a < MIN(convertType) ? MIN(convertType) : a;

ta = a > MAX(convertType) ? MAX(convertType) : a;

tb = b < MIN(convertType) ? MIN(convertType) : b;

tb = b > MAX(convertType) ? MAX(convertType) : b;



size = sizeInBits(convertType);

td = tb ;

for (i = size; i <= 2 * size - 1; i++) {

    td[i] = ta[i - size];

}



if (isU16(convertType) || isS16(convertType)) {

    d = td;

} else {

    for (i = 0; i < 2 * size; i++) {

        d[i] = td[i];

    }

    for (i = 2 * size; i <= 31; i++) {

        d[i] = c[i - 2 * size];

    }

}
```

`.sat` modifier limits the converted values to `MIN(convertType)`.. `MAX(convertedType)` (no
overflow) if the corresponding inputs are not in the range of datatype specified as
`.convertType`.

PTX ISA Notes

Introduced in PTX ISA version 6.5.

Target ISA Notes

Requires `sm_72` or higher.

Sub byte types (`.u4`/`.s4` and `.u2`/`.s2`) requires `sm_75` or higher.

Examples

```
cvt.pack.sat.s16.s32      %r1, %r2, %r3;           // 32-bit to 16-bit conversion

cvt.pack.sat.u8.s32.b32   %r4, %r5, %r6, 0;        // 32-bit to 8-bit conversion

cvt.pack.sat.u8.s32.b32   %r7, %r8, %r9, %r4;      // %r7 = { %r5, %r6, %r8, %r9 }

cvt.pack.sat.u4.s32.b32   %r10, %r12, %r13, %r14;  // 32-bit to 4-bit conversion

cvt.pack.sat.s2.s32.b32   %r15, %r16, %r17, %r18;  // 32-bits to 2-bit conversion
```

#### 9.7.9.23. [Data Movement and Conversion Instructions: `mapa`](#data-movement-and-conversion-instructions-mapa)[](#data-movement-and-conversion-instructions-mapa "Permalink to this headline")

`mapa`

Map the address of the shared variable in the target CTA.

Syntax

```
mapa{.space}.type          d, a, b;



// Maps shared memory address in register a into CTA b.

mapa.shared::cluster.type  d, a, b;



// Maps shared memory variable into CTA b.

mapa.shared::cluster.type  d, sh, b;



// Maps shared memory variable into CTA b.

mapa.shared::cluster.type  d, sh + imm, b;



// Maps generic address in register a into CTA b.

mapa.type                  d, a, b;



.space = { .shared::cluster }

.type  = { .u32, .u64 }
```

Description

Get address in the CTA specified by operand `b` which corresponds to the address specified by
operand `a`.

Instruction type `.type` indicates the type of the destination operand `d` and the source
operand `a`.

When space is `.shared::cluster`, source `a` is either a shared memory variable or a register
containing a valid shared memory address and register `d` contains a shared memory address. When
the optional qualifier `.space` is not specified, both `a` and `d` are registers containing
generic addresses pointing to shared memory.

`b` is a 32-bit integer operand representing the rank of the target CTA.

Destination register `d` will hold an address in CTA `b` corresponding to operand `a`.

PTX ISA Notes

Introduced in PTX ISA version 7.8.

Target ISA Notes

Requires `sm_90` or higher.

Examples

```
mapa.shared::cluster.u64 d1, %reg1, cta;

mapa.shared::cluster.u32 d2, sh, 3;

mapa.u64                 d3, %reg2, cta;
```

#### 9.7.9.24. [Data Movement and Conversion Instructions: `getctarank`](#data-movement-and-conversion-instructions-getctarank)[](#data-movement-and-conversion-instructions-getctarank "Permalink to this headline")

`getctarank`

Generate the CTA rank of the address.

Syntax

```
getctarank{.space}.type d, a;



// Get cta rank from source shared memory address in register a.

getctarank.shared::cluster.type d, a;



// Get cta rank from shared memory variable.

getctarank.shared::cluster.type d, var;



// Get cta rank from shared memory variable+offset.

getctarank.shared::cluster.type d, var + imm;



// Get cta rank from generic address of shared memory variable in register a.

getctarank.type d, a;



.space = { .shared::cluster }

.type  = { .u32, .u64 }
```

Description

Write the destination register `d` with the rank of the CTA which contains the address specified
in operand `a`.

Instruction type `.type` indicates the type of source operand `a`.

When space is `.shared::cluster`, source `a` is either a shared memory variable or a register
containing a valid shared memory address. When the optional qualifier `.space` is not specified,
`a` is a register containing a generic addresses pointing to shared memory. Destination `d` is
always a 32-bit register which holds the rank of the CTA.

PTX ISA Notes

Introduced in PTX ISA version 7.8.

Target ISA Notes

Requires `sm_90` or higher.

Examples

```
getctarank.shared::cluster.u32 d1, addr;

getctarank.shared::cluster.u64 d2, sh + 4;

getctarank.u64                 d3, src;
```

#### 9.7.9.25. [Data Movement and Conversion Instructions: Asynchronous copy](#data-movement-and-conversion-instructions-asynchronous-copy)[](#data-movement-and-conversion-instructions-asynchronous-copy "Permalink to this headline")

An asynchronous copy operation performs the underlying operation asynchronously in the background,
thus allowing the issuing threads to perform subsequent tasks.

An asynchronous copy operation can be a *bulk* operation that operates on a large amount of data, or
a *non-bulk* operation that operates on smaller sized data. The amount of data handled by a bulk
asynchronous operation must be a multiple of 16 bytes.

An asynchronous copy operation typically includes the following sequence:

* Optionally, reading from the tensormap.
* Reading data from the source location(s).
* Writing data to the destination location(s).
* Writes being made visible to the executing thread or other threads.

##### 9.7.9.25.1. [Completion Mechanisms for Asynchronous Copy Operations](#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms)[](#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms "Permalink to this headline")

A thread must explicitly wait for the completion of an asynchronous copy operation in order to
access the result of the operation. Once an asynchronous copy operation is initiated, modifying the
source memory location or tensor descriptor or reading from the destination memory location before
the asynchronous operation completes, exhibits undefined behavior.

This section describes two asynchronous copy operation completion mechanisms supported in PTX:
Async-group mechanism and mbarrier-based mechanism.

Asynchronous operations may be tracked by either of the completion mechanisms or both mechanisms.
The tracking mechanism is instruction/instruction-variant specific.

###### 9.7.9.25.1.1. [Async-group mechanism](#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms-async-group)[](#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms-async-group "Permalink to this headline")

When using the async-group completion mechanism, the issuing thread specifies a group of
asynchronous operations, called *async-group*, using a *commit* operation and tracks the completion
of this group using a *wait* operation. The thread issuing the asynchronous operation must create
separate *async-groups* for bulk and non-bulk asynchronous operations.

A *commit* operation creates a per-thread *async-group* containing all prior asynchronous operations
tracked by *async-group* completion and initiated by the executing thread but none of the asynchronous
operations following the commit operation. A committed asynchronous operation belongs to a single
*async-group*.

When an *async-group* completes, all the asynchronous operations belonging to that group are
complete and the executing thread that initiated the asynchronous operations can read the result of
the asynchronous operations. All *async-groups* committed by an executing thread always complete in
the order in which they were committed. There is no ordering between asynchronous operations within
an *async-group*.

A typical pattern of using *async-group* as the completion mechanism is as follows:

* Initiate the asynchronous operations.
* Group the asynchronous operations into an *async-group* using a *commit* operation.
* Wait for the completion of the async-group using the wait operation.
* Once the *async-group* completes, access the results of all asynchronous operations in that
  *async-group*.

###### 9.7.9.25.1.2. [Mbarrier-based mechanism](#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms-mbarrier)[](#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms-mbarrier "Permalink to this headline")

A thread can track the completion of one or more asynchronous operations using the current phase of
an *mbarrier object*. When the current phase of the *mbarrier object* is complete, it implies that
all asynchronous operations tracked by this phase are complete, and all threads participating in
that *mbarrier object* can access the result of the asynchronous operations.

The *mbarrier object* to be used for tracking the completion of an asynchronous operation can be
either specified along with the asynchronous operation as part of its syntax, or as a separate
operation. For a bulk asynchronous operation, the *mbarrier object* must be specified in the
asynchronous operation, whereas for non-bulk operations, it can be specified after the asynchronous
operation.

A typical pattern of using mbarrier-based completion mechanism is as follows:

* Initiate the asynchronous operations.
* Set up an *mbarrier object* to track the asynchronous operations in its current phase, either as
  part of the asynchronous operation or as a separate operation.
* Wait for the *mbarrier object* to complete its current phase using `mbarrier.test_wait` or
  `mbarrier.try_wait`.
* Once the `mbarrier.test_wait` or `mbarrier.try_wait` operation returns `True`, access the
  results of the asynchronous operations tracked by the *mbarrier object*.

##### 9.7.9.25.2. [Async Proxy](#async-proxy)[](#async-proxy "Permalink to this headline")

The `cp{.reduce}.async.bulk` operations are performed in the *asynchronous proxy* (or *async
proxy*).

Accessing the same memory location across multiple proxies needs a cross-proxy fence. For the
*async proxy*, `fence.proxy.async` should be used to synchronize memory between *generic
proxy* and the *async proxy*.

The completion of a `cp{.reduce}.async.bulk` operation is followed by an implicit *generic-async*
proxy fence. So the result of the asynchronous operation is made visible to the generic proxy as
soon as its completion is observed. *Async-group* OR *mbarrier-based* completion mechanism must
be used to wait for the completion of the `cp{.reduce}.async.bulk` instructions.

##### 9.7.9.25.3. [Data Movement and Conversion Instructions: Non-bulk copy](#data-movement-and-conversion-instructions-non-bulk-copy)[](#data-movement-and-conversion-instructions-non-bulk-copy "Permalink to this headline")

###### 9.7.9.25.3.1. [Data Movement and Conversion Instructions: `cp.async`](#data-movement-and-conversion-instructions-cp-async)[](#data-movement-and-conversion-instructions-cp-async "Permalink to this headline")

`cp.async`

Initiates an asynchronous copy operation from one state space to another.

Syntax

```
cp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}

                         [dst], [src], cp-size{, src-size}{, cache-policy} ;

cp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}

                         [dst], [src], 16{, src-size}{, cache-policy} ;

cp.async.ca.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}

                         [dst], [src], cp-size{, ignore-src}{, cache-policy} ;

cp.async.cg.shared{::cta}.global{.level::cache_hint}{.level::prefetch_size}

                         [dst], [src], 16{, ignore-src}{, cache-policy} ;



.level::cache_hint =     { .L2::cache_hint }

.level::prefetch_size =  { .L2::64B, .L2::128B, .L2::256B }

cp-size =                { 4, 8, 16 }
```

Description

`cp.async` is a non-blocking instruction which initiates an asynchronous copy operation of data
from the location specified by source address operand `src` to the location specified by
destination address operand `dst`. Operand `src` specifies a location in the global state space
and `dst` specifies a location in the shared state space.

Operand `cp-size` is an integer constant which specifies the size of data in bytes to be copied to
the destination `dst`. `cp-size` can only be 4, 8 and 16.

Instruction `cp.async` allows optionally specifying a 32-bit integer operand `src-size`. Operand
`src-size` represents the size of the data in bytes to be copied from `src` to `dst` and must
be less than `cp-size`. In such case, remaining bytes in destination `dst` are filled with
zeros. Specifying `src-size` larger than `cp-size` results in undefined behavior.

The optional and non-immediate predicate argument `ignore-src` specifies whether the data from the
source location `src` should be ignored completely. If the source data is ignored then zeros will
be copied to destination `dst`. If the argument `ignore-src` is not specified then it defaults
to `False`.

Supported alignment requirements and addressing modes for operand `src` and `dst` are described
in [Addresses as Operands](#addresses-as-operands).

The mandatory `.async` qualifier indicates that the `cp` instruction will initiate the memory
copy operation asynchronously and control will return to the executing thread before the copy
operation is complete. The executing thread can then use
[async-group based completion mechanism](#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms-async-group)
or the [mbarrier based completion mechanism](#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms-mbarrier)
to wait for completion of the asynchronous copy operation.
No other synchronization mechanism guarantees the completion of the asynchronous
copy operations.

There is no ordering guarantee between two `cp.async` operations if they are not explicitly
synchronized using `cp.async.wait_all` or `cp.async.wait_group` or [mbarrier instructions](#parallel-synchronization-and-communication-instructions-mbarrier).

As described in [Cache Operators](#cache-operators), the `.cg` qualifier indicates
caching of data only at global level cache L2 and not at L1 whereas `.ca` qualifier indicates
caching of data at all levels including L1 cache. Cache operator are treated as performance hints
only.

`cp.async` is treated as a weak memory operation in the [Memory Consistency Model](#memory-consistency-model).

The `.level::prefetch_size` qualifier is a hint to fetch additional data of the specified size
into the respective cache level.The sub-qualifier `prefetch_size` can be set to either of `64B`,
`128B`, `256B` thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes
respectively.

The qualifier `.level::prefetch_size` may only be used with `.global` state space and with
generic addressing where the address points to `.global` state space. If the generic address does
not fall within the address window of the global memory, then the prefetching behavior is undefined.

The `.level::prefetch_size` qualifier is treated as a performance hint only.

When the optional argument `cache-policy` is specified, the qualifier `.level::cache_hint` is
required. The 64-bit operand `cache-policy` specifies the cache eviction policy that may be used
during the memory access.

The qualifier `.level::cache_hint` is only supported for `.global` state space and for generic
addressing where the address points to the `.global` state space.

`cache-policy` is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.

PTX ISA Notes

Introduced in PTX ISA version 7.0.

Support for `.level::cache_hint` and `.level::prefetch_size` qualifiers introduced in PTX ISA
version 7.4.

Support for `ignore-src` operand introduced in PTX ISA version 7.5.

Support for sub-qualifier `::cta` introduced in PTX ISA version 7.8.

Target ISA Notes

Requires `sm_80` or higher.

Sub-qualifier `::cta` requires `sm_30` or higher.

Examples

```
cp.async.ca.shared.global  [shrd],    [gbl + 4], 4;

cp.async.ca.shared::cta.global  [%r0 + 8], [%r1],     8;

cp.async.cg.shared.global  [%r2],     [%r3],     16;



cp.async.cg.shared.global.L2::64B   [%r2],      [%r3],     16;

cp.async.cg.shared.global.L2::128B  [%r0 + 16], [%r1],     16;

cp.async.cg.shared.global.L2::256B  [%r2 + 32], [%r3],     16;



createpolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 0.25;

cp.async.ca.shared.global.L2::cache_hint [%r2], [%r1], 4, cache-policy;



cp.async.ca.shared.global                   [shrd], [gbl], 4, p;

cp.async.cg.shared.global.L2::cache_hint   [%r0], [%r2], 16, q, cache-policy;
```

###### 9.7.9.25.3.2. [Data Movement and Conversion Instructions: `cp.async.commit_group`](#data-movement-and-conversion-instructions-cp-async-commit-group)[](#data-movement-and-conversion-instructions-cp-async-commit-group "Permalink to this headline")

`cp.async.commit_group`

Commits all prior initiated but uncommitted `cp.async` instructions into a *cp.async-group*.

Syntax

```
cp.async.commit_group ;
```

Description

`cp.async.commit_group` instruction creates a new *cp.async-group* per thread and batches all
prior `cp.async` instructions initiated by the executing thread but not committed to any
*cp.async-group* into the new *cp.async-group*. If there are no uncommitted `cp.async`
instructions then `cp.async.commit_group` results in an empty *cp.async-group.*

An executing thread can wait for the completion of all `cp.async` operations in a *cp.async-group*
using `cp.async.wait_group`.

There is no memory ordering guarantee provided between any two `cp.async` operations within the
same *cp.async-group*. So two or more `cp.async` operations within a *cp.async-group* copying data
to the same location results in undefined behavior.

PTX ISA Notes

Introduced in PTX ISA version 7.0.

Target ISA Notes

Requires `sm_80` or higher.

Examples

```
// Example 1:

cp.async.ca.shared.global [shrd], [gbl], 4;

cp.async.commit_group ; // Marks the end of a cp.async group



// Example 2:

cp.async.ca.shared.global [shrd1],   [gbl1],   8;

cp.async.ca.shared.global [shrd1+8], [gbl1+8], 8;

cp.async.commit_group ; // Marks the end of cp.async group 1



cp.async.ca.shared.global [shrd2],    [gbl2],    16;

cp.async.cg.shared.global [shrd2+16], [gbl2+16], 16;

cp.async.commit_group ; // Marks the end of cp.async group 2
```

###### 9.7.9.25.3.3. [Data Movement and Conversion Instructions: `cp.async.wait_group` / `cp.async.wait_all`](#data-movement-and-conversion-instructions-cp-async-wait-group)[](#data-movement-and-conversion-instructions-cp-async-wait-group "Permalink to this headline")

`cp.async.wait_group`, `cp.async.wait_all`

Wait for completion of prior asynchronous copy operations.

Syntax

```
cp.async.wait_group N;

cp.async.wait_all ;
```

Description

`cp.async.wait_group` instruction will cause executing thread to wait till only `N` or fewer of
the most recent *cp.async-group*s are pending and all the prior *cp.async-group*s committed by
the executing threads are complete. For example, when `N` is 0, the executing thread waits on all
the prior *cp.async-group*s to complete. Operand `N` is an integer constant.

`cp.async.wait_all` is equivalent to :

```
cp.async.commit_group;

cp.async.wait_group 0;
```

An empty *cp.async-group* is considered to be trivially complete.

Writes performed by `cp.async` operations are made visible to the executing thread only after:

1. The completion of `cp.async.wait_all` or
2. The completion of `cp.async.wait_group` on the *cp.async-group* in which the `cp.async`
   belongs to or
3. [mbarrier.test\_wait](#parallel-synchronization-and-communication-instructions-mbarrier-test-wait-try-wait)
   returns `True` on an *mbarrier object* which is tracking the completion of the `cp.async`
   operation.

There is no ordering between two `cp.async` operations that are not synchronized with
`cp.async.wait_all` or `cp.async.wait_group` or [mbarrier objects](#parallel-synchronization-and-communication-instructions-mbarrier).

`cp.async.wait_group` and `cp.async.wait_all` does not provide any ordering and visibility
guarantees for any other memory operation apart from `cp.async`.

PTX ISA Notes

Introduced in PTX ISA version 7.0.

Target ISA Notes

Requires `sm_80` or higher.

Examples

```
// Example of .wait_all:

cp.async.ca.shared.global [shrd1], [gbl1], 4;

cp.async.cg.shared.global [shrd2], [gbl2], 16;

cp.async.wait_all;  // waits for all prior cp.async to complete



// Example of .wait_group :

cp.async.ca.shared.global [shrd3], [gbl3], 8;

cp.async.commit_group;  // End of group 1



cp.async.cg.shared.global [shrd4], [gbl4], 16;

cp.async.commit_group;  // End of group 2



cp.async.cg.shared.global [shrd5], [gbl5], 16;

cp.async.commit_group;  // End of group 3



cp.async.wait_group 1;  // waits for group 1 and group 2 to complete
```

##### 9.7.9.25.4. [Data Movement and Conversion Instructions: Bulk copy](#data-movement-and-conversion-instructions-bulk-copy)[](#data-movement-and-conversion-instructions-bulk-copy "Permalink to this headline")

###### 9.7.9.25.4.1. [Data Movement and Conversion Instructions: `cp.async.bulk`](#data-movement-and-conversion-instructions-cp-async-bulk)[](#data-movement-and-conversion-instructions-cp-async-bulk "Permalink to this headline")

`cp.async.bulk`

Initiates an asynchronous copy operation from one state space to another.

Syntax

```
// global -> shared::cta

cp.async.bulk.dst.src.completion_mechanism{.level::cache_hint}

                      [dstMem], [srcMem], size, [mbar] {, cache-policy}



.dst =                  { .shared::cta }

.src =                  { .global }

.completion_mechanism = { .mbarrier::complete_tx::bytes }

.level::cache_hint =    { .L2::cache_hint }





// global -> shared::cluster

cp.async.bulk.dst.src.completion_mechanism{.multicast}{.level::cache_hint}

                      [dstMem], [srcMem], size, [mbar] {, ctaMask} {, cache-policy}



.dst =                  { .shared::cluster }

.src =                  { .global }

.completion_mechanism = { .mbarrier::complete_tx::bytes }

.level::cache_hint =    { .L2::cache_hint }

.multicast =            { .multicast::cluster  }





// shared::cta -> shared::cluster

cp.async.bulk.dst.src.completion_mechanism [dstMem], [srcMem], size, [mbar]



.dst =                  { .shared::cluster }

.src =                  { .shared::cta }

.completion_mechanism = { .mbarrier::complete_tx::bytes }





// shared::cta -> global

cp.async.bulk.dst.src.completion_mechanism{.level::cache_hint}{.cp_mask}

                      [dstMem], [srcMem], size {, cache-policy} {, byteMask}



.dst =                  { .global }

.src =                  { .shared::cta }

.completion_mechanism = { .bulk_group }

.level::cache_hint =    { .L2::cache_hint }
```

Description

`cp.async.bulk` is a non-blocking instruction which initiates an asynchronous bulk-copy operation
from the location specified by source address operand `srcMem` to the location specified by
destination address operand `dstMem`.

The direction of bulk-copy is from the state space specified by the `.src` modifier to the state
space specified by the `.dst` modifiers.

The 32-bit operand `size` specifies the amount of memory to be copied, in terms of number of
bytes. `size` must be a multiple of 16. If the value is not a multiple of 16, then the behavior is
undefined. The memory range `[dstMem, dstMem + size - 1]` must not overflow the destination memory
space and the memory range `[srcMem, srcMem + size - 1]` must not overflow the source memory
space. Otherwise, the behavior is undefined. The addresses `dstMem` and `srcMem` must be aligned
to 16 bytes.

When the destination of the copy is `.shared::cta` the destination address has to be in the shared
memory of the executing CTA within the cluster, otherwise the behavior is undefined.

When the source of the copy is `.shared::cta` and the destination is `.shared::cluster`, the
destination has to be in the shared memory of a different CTA within the cluster.

The modifier `.completion_mechanism` specifies the completion mechanism that is supported on the
instruction variant. The completion mechanisms that are supported for different variants are
summarized in the following table:

| .completion-mechanism | `.dst` | `.src` | Completion mechanism | |
| --- | --- | --- | --- | --- |
| Needed for completion of entire Async operation | optionally can be used for the completion of - Reading data from the source - Reading from the tensormap, if applicable |
| `.mbarrier::...` | `.shared::cta` | `.global` | mbarrier based | *Bulk async-group* based |
| `.shared::cluster` | `.global` |
| `.shared::cluster` | `.shared::cta` |
| `.bulk_group` | `.global` | `.shared::cta` | *Bulk async-group* based |

The modifier `.mbarrier::complete_tx::bytes` specifies that the `cp.async.bulk` variant uses
mbarrier based completion mechanism. The [complete-tx](#parallel-synchronization-and-communication-instructions-mbarrier-complete-tx-operation)
operation, with `completeCount` argument equal to amount of data copied in bytes, will be
performed on the mbarrier object specified by the operand `mbar`.

The modifier `.bulk_group` specifies that the `cp.async.bulk` variant uses *bulk async-group*
based completion mechanism.

The optional modifier `.multicast::cluster` allows copying of data from global memory to shared
memory of multiple CTAs in the cluster. Operand `ctaMask` specifies the destination CTAs in the
cluster such that each bit position in the 16-bit `ctaMask` operand corresponds to the `%ctaid`
of the destination CTA. The source data is multicast to the same CTA-relative offset as `dstMem`
in the shared memory of each destination CTA. The mbarrier signal is also multicast to the same
CTA-relative offset as `mbar` in the shared memory of the destination CTA.

When the optional argument `cache-policy` is specified, the qualifier `.level::cache_hint` is
required. The 64-bit operand `cache-policy` specifies the cache eviction policy that may be used
during the memory access.

`cache-policy` is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program. The
qualifier `.level::cache_hint` is only supported when at least one of the `.src` or `.dst`
statespaces is `.global` state space.

When the optional qualifier `.cp_mask` is specified, the argument `byteMask` is required.
The i-th bit in the 16-bit wide `byteMask` operand specifies whether the i-th byte of each 16-byte
wide chunk of source data is copied to the destination. If the bit is set, the byte is copied.

The copy operation in `cp.async.bulk` is treated as a weak memory operation and the
[complete-tx](#parallel-synchronization-and-communication-instructions-mbarrier-complete-tx-operation)
operation on the mbarrier has `.release` semantics at the `.cluster` scope as described in the
[Memory Consistency Model](#memory-consistency-model).

Notes

`.multicast::cluster` qualifier is optimized for target architecture `sm_90a`/`sm_100f`/`sm_100a`/
`sm_103f`/`sm_103a`/`sm_110f`/`sm_110a` and may have substantially reduced performance on other
targets and hence `.multicast::cluster` is advised to be used with `.target` `sm_90a`/`sm_100f`/
`sm_100a`/`sm_103f`/`sm_103a`/`sm_110f`/`sm_110a`.

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Support for `.shared::cta` as destination state space is introduced in PTX ISA version 8.6.

Support for `.cp_mask` qualifier introduced in PTX ISA version 8.6.

Target ISA Notes

Requires `sm_90` or higher.

`.multicast::cluster` qualifier advised to be used with `.target` `sm_90a` or `sm_100f` or
`sm_100a` or `sm_103f` or `sm_103a` or `sm_110f` or `sm_110a`.

Support for `.cp_mask` qualifier requires `sm_100` or higher.

Examples

```
// .global -> .shared::cta (strictly non-remote):

cp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];



cp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes.L2::cache_hint

                                             [dstMem], [srcMem], size, [mbar], cache-policy;



// .global -> .shared::cluster:

cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];



cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster

                                             [dstMem], [srcMem], size, [mbar], ctaMask;



cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint

                                             [dstMem], [srcMem], size, [mbar], cache-policy;





// .shared::cta -> .shared::cluster (strictly remote):

cp.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar];



// .shared::cta -> .global:

cp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], size;



cp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint} [dstMem], [srcMem], size, cache-policy;



// .shared::cta -> .global with .cp_mask:

cp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.cp_mask [dstMem], [srcMem], size, cache-policy, byteMask;
```

###### 9.7.9.25.4.2. [Data Movement and Conversion Instructions: `cp.reduce.async.bulk`](#data-movement-and-conversion-instructions-cp-reduce-async-bulk)[](#data-movement-and-conversion-instructions-cp-reduce-async-bulk "Permalink to this headline")

`cp.reduce.async.bulk`

Initiates an asynchronous reduction operation.

Syntax

```
cp.reduce.async.bulk.dst.src.completion_mechanism.redOp.type

              [dstMem], [srcMem], size, [mbar]



.dst =                  { .shared::cluster }

.src =                  { .shared::cta }

.completion_mechanism = { .mbarrier::complete_tx::bytes }

.redOp=                 { .and, .or, .xor,

                          .add, .inc, .dec,

                          .min, .max }

.type =                 { .b32, .u32, .s32, .b64, .u64 }





cp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.redOp.type

               [dstMem], [srcMem], size{, cache-policy}



.dst =                  { .global      }

.src =                  { .shared::cta }

.completion_mechanism = { .bulk_group }

.level::cache_hint    = { .L2::cache_hint }

.redOp=                 { .and, .or, .xor,

                          .add, .inc, .dec,

                          .min, .max }

.type =                 { .f16, .bf16, .b32, .u32, .s32, .b64, .u64, .s64, .f32, .f64 }





cp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.add.noftz.type

               [dstMem], [srcMem], size{, cache-policy}

.dst  =                 { .global }

.src  =                 { .shared::cta }

.completion_mechanism = { .bulk_group }

.type =                 { .f16, .bf16 }
```

Description

`cp.reduce.async.bulk` is a non-blocking instruction which initiates an asynchronous reduction
operation on an array of memory locations specified by the destination address operand `dstMem`
with the source array whose location is specified by the source address operand `srcMem`. The size
of the source and the destination array must be the same and is specified by the operand `size`.

Each data element in the destination array is reduced inline with the corresponding data element in
the source array with the reduction operation specified by the modifier `.redOp`. The type of each
data element in the source and the destination array is specified by the modifier `.type`.

The source address operand `srcMem` is located in the state space specified by `.src` and the
destination address operand `dstMem` is located in the state specified by the `.dst`.

The 32-bit operand `size` specifies the amount of memory to be copied from the source location and
used in the reduction operation, in terms of number of bytes. `size` must be a multiple of 16. If
the value is not a multiple of 16, then the behavior is undefined. The memory range `[dstMem,
dstMem + size - 1]` must not overflow the destination memory space and the memory range `[srcMem,
srcMem + size - 1]` must not overflow the source memory space. Otherwise, the behavior is
undefined. The addresses `dstMem` and `srcMem` must be aligned to 16 bytes.

The operations supported by `.redOp` are classified as follows:

* The bit-size operations are `.and`, `.or`, and `.xor`.
* The integer operations are `.add`, `.inc`, `.dec`, `.min`, and `.max`. The `.inc` and
  `.dec` operations return a result in the range `[0..x]` where `x` is the value at the source
  state space.
* The floating point operation `.add` rounds to the nearest even. The current implementation of
  `cp.reduce.async.bulk.add.f32` flushes subnormal inputs and results to sign-preserving zero. The
  `cp.reduce.async.bulk.add.f16` and `cp.reduce.async.bulk.add.bf16` operations require
  `.noftz` qualifier. It preserves input and result subnormals, and does not flush them to zero.

The following table describes the valid combinations of `.redOp` and element type:

| `.dst` | `.redOp` | Element type |
| --- | --- | --- |
| `.shared::cluster` | `.add` | `.u32`, `.s32`, `.u64` |
| `.min`, `.max` | `.u32`, `.s32` |
| `.inc`, `.dec` | `.u32` |
| `.and`, `.or`, `.xor` | `.b32` |
| `.global` | `.add` | `.u32`, `.s32`, `.u64`, `.f32`, `.f64`, `.f16`, `.bf16` |
| `.min`, `.max` | `.u32`, `.s32`, `.u64`, `.s64`, `.f16`, `.bf16` |
| `.inc`, `.dec` | `.u32` |
| `.and`, `.or`, `.xor` | `.b32`, `.b64` |

The modifier `.completion_mechanism` specifies the completion mechanism that is supported on the
instruction variant. The completion mechanisms that are supported for different variants are
summarized in the following table:

| .completion-mechanism | `.dst` | `.src` | Completion mechanism | |
| --- | --- | --- | --- | --- |
| Needed for completion of entire Async operation | optionally can be used for the completion of - Reading data from the source - Reading from the tensormap, if applicable |
| `.mbarrier::...` | `.shared::cluster` | `.global` | mbarrier based | *Bulk async-group* based |
| `.shared::cluster` | `.shared::cta` |
| `.bulk_group` | `.global` | `.shared::cta` | *Bulk async-group* based |

The modifier `.mbarrier::complete_tx::bytes` specifies that the `cp.reduce.async.bulk` variant
uses mbarrier based completion mechanism. The [complete-tx](#parallel-synchronization-and-communication-instructions-mbarrier-complete-tx-operation)
operation, with `completeCount` argument equal to amount of data copied in bytes, will be
performed on the mbarrier object specified by the operand `mbar`.

The modifier `.bulk_group` specifies that the `cp.reduce.async.bulk` variant uses *bulk
async-group* based completion mechanism.

When the optional argument `cache-policy` is specified, the qualifier `.level::cache_hint` is
required. The 64-bit operand `cache-policy` specifies the cache eviction policy that may be used
during the memory access.

`cache-policy` is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program. The
qualifier `.level::cache_hint` is only supported when at least one of the `.src` or `.dst`
statespaces is `.global` state space.

Each reduction operation performed by the `cp.reduce.async.bulk` has individually `.relaxed.gpu`
memory ordering semantics. The load operations in `cp.reduce.async.bulk` are treated as weak
memory operation and the [complete-tx](#parallel-synchronization-and-communication-instructions-mbarrier-complete-tx-operation)
operation on the mbarrier has `.release` semantics at the `.cluster` scope as described in the
[Memory Consistency Model](#memory-consistency-model).

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Target ISA Notes

Requires `sm_90` or higher.

Examples

```
cp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.add.u64

                                                                  [dstMem], [srcMem], size, [mbar];



cp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.min.s32

                                                                  [dstMem], [srcMem], size, [mbar];



cp.reduce.async.bulk.global.shared::cta.bulk_group.min.f16 [dstMem], [srcMem], size;



cp.reduce.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.xor.s32 [dstMem], [srcMem], size, policy;



cp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size;
```

###### 9.7.9.25.4.3. [Data Movement and Conversion Instructions: `cp.async.bulk.prefetch`](#data-movement-and-conversion-instructions-cp-async-bulk-prefetch)[](#data-movement-and-conversion-instructions-cp-async-bulk-prefetch "Permalink to this headline")

`cp.async.bulk.prefetch`

Provides a hint to the system to initiate the asynchronous prefetch of data to the cache.

Syntax

```
cp.async.bulk.prefetch.L2.src{.level::cache_hint}   [srcMem], size {, cache-policy}



.src =                { .global }

.level::cache_hint =  { .L2::cache_hint }
```

Description

`cp.async.bulk.prefetch` is a non-blocking instruction which may initiate an asynchronous prefetch
of data from the location specified by source address operand `srcMem`, in `.src` statespace, to
the L2 cache.

The 32-bit operand `size` specifies the amount of memory to be prefetched in terms of number of
bytes. `size` must be a multiple of 16. If the value is not a multiple of 16, then the behavior is
undefined. The address `srcMem` must be aligned to 16 bytes.

When the optional argument `cache-policy` is specified, the qualifier `.level::cache_hint` is
required. The 64-bit operand `cache-policy` specifies the cache eviction policy that may be used
during the memory access.

`cache-policy` is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.

`cp.async.bulk.prefetch` is treated as a weak memory operation in the
[Memory Consistency Model](#memory-consistency-model).

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Target ISA Notes

Requires `sm_90` or higher.

Examples

```
cp.async.bulk.prefetch.L2.global                 [srcMem], size;



cp.async.bulk.prefetch.L2.global.L2::cache_hint  [srcMem], size, policy;
```

##### 9.7.9.25.5. [Data Movement and Conversion Instructions: Tensor copy](#data-movement-and-conversion-instructions-tensor-copy)[](#data-movement-and-conversion-instructions-tensor-copy "Permalink to this headline")

###### 9.7.9.25.5.1. [Restriction on Tensor Copy instructions](#data-movement-and-conversion-instructions-tensor-copy-restrictions)[](#data-movement-and-conversion-instructions-tensor-copy-restrictions "Permalink to this headline")

Following are the restrictions on the types `.b4x16`, `.b4x16_p64`, `.b6x16_p32` and
`.b6p2x16`:

1. `cp.reduce.async.bulk` doesn’t support the types `.b4x16`, `.b4x16_p64`, `.b6x16_p32`
   and `.b6p2x16`.
2. `cp.async.bulk.tensor` with the direction `.global.shared::cta` doesn’t support the
   type `.b4x16_p64`.
3. `cp.async.bulk.tensor` with the direction `.shared::cluster.global` doesn’t support
   the sub-byte types on `sm_120a`.
4. OOB-NaN fill mode doesn’t support the types `.b4x16`, `.b4x16_p64`, `.b6x16_p32`
   and `.b6p2x16`.
5. Box-Size[0] must be exactly:

   1. 96B for `b6x16_p32` and `.b6p2x16`.
   2. 64B for `b4x16_p64`.
6. Tensor-Size[0] must be a multiple of:

   1. 96B for `b6x16_p32` and `.b6p2x16`.
   2. 64B for `b4x16_p64`.
7. For `.b4x16_p64`, `.b6x16_p32` and `.b6p2x16`, the first coordinate in the tensorCoords
   argument vector must be a multiple of 128.
8. For `.b4x16_p64`, `.b6x16_p32` and `.b6p2x16`, the global memory address must be 32B aligned.
   Additionally, tensor stride in every dimension must be 32B aligned.
9. `.b4x16_p64`, `.b6x16_p32` and `.b6p2x16` supports the following swizzling modes:

   1. None.
   2. 128B (With all potential swizzle atomicity values except: 32B with 8B flip)

Following are the restrictions on the 96B swizzle mode:

1. The `.swizzle_atomicity` must be 16B.
2. The `.interleave_layout` must not be set.
3. Box-Size[0] must be less than or equal to 96B.
4. The type must not be among following: `.b4x16_p64`, `.b6x16_p32` and `.b6p2x16`.
5. The `.load_mode` must not be set to `.im2col::w::128`.

Following are the restrictions on the `.global.shared::cta` direction:

1. Starting co-ordinates for Bounding Box (`tensorCoords`) must be non-negative.
2. The bounding box along the D, W and H dimensions must stay within the tensor boundaries.
   This implies:

   1. Bounding-Box Lower-Corner must be non-negative.
   2. Bounding-Box Upper-Corner must be non-positive.

Following are the restrictions for `sm_120a`:

1. `cp.async.bulk.tensor` with the direction `.shared::cluster.global` doesn’t support:

   1. the sub-byte types
   2. the qualifier `.swizzle_atomicity`

Following are the restrictions for `sm_103a` while using type `.b6p2x16` on
`cp.async.bulk.tensor` with the direction `.global.shared::cta`:

1. Box-Size[0] must be exactly either of 48B or 96B.
2. The global memory address must be 16B aligned.
3. Tensor Stride in every dimension must be 16B aligned.
4. The first coordinate in the tensorCoords argument vector must be a multiple of 64.
5. Tensor-Size[0] must be a multiple of 48B.
6. The following swizzle modes are supported:

   1. None.
   2. 128B (With all potential swizzle atomicity values except: 32B with 8B flip)
   3. 64B swizzle with 16B swizzle atomicity

###### 9.7.9.25.5.2. [Data Movement and Conversion Instructions: `cp.async.bulk.tensor`](#data-movement-and-conversion-instructions-cp-async-bulk-tensor)[](#data-movement-and-conversion-instructions-cp-async-bulk-tensor "Permalink to this headline")

`cp.async.bulk.tensor`

Initiates an asynchronous copy operation on the tensor data from one state space to another.

Syntax

```
// global -> shared::cta

cp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.cta_group}{.level::cache_hint}

                                   [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colInfo} {, cache-policy}



.dst =                  { .shared::cta }

.src =                  { .global }

.dim =                  { .1d, .2d, .3d, .4d, .5d }

.completion_mechanism = { .mbarrier::complete_tx::bytes }

.cta_group =            { .cta_group::1, .cta_group::2 }

.load_mode =            { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }

.level::cache_hint =    { .L2::cache_hint }





// global -> shared::cluster

cp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.multicast}{.cta_group}{.level::cache_hint}

                                   [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colInfo}

                                   {, ctaMask} {, cache-policy}



.dst =                  { .shared::cluster }

.src =                  { .global }

.dim =                  { .1d, .2d, .3d, .4d, .5d }

.completion_mechanism = { .mbarrier::complete_tx::bytes }

.cta_group =            { .cta_group::1, .cta_group::2 }

.load_mode =            { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }

.level::cache_hint =    { .L2::cache_hint }

.multicast =            { .multicast::cluster  }





// shared::cta -> global

cp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.level::cache_hint}

                                   [tensorMap, tensorCoords], [srcMem] {, cache-policy}



.dst =                  { .global }

.src =                  { .shared::cta }

.dim =                  { .1d, .2d, .3d, .4d, .5d }

.completion_mechanism = { .bulk_group }

.load_mode =            { .tile, .tile::scatter4, .im2col_no_offs }

.level::cache_hint =    { .L2::cache_hint }
```

Description

`cp.async.bulk.tensor` is a non-blocking instruction which initiates an asynchronous copy
operation of tensor data from the location in `.src` state space to the location in the `.dst`
state space.

The operand `dstMem` specifies the location in the `.dst` state space into which the tensor data
has to be copied and `srcMem` specifies the location in the `.src` state space from which the
tensor data has to be copied.

When `.dst` is specified as `.shared::cta`, the address `dstMem` must be in the shared memory
of the executing CTA within the cluster, otherwise the behavior is undefined.

When `.dst` is specified as `.shared::cluster`, the address `dstMem` can be in the shared memory
of any of the CTAs within the current cluster.

The operand `tensorMap` is the generic address of the opaque tensor-map object which resides
in `.param` space or `.const` space or `.global` space. The operand `tensorMap` specifies
the properties of the tensor copy operation, as described in [Tensor-map](#tensor-tensormap).
The `tensorMap` is accessed in tensormap proxy. Refer to the *CUDA programming guide* for creating
the tensor-map objects on the host side.

The dimension of the tensor data is specified by the `.dim` modifier.

The vector operand `tensorCoords` specifies the starting coordinates in the tensor data in the
global memory from or to which the copy operation has to be performed. The individual tensor
coordinates in `tensorCoords` are of type `.s32`. The format of vector argument `tensorCoords`
is dependent on `.load_mode` specified and is as follows:

| .load\_mode | tensorCoords | Semantics |
| --- | --- | --- |
| `.tile::scatter4` | {col\_idx, row\_idx0, row\_idx1, row\_idx2, row\_idx3} | Fixed length vector of size 5. The five elements together specify the start co-ordinates of the four rows. |
| `.tile::gather4` |
| Rest all | {d0, .., dn} for n = .dim | Vector of n elements where n = .dim. The elements indicate the offset in each of the dimension. |

The modifier `.completion_mechanism` specifies the completion mechanism that is supported on the
instruction variant. The completion mechanisms that are supported for different variants are
summarized in the following table:

| .completion-mechanism | `.dst` | `.src` | Completion mechanism | |
| --- | --- | --- | --- | --- |
| Needed for completion of entire Async operation | optionally can be used for the completion of - Reading data from the source - Reading from the tensormap, if applicable |
| `.mbarrier::...` | `.shared::cta` | `.global` | mbarrier based | *Bulk async-group* based |
| `.shared::cluster` | `.global` |
| `.bulk_group` | `.global` | `.shared::cta` | *Bulk async-group* based |

The modifier `.mbarrier::complete_tx::bytes` specifies that the `cp.async.bulk.tensor` variant
uses mbarrier based completion mechanism. Upon the completion of the asynchronous copy operation, the
[complete-tx](#parallel-synchronization-and-communication-instructions-mbarrier-complete-tx-operation)
operation, with `completeCount` argument equal to amount of data copied in bytes, will be
performed on the mbarrier object specified by the operand `mbar`.

The modifier `.cta_group` can only be specified with the mbarrier based completion mechanism. The
modifier `.cta_group` is used to signal either the odd numbered CTA or the even numbered CTA among
the [CTA-Pair](#tcgen05-cta-pair). When `.cta_group::1` is specified, the mbarrier object `mbar`
that is specified must be in the shared memory of the same CTA as the shared memory destination `dstMem`.
When `.cta_group::2` is specified, the mbarrier object `mbar` can be in shared memory of either the
same CTA as the shared memory destination `dstMem` or in its [peer-CTA](#tcgen05-peer-cta). If
`.cta_group` is not specified, then it defaults to `.cta_group::1`.

The modifier `.bulk_group` specifies that the `cp.async.bulk.tensor` variant uses *bulk
async-group* based completion mechanism.

The qualifier `.load_mode` specifies how the data in the source location is copied into the
destination location. If `.load_mode` is not specified, it defaults to `.tile`.

In `.tile` mode, the multi-dimensional layout of the source tensor is preserved at the destination.
In `.tile::gather4` mode, four rows in 2-dimnesional source tensor are combined to form a single 2-dimensional
destination tensor. In `.tile::scatter4` mode, single 2-dimensional source tensor is divided into four rows
in 2-dimensional destination tensor. Details of `.tile::scatter4`/`.tile::gather4` modes are described
in [.tile::scatter4 and .tile::gather4 modes](#tensor-tiled-scatter4-gather4-modes).

In `.im2col` and `.im2col::*` modes, some dimensions of the source tensors are unrolled in a single
dimensional column at the destination. Details of the `im2col` and `.im2col::*` modes are described
in [im2col mode](#tensor-im2col-mode) and [im2col::w and im2col::w::128 modes](#tensor-im2col-w-w128-modes)
respectively. In `.im2col` and `.im2col::*` modes, the tensor has to be at least 3-dimensional. The vector
operand `im2colInfo` can be specified only when `.load_mode` is `.im2col` or `.im2col::w` or
`.im2col::w::128`. The format of the vector argument `im2colInfo` is dependent on the exact im2col mode
and is as follows:

| Exact im2col mode | im2colInfo argument | Semantics |
| --- | --- | --- |
| `.im2col` | { i2cOffW , i2cOffH , i2cOffD } for `.dim` = `.5d` | A vector of im2col offsets whose vector size is two less than number of dimensions .dim. |
| `.im2col::w` | { wHalo, wOffset } | A vector of 2 arguments containing [wHalo](#tensor-im2col-w-w128-modes-whalo) and [wOffset](#tensor-im2col-w-w128-modes-woffset) arguments. |
| `.im2col::w::128` |
| `.im2col_no_offs` | `im2colInfo` is not applicable. | `im2colInfo` is not applicable. |

Argument `wHalo` is a 16bit unsigned integer whose valid set of values differs on the load-mode and is as follows:
- Im2col::w mode : valid range is [0, 512).
- Im2col::w::128 mode : valid range is [0, 32).

Argument `wOffset` is a 16bit unsigned integer whose valid range of values is [0, 32).

The optional modifier `.multicast::cluster` allows copying of data from global memory to shared
memory of multiple CTAs in the cluster. Operand `ctaMask` specifies the destination CTAs in the
cluster such that each bit position in the 16-bit `ctaMask` operand corresponds to the `%ctaid`
of the destination CTA. The source data is multicast to the same offset as `dstMem` in the shared
memory of each destination CTA. When `.cta_group` is specified as:

* `.cta_group::1` : The mbarrier signal is also multicasted to the same offset as `mbar` in
  the shared memory of the destination CTA.
* `.cta_group::2` : The mbarrier signal is multicasted either to all the odd numbered CTAs or the
  even numbered CTAs within the corresponding [CTA-Pair](#tcgen05-cta-pair). For each destination
  CTA specified in the `ctaMask`, the mbarrier signal is sent either to the destination CTA or its
  [peer-CTA](#tcgen05-peer-cta) based on CTAs `%cluster_ctarank` parity of shared memory where
  the mbarrier object `mbar` resides.

When the optional argument `cache-policy` is specified, the qualifier `.level::cache_hint` is
required. The 64-bit operand `cache-policy` specifies the cache eviction policy that may be used
during the memory access.

`cache-policy` is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.

The copy operation in `cp.async.bulk.tensor` is treated as a weak memory operation and the
[complete-tx](#parallel-synchronization-and-communication-instructions-mbarrier-complete-tx-operation)
operation on the mbarrier has `.release` semantics at the `.cluster` scope as described in the
[Memory Consistency Model](#memory-consistency-model).

Notes

`.multicast::cluster` qualifier is optimized for target architecture `sm_90a`/`sm_100f`/`sm_100a`/
`sm_103f`/`sm_103a`/`sm_110f`/`sm_110a` and may have substantially reduced performance on other
targets and hence `.multicast::cluster` is advised to be used with `.target` `sm_90a`/`sm_100f`/
`sm_100a`/`sm_103f`/`sm_103a`/`sm_110f`/`sm_110a`.

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Support for `.shared::cta` as destination state space is introduced in PTX ISA version 8.6.

Support for qualifiers `.tile::gather4` and `.tile::scatter4` introduced in PTX ISA version 8.6.

Support for qualifiers `.im2col::w` and `.im2col::w::128` introduced in PTX ISA version 8.6.

Support for qualifier `.cta_group` introduced in PTX ISA version 8.6.

Target ISA Notes

Requires `sm_90` or higher.

`.multicast::cluster` qualifier advised to be used with `.target` `sm_90a` or `sm_100f` or
`sm_100a` or `sm_103f` or `sm_103a` or `sm_110f` or `sm_110a`.

Qualifiers `.tile::gather4` and `.im2col::w` require:

* `sm_100a` when destination state space is `.shared::cluster` and is supported on `sm_100f` from PTX ISA version 8.8.
* `sm_100` or higher when destination state space is `.shared::cta`.

Qualifier `.tile::scatter4` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Qualifier `.im2col::w::128` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Qualifier `.cta_group` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Examples

```
.reg .b16 ctaMask;

.reg .u16 i2cOffW, i2cOffH, i2cOffD;

.reg .b64 l2CachePolicy;



cp.async.bulk.tensor.1d.shared::cta.global.mbarrier::complete_tx::bytes.tile  [sMem0], [tensorMap0, {tc0}], [mbar0];



@p cp.async.bulk.tensor.5d.shared::cta.global.im2col.mbarrier::complete_tx::bytes

                     [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD};



cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.tile  [sMem0], [tensorMap0, {tc0}], [mbar0];



@p cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster

                     [sMem1], [tensorMap1, {tc0, tc1}], [mbar2], ctaMask;



@p cp.async.bulk.tensor.5d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes

                     [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD};



@p cp.async.bulk.tensor.3d.im2col.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint

                     [sMem3], [tensorMap3, {tc0, tc1, tc2}], [mbar3], {i2cOffW}, policy;



@p cp.async.bulk.tensor.1d.global.shared::cta.bulk_group  [tensorMap3, {tc0}], [sMem3];



cp.async.bulk.tensor.2d.tile::gather4.shared::cluster.global.mbarrier::complete_tx::bytes

                     [sMem5], [tensorMap6, {x0, y0, y1, y2, y3}], [mbar5];



cp.async.bulk.tensor.3d.im2col::w.shared::cluster.global.mbarrier::complete_tx::bytes

                     [sMem4], [tensorMap5, {t0, t1, t2}], [mbar4], {im2colwHalo, im2colOff};



cp.async.bulk.tensor.1d.shared::cluster.global.tile.cta_group::2

                     [sMem6], [tensorMap7, {tc0}], [peerMbar];
```

###### 9.7.9.25.5.3. [Data Movement and Conversion Instructions: `cp.reduce.async.bulk.tensor`](#data-movement-and-conversion-instructions-cp-reduce-async-bulk-tensor)[](#data-movement-and-conversion-instructions-cp-reduce-async-bulk-tensor "Permalink to this headline")

`cp.reduce.async.bulk.tensor`

Initiates an asynchronous reduction operation on the tensor data.

Syntax

```
// shared::cta -> global:

cp.reduce.async.bulk.tensor.dim.dst.src.redOp{.load_mode}.completion_mechanism{.level::cache_hint}

                                          [tensorMap, tensorCoords], [srcMem] {,cache-policy}



.dst =                  { .global }

.src =                  { .shared::cta }

.dim =                  { .1d, .2d, .3d, .4d, .5d }

.completion_mechanism = { .bulk_group }

.load_mode =            { .tile, .im2col_no_offs }

.redOp =                { .add, .min, .max, .inc, .dec, .and, .or, .xor}
```

Description

`cp.reduce.async.bulk.tensor` is a non-blocking instruction which initiates an asynchronous
reduction operation of tensor data in the `.dst` state space with tensor data in the `.src`
state space.

The operand `srcMem` specifies the location of the tensor data in the `.src` state space using
which the reduction operation has to be performed.

The operand `tensorMap` is the generic address of the opaque tensor-map object which resides
in `.param` space or `.const` space or `.global` space. The operand `tensorMap` specifies
the properties of the tensor copy operation, as described in [Tensor-map](#tensor-tensormap).
The `tensorMap` is accessed in tensormap proxy. Refer to the *CUDA programming guide* for creating
the tensor-map objects on the host side.

Each element of the tensor data in the `.dst` state space is reduced inline with the corresponding
element from the tensor data in the `.src` state space. The modifier `.redOp` specifies the
reduction operation used for the inline reduction. The type of each tensor data element in the
source and the destination tensor is specified in [Tensor-map](#tensor-tensormap).

The dimension of the tensor is specified by the `.dim` modifier.

The vector operand `tensorCoords` specifies the starting coordinates of the tensor data in the
global memory on which the reduce operation is to be performed. The number of tensor coordinates in
the vector argument `tensorCoords` should be equal to the dimension specified by the modifier
`.dim`. The individual tensor coordinates are of the type `.s32`.

The following table describes the valid combinations of `.redOp` and element type:

| `.redOp` | Element type |
| --- | --- |
| `.add` | `.u32`, `.s32`, `.u64`, `.f32`, `.f16`, `.bf16` |
| `.min`, `.max` | `.u32`, `.s32`, `.u64`, `.s64`, `.f16`, `.bf16` |
| `.inc`, `.dec` | `.u32` |
| `.and`, `.or`, `.xor` | `.b32`, `.b64` |

The modifier `.completion_mechanism` specifies the completion mechanism that is supported on the
instruction variant. Value `.bulk_group` of the modifier `.completion_mechanism` specifies that
`cp.reduce.async.bulk.tensor` instruction uses *bulk async-group* based completion mechanism.

The qualifier `.load_mode` specifies how the data in the source location is copied into the
destination location. If `.load_mode` is not specified, it defaults to `.tile`. In `.tile`
mode, the multi-dimensional layout of the source tensor is preserved at the destination. In
`.im2col_no_offs` mode, some dimensions of the source tensors are unrolled in a single dimensional
column at the destination. Details of the `im2col` mode are described in
[im2col mode](#tensor-im2col-mode). In `.im2col` mode, the tensor has to be at least
3-dimensional.

When the optional argument `cache-policy` is specified, the qualifier `.level::cache_hint` is
required. The 64-bit operand `cache-policy` specifies the cache eviction policy that may be used
during the memory access.

`cache-policy` is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program. The
qualifier `.level::cache_hint` is only supported when at least one of the `.src` or `.dst`
statespaces is `.global` state space.

Each reduction operation performed by `cp.reduce.async.bulk.tensor` has individually
`.relaxed.gpu` memory ordering semantics. The load operations in `cp.reduce.async.bulk.tensor`
are treated as weak memory operations and the [complete-tx](#parallel-synchronization-and-communication-instructions-mbarrier-complete-tx-operation)
operation on the mbarrier has `.release` semantics at the `.cluster` scope as described in the
[Memory Consistency Model](#memory-consistency-model).

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Target ISA Notes

Requires `sm_90` or higher.

Examples

```
cp.reduce.async.bulk.tensor.1d.global.shared::cta.add.tile.bulk_group

                                             [tensorMap0, {tc0}], [sMem0];



cp.reduce.async.bulk.tensor.2d.global.shared::cta.and.bulk_group.L2::cache_hint

                                             [tensorMap1, {tc0, tc1}], [sMem1] , policy;



cp.reduce.async.bulk.tensor.3d.global.shared::cta.xor.im2col.bulk_group

                                             [tensorMap2, {tc0, tc1, tc2}], [sMem2]
```

###### 9.7.9.25.5.4. [Data Movement and Conversion Instructions: `cp.async.bulk.prefetch.tensor`](#data-movement-and-conversion-instructions-cp-async-bulk-prefetch-tensor)[](#data-movement-and-conversion-instructions-cp-async-bulk-prefetch-tensor "Permalink to this headline")

`cp.async.bulk.prefetch.tensor`

Provides a hint to the system to initiate the asynchronous prefetch of tensor data to the cache.

Syntax

```
// global -> shared::cluster:

cp.async.bulk.prefetch.tensor.dim.L2.src{.load_mode}{.level::cache_hint} [tensorMap, tensorCoords]

                                                             {, im2colInfo } {, cache-policy}



.src =                { .global }

.dim =                { .1d, .2d, .3d, .4d, .5d }

.load_mode =          { .tile, .tile::gather4, .im2col, .im2col::w, .im2col::w::128 }

.level::cache_hint =  { .L2::cache_hint }
```

Description

`cp.async.bulk.prefetch.tensor` is a non-blocking instruction which may initiate an asynchronous
prefetch of tensor data from the location in `.src` statespace to the L2 cache.

The operand `tensorMap` is the generic address of the opaque tensor-map object which resides
in `.param` space or `.const` space or `.global` space. The operand `tensorMap` specifies
the properties of the tensor copy operation, as described in [Tensor-map](#tensor-tensormap).
The `tensorMap` is accessed in tensormap proxy. Refer to the *CUDA programming guide* for creating
the tensor-map objects on the host side.

The dimension of the tensor data is specified by the `.dim` modifier.

The vector operand `tensorCoords` specifies the starting coordinates in the tensor data in the
global memory from which the copy operation has to be performed. The individual tensor
coordinates in `tensorCoords` are of type `.s32`. The format of vector argument `tensorCoords`
is dependent on `.load_mode` specified and is as follows:

| .load\_mode | tensorCoords | Semantics |
| --- | --- | --- |
| `.tile::gather4` | {col\_idx, row\_idx0, row\_idx1, row\_idx2, row\_idx3} | Fixed length vector of size 5. The five elements together specify the start co-ordinates of the four rows. |
| Rest all | {d0, .., dn} for n = .dim | Vector of n elements where n = .dim. The elements indicate the offset in each of the dimension. |

The qualifier `.load_mode` specifies how the data in the source location is copied into the
destination location. If `.load_mode` is not specified, it defaults to `.tile`.

In `.tile` mode, the multi-dimensional layout of the source tensor is preserved at the destination.
In `.tile::gather4` mode, four rows in the 2-dimnesional source tensor are fetched to L2 cache.
Details of `.tile::gather4` modes are described
in [.tile::scatter4 and .tile::gather4 modes](#tensor-tiled-scatter4-gather4-modes).

In `.im2col` and `.im2col::*` modes, some dimensions of the source tensors are unrolled in a single
dimensional column at the destination. Details of the `im2col` and `.im2col::*` modes are described in
[im2col mode](#tensor-im2col-mode) and [im2col::w and im2col::w::128 modes](#tensor-im2col-w-w128-modes)
respectively. In `.im2col` and `.im2col::*` modes, the tensor has to be at least 3-dimensional. The vector
operand `im2colInfo` can be specified only when `.load_mode` is `.im2col` or `.im2col::w` or
`.im2col::w::128`. The format of the vector argument `im2colInfo` is dependent on the exact im2col mode
and is as follows:

| Exact im2col mode | im2colInfo argument | Semantics |
| --- | --- | --- |
| `.im2col` | { i2cOffW , i2cOffH , i2cOffD } for `.dim` = `.5d` | A vector of im2col offsets whose vector size is two less than number of dimensions .dim. |
| `.im2col::w` | { wHalo, wOffset } | A vector of 2 arguments containing [wHalo](#tensor-im2col-w-w128-modes-whalo) and [wOffset](#tensor-im2col-w-w128-modes-woffset) arguments. |
| `.im2col::w::128` |

When the optional argument `cache-policy` is specified, the qualifier `.level::cache_hint` is
required. The 64-bit operand `cache-policy` specifies the cache eviction policy that may be used
during the memory access.

`cache-policy` is a hint to the cache subsystem and may not always be respected. It is treated as
a performance hint only, and does not change the memory consistency behavior of the program.

`cp.async.bulk.prefetch.tensor` is treated as a weak memory operation in the
[Memory Consistency Model](#memory-consistency-model).

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Support for qualifier `.tile::gather4` introduced in PTX ISA version 8.6.

Support for qualifiers `.im2col::w` and `.im2col::w::128` introduced in PTX ISA version 8.6.

Target ISA Notes

Requires `sm_90` or higher.

Qualifier `.tile::gather4` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Qualifiers `.im2col::w` and `.im2col::w::128` are supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* And are supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
* `sm_110f` or higher in the same family

Examples

```
.reg .b16 ctaMask, im2colwHalo, im2colOff;

.reg .u16 i2cOffW, i2cOffH, i2cOffD;

.reg .b64 l2CachePolicy;



cp.async.bulk.prefetch.tensor.1d.L2.global.tile  [tensorMap0, {tc0}];



@p cp.async.bulk.prefetch.tensor.2d.L2.global    [tensorMap1, {tc0, tc1}];



@p cp.async.bulk.prefetch.tensor.5d.L2.global.im2col

                      [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], {i2cOffW, i2cOffH, i2cOffD};



@p cp.async.bulk.prefetch.tensor.3d.L2.global.im2col.L2::cache_hint

                      [tensorMap3, {tc0, tc1, tc2}], {i2cOffW}, policy;



cp.async.bulk.prefetch.tensor.2d.L2.global.tile::gather4 [tensorMap5, {col_idx, row_idx0, row_idx1, row_idx2, row_idx3}];



cp.async.bulk.prefetch.tensor.4d.L2.global.im2col::w::128

                      [tensorMap4, {t0, t1, t2, t3}], {im2colwHalo, im2colOff};
```

##### 9.7.9.25.6. [Data Movement and Conversion Instructions: Bulk and Tensor copy completion instructions](#data-movement-and-conversion-instructions-bulk-tensor-copy-completion)[](#data-movement-and-conversion-instructions-bulk-tensor-copy-completion "Permalink to this headline")

###### 9.7.9.25.6.1. [Data Movement and Conversion Instructions: `cp.async.bulk.commit_group`](#data-movement-and-conversion-instructions-cp-async-bulk-commit-group)[](#data-movement-and-conversion-instructions-cp-async-bulk-commit-group "Permalink to this headline")

`cp.async.bulk.commit_group`

Commits all prior initiated but uncommitted `cp.async.bulk` instructions into a
*cp.async.bulk-group*.

Syntax

```
cp.async.bulk.commit_group;
```

Description

`cp.async.bulk.commit_group` instruction creates a new per-thread *bulk async-group* and batches
all prior `cp{.reduce}.async.bulk.{.prefetch}{.tensor}` instructions satisfying the following
conditions into the new *bulk async-group*:

* The prior `cp{.reduce}.async.bulk.{.prefetch}{.tensor}` instructions use *bulk\_group* based
  completion mechanism, and
* They are initiated by the executing thread but not committed to any *bulk async-group*.

If there are no uncommitted `cp{.reduce}.async.bulk.{.prefetch}{.tensor}` instructions then
`cp.async.bulk.commit_group` results in an empty *bulk async-group*.

An executing thread can wait for the completion of all
`cp{.reduce}.async.bulk.{.prefetch}{.tensor}` operations in a *bulk async-group* using
`cp.async.bulk.wait_group`.

There is no memory ordering guarantee provided between any two
`cp{.reduce}.async.bulk.{.prefetch}{.tensor}` operations within the same *bulk async-group*.

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Target ISA Notes

Requires `sm_90` or higher.

Examples

```
cp.async.bulk.commit_group;
```

###### 9.7.9.25.6.2. [Data Movement and Conversion Instructions: `cp.async.bulk.wait_group`](#data-movement-and-conversion-instructions-cp-async-bulk-wait-group)[](#data-movement-and-conversion-instructions-cp-async-bulk-wait-group "Permalink to this headline")

`cp.async.bulk.wait_group`

Wait for completion of *bulk async-groups*.

Syntax

```
cp.async.bulk.wait_group{.read} N;
```

Description

`cp.async.bulk.wait_group` instruction will cause the executing thread to wait until only N or
fewer of the most recent *bulk async-groups* are pending and all the prior *bulk async-groups*
committed by the executing threads are complete. For example, when N is 0, the executing thread
waits on all the prior *bulk async-groups* to complete. Operand N is an integer constant.

By default, `cp.async.bulk.wait_group` instruction will cause the executing thread to wait until
completion of all the bulk async operations in the specified *bulk async-group*. A bulk async
operation includes the following:

* Optionally, reading from the tensormap.
* Reading from the source locations.
* Writing to their respective destination locations.
* Writes being made visible to the executing thread.

The optional `.read` modifier indicates that the waiting has to be done until all the bulk
async operations in the specified *bulk async-group* have completed:

1. reading from the tensormap
2. the reading from their source locations.

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Target ISA Notes

Requires `sm_90` or higher.

Examples

```
cp.async.bulk.wait_group.read   0;

cp.async.bulk.wait_group        2;
```

#### 9.7.9.26. [Data Movement and Conversion Instructions: `tensormap.replace`](#data-movement-and-conversion-instructions-tensormap-replace)[](#data-movement-and-conversion-instructions-tensormap-replace "Permalink to this headline")

`tensormap.replace`

Modifies the field of a tensor-map object.

Syntax

```
tensormap.replace.mode.field1{.ss}.b1024.type  [addr], new_val;

tensormap.replace.mode.field2{.ss}.b1024.type  [addr], ord, new_val;

tensormap.replace.mode.field3{.ss}.b1024.type  [addr], new_val;



.mode    = { .tile }

.field1  = { .global_address, .rank }

.field2  = { .box_dim, .global_dim, .global_stride, .element_stride  }

.field3  = { .elemtype,  .interleave_layout, .swizzle_mode, .swizzle_atomicity, .fill_mode }

.ss      = { .global, .shared::cta }

.type    = { .b32, .b64 }
```

Description

The `tensormap.replace` instruction replaces the field, specified by `.field` qualifier,
of the tensor-map object at the location specified by the address operand `addr` with a
new value. The new value is specified by the argument `new_val`.

Qualifier `.mode` specifies the mode of the [tensor-map](#tensor-tensormap) object
located at the address operand `addr`.

Instruction type `.b1024` indicates the size of the [tensor-map](#tensor-tensormap)
object, which is 1024 bits.

Operand `new_val` has the type `.type`. When `.field` is specified as `.global_address`
or `.global_stride`, `.type` must be `.b64`. Otherwise, `.type` must be `.b32`.

The immediate integer operand `ord` specifies the ordinal of the field across the rank of the
tensor which needs to be replaced in the [tensor-map](#tensor-tensormap) object.

For field `.rank`, the operand `new_val` must be ones less than the desired tensor rank as
this field uses zero-based numbering.

When `.field3` is specified, the operand `new_val` must be an immediate and the
[Table 33](#tensormap-new-val-validity) shows the mapping of the operand `new_val` across various fields.

Table 33 Tensormap new\_val validity[](#tensormap-new-val-validity "Permalink to this table")








| **new\_val** | **.field3** | | | | |
| --- | --- | --- | --- | --- | --- |
| **.elemtype** | **.interleave\_layout** | **.swizzle\_mode** | **.swizzle\_atomicity** | **.fill\_mode** |
| 0 | `.u8` | No interleave | No swizzling | 16B | Zero fill |
| 1 | `.u16` | 16B interleave | 32B swizzling | 32B | OOB-NaN fill |
| 2 | `.u32` | 32B interleave | 64B swizzling | 32B + 8B flip | x |
| 3 | `.s32` | x | 128B swizzling | 64B | x |
| 4 | `.u64` | x | 96B swizzling | x | x |
| 5 | `.s64` | x | x | x | x |
| 6 | `.f16` | x | x | x | x |
| 7 | `.f32` | x | x | x | x |
| 8 | `.f32.ftz` | x | x | x | x |
| 9 | `.f64` | x | x | x | x |
| 10 | `.bf16` | x | x | x | x |
| 11 | `.tf32` | x | x | x | x |
| 12 | `.tf32.ftz` | x | x | x | x |
| 13 | `.b4x16` | x | x | x | x |
| 14 | `.b4x16_p64` | x | x | x | x |
| 15 | `.b6x16_p32` or `.b6p2x16` | x | x | x | x |

Note

The values of `.elemtype` do not correspond to the values of the `CUtensorMapDataType` enum used in the driver API.

If no state space is specified then [Generic Addressing](#generic-addressing) is used.
If the address specified by `addr` does not fall within the address window of `.global`
or `.shared::cta` state space then the behavior is undefined.

`tensormap.replace` is treated as a weak memory operation, on the entire 1024-bit opaque
[tensor-map](#tensor-tensormap) object, in the [Memory Consistency Model](#memory-consistency-model).

PTX ISA Notes

Introduced in PTX ISA version 8.3.

Qualifier `.swizzle_atomicity` introduced in PTX ISA version 8.6.

Qualifier `.elemtype` with values from `13` to `15`, both inclusive, is
supported in PTX ISA version 8.7 onwards.

Qualifier `.swizzle_mode` with value `4` is supported from PTX ISA version 8.8 onwards.

Target ISA Notes

Supported on following architectures:

* `sm_90a`
* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_120a`
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
  + `sm_120f` or higher in the same family
* `sm_110f` or higher in the same family

Qualifier `.swizzle_atomicity` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_120a` (refer to [section](#data-movement-and-conversion-instructions-tensor-copy-restrictions)
  for restrictions on sm\_120a)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
  + `sm_120f` or higher in the same family
* `sm_110f` or higher in the same family

`.field3` variant `.elemtype` corresponding to `new_val` values `13`, `14`
and `15` is supported on following architectures:

* `sm_100a`
* `sm_101a` (Renamed to `sm_110a` from PTX ISA version 9.0)
* `sm_120a` (refer to [section](#data-movement-and-conversion-instructions-tensor-copy-restrictions)
  for restrictions on sm\_120a)
* And is supported on following family-specific architectures from PTX ISA version 8.8:

  + `sm_100f` or higher in the same family
  + `sm_101f` or higher in the same family (Renamed to `sm_110f` from PTX ISA version 9.0)
  + `sm_120f` or higher in the same family
* `sm_110f` or higher in the same family

`.field3` variant `.swizzle_mode` corresponding to `new_val` value `4` is supported on
following architectures:

* `sm_103a` (refer to [section](#data-movement-and-conversion-instructions-tensor-copy-restrictions)
  for restrictions on sm\_103a)

Examples

```
tensormap.replace.tile.global_address.shared::cta.b1024.b64   [sMem], new_val;
```