## 5.5. Tensors 

A tensor is a multi-dimensional matrix structure in the memory. Tensor is defined by the following
properties:

* Dimensionality
* Dimension sizes across each dimension
* Individual element types
* Tensor stride across each dimension

PTX supports instructions which can operate on the tensor data. PTX Tensor instructions include:

* Copying data between global and shared memories
* Reducing the destination tensor data with the source.

The Tensor data can be operated on by various `wmma.mma`, `mma` and `wgmma.mma_async`
instructions.

PTX Tensor instructions treat the tensor data in the global memory as a multi-dimensional structure
and treat the data in the shared memory as a linear data.

### 5.5.1. [Tensor Dimension, size and format](#tensor-dimension-size-format)[](#tensor-dimension-size-format "Permalink to this headline")

Tensors can have dimensions: 1D, 2D, 3D, 4D or 5D.

Each dimension has a size which represents the number of elements along the dimension. The elements
can have one the following types:

* Bit-sized type: `.b32`, `.b64`
* Sub-byte types: `.b4x16`, `.b4x16_p64`, `.b6x16_p32`, `.b6p2x16`
* Integer: `.u8`, `.u16`, `.u32`, `.s32`, `.u64`, `.s64`
* Floating point and alternate floating point: `.f16`, `.bf16`, `.tf32`, `.f32`, `.f64`
  (rounded to nearest even).

Tensor can have padding at the end in each of the dimensions to provide alignment for the data in
the subsequent dimensions. Tensor stride can be used to specify the amount of padding in each
dimension.

#### 5.5.1.1. [Sub-byte Types](#tensor-dimension-size-format-sub-bytes)[](#tensor-dimension-size-format-sub-bytes "Permalink to this headline")

##### 5.5.1.1.1. [Padding and alignment of the sub-byte types](#tensor-dimension-size-format-sub-bytes-padding-align)[](#tensor-dimension-size-format-sub-bytes-padding-align "Permalink to this headline")

The sub-byte types are expected to packed contiguously in the global memory and
the Tensor copy instruction will expand them by appending empty spaces as shown below:

1. Type `.b4x16`:
   With this type, there is no padding involved and the packed sixteen `.b4` elements
   in a 64-bits container is copied as is between the shared memory and the global memory.
2. Type `.b4x16_p64`:
   With this type, sixteen contiguous 4-bits of data is copied from global memory to the
   shared memory with the append of 64-bits of padding as shown in
   [Figure 5](#tensor-dimension-size-format-sub-bytes-padding-align-b4-16-p64)

   ![_images/tensor-dimension-size-format-sub-bytes-padding-align-b4-16-p64.png](_images/tensor-dimension-size-format-sub-bytes-padding-align-b4-16-p64.png)


   Figure 5 Layout for .b4x16\_p64[](#tensor-dimension-size-format-sub-bytes-padding-align-b4-16-p64 "Permalink to this image")

   The padded region that gets added is un-initialized.
3. Type `.b6x16_p32`:
   With this type, sixteen 6-bits of data is copied from global memory to the shared memory
   with an append of 32-bits of padding as shown in
   [Figure 6](#tensor-dimension-size-format-sub-bytes-padding-align-b6-16-p32)

   ![_images/tensor-dimension-size-format-sub-bytes-padding-align-b6-16-p32.png](_images/tensor-dimension-size-format-sub-bytes-padding-align-b6-16-p32.png)


   Figure 6 Layout for .b6x16\_p32[](#tensor-dimension-size-format-sub-bytes-padding-align-b6-16-p32 "Permalink to this image")

   The padded region that gets added is un-initialized.
4. Type `.b6p2x16`:
   With this type, sixteen elements, each containing 6-bits of data at the LSB and 2-bits
   of padding at the MSB, are copied from shared memory into the global memory by discarding
   the 2-bits of padding data and packing the 6-bits data contiguously as shown in
   [Figure 7](#tensor-dimension-size-format-sub-bytes-padding-align-b6-p2-16)

   ![_images/tensor-dimension-size-format-sub-bytes-padding-align-b6-p2-16.png](_images/tensor-dimension-size-format-sub-bytes-padding-align-b6-p2-16.png)


   Figure 7 Layout for .b6p2x16[](#tensor-dimension-size-format-sub-bytes-padding-align-b6-p2-16 "Permalink to this image")

In case of `.b6x16_p32` and `.b4x16_p64`, the padded region that gets added is
un-initialized.

The types `.b6x16_p32` and `.b6p2x16` share the same encoding value in the
descriptor (value 15) as the two types are applicable for different types of
tensor copy operations:

| Type | Valid Tensor Copy Direction |
| --- | --- |
| `.b6x16_p32` | `.shared::cluster.global`, `.shared::cta.global` |
| `.b6p2x16` | `.global.shared::cta` |

### 5.5.2. [Tensor Access Modes](#tensor-access-modes)[](#tensor-access-modes "Permalink to this headline")

Tensor data can be accessed in two modes:

* Tiled mode:

  In tiled mode, the source multi-dimensional tensor layout is preserved at the destination.
* Im2col mode:

  In im2col mode, the elements in the Bounding Box of the source tensor are rearranged into columns
  at the destination. Refer [here](https://in.mathworks.com/help/images/ref/im2col.html) for more details.

### 5.5.3. [Tiled Mode](#tensor-tiled-mode)[](#tensor-tiled-mode "Permalink to this headline")

This section talks about how Tensor and Tensor access work in tiled mode.

#### 5.5.3.1. [Bounding Box](#tensor-tiled-mode-bounding-box)[](#tensor-tiled-mode-bounding-box "Permalink to this headline")

A tensor can be accessed in chunks known as *Bounding Box*. The Bounding Box has the same
dimensionality as the tensor they are accessing into. Size of each bounding Box must be a multiple
of 16 bytes. The address of the bounding Box must also be aligned to 16 bytes.

Bounding Box has the following access properties:

* Bounding Box dimension sizes
* Out of boundary access mode
* Traversal strides

The tensor-coordinates, specified in the PTX tensor instructions, specify the starting offset of the
bounding box. Starting offset of the bounding box along with the rest of the bounding box
information together are used to determine the elements which are to be accessed.

#### 5.5.3.2. [Traversal-Stride](#tensor-tiled-mode-traversal-stride)[](#tensor-tiled-mode-traversal-stride "Permalink to this headline")

While the Bounding Box is iterating the tensor across a dimension, the traversal stride specifies
the exact number of elements to be skipped. If no jump over is required, default value of 1 must be
specified.

The traversal stride in dimension 0 can be used for the [Interleave layout](#tensor-interleaved-layout).
For non-interleaved layout, the traversal stride in
dimension 0 must always be 1.

[Figure 8](#tensor-tiled-mode-bb-example) illustrates tensor, tensor size, tensor stride,
Bounding Box size and traversal stride.

![_images/tensor-tiled-mode-bounding-box-example.png](_images/tensor-tiled-mode-bounding-box-example.png)


Figure 8 Tiled mode bounding box, tensor size and traversal stride[](#tensor-tiled-mode-bb-example "Permalink to this image")

#### 5.5.3.3. [Out of Boundary Access](#tensor-tiled-mode-oob-access)[](#tensor-tiled-mode-oob-access "Permalink to this headline")

PTX Tensor operation can detect and handle the case when the Bounding Box crosses the tensor
boundary in any dimension. There are 2 modes:

* Zero fill mode:

  Elements in the Bounding Box which fall outside of the tensor boundary are set to 0.
* `OOB-NaN` fill mode:

  Elements in the Bounding Box which fall outside of the tensor boundary are set to a special NaN
  called `OOB-NaN`.

[Figure 9](#tensor-oob-access) shows an example of the out of boundary access.

![_images/tensor-oob-access.png](_images/tensor-oob-access.png)


Figure 9 Out of boundary access[](#tensor-oob-access "Permalink to this image")

#### 5.5.3.4. [`.tile::scatter4` and `.tile::gather4` modes](#tensor-tiled-scatter4-gather4-modes)[](#tensor-tiled-scatter4-gather4-modes "Permalink to this headline")

These modes are similar to the tiled mode with restriction that these modes work only on 2D tensor data.
`Tile::scatter4` and `Tile::gather4` modes are used to access multiple non-contiguous rows of tensor data.

In `Tile::scatter4` mode single 2D source tensor is divided into four rows in the 2D destination tensor.
In `Tile::gather4` mode four rows in the source 2D tensor are combined to form single 2D destination tensor.

These modes work on four rows and hence the instruction will take:

1. four tensor coordinates across the dimension 0
2. one tensor coordinate across the dimension 1

The interleave layout is not supported for `.tile::scatter4` and `.tile::gather4` modes.

All other constraints and rules of the tile mode apply to these modes as well.

##### 5.5.3.4.1. [Bounding Box](#tensor-tiled-scatter4-gather4-modes-bounding-box)[](#tensor-tiled-scatter4-gather4-modes-bounding-box "Permalink to this headline")

For `Tile::scatter4` and `Tile::gather4` modes, four request coordinates will form four Bounding
Boxes in the tensor space.

[Figure 10](#tiled-scatter4-gather4-bounding-box) shows an example of the same with start
coordinates (1, 2), (1, 5), (1, 0) and (1, 9).

The size of the bounding box in the dimension 0 represents the length of the rows.
The size of the bounding box in the dimension 1 must be one.

![_images/tiled-scatter4-gather4-bounding-box.png](_images/tiled-scatter4-gather4-bounding-box.png)


Figure 10 tiled::scatter4/tiled::gather4 mode bounding box example[](#tiled-scatter4-gather4-bounding-box "Permalink to this image")

### 5.5.4. [`im2col` mode](#tensor-im2col-mode)[](#tensor-im2col-mode "Permalink to this headline")

Im2col mode supports the following tensor dimensions : 3D, 4D and 5D. In this mode, the tensor data
is treated as a batch of images with the following properties:

* N : number of images in the batch
* D, H, W : size of a 3D image (depth, height and width)
* C: channels per image element

The above properties are associated with 3D, 4D and 5D tensors as follows:

| Dimension | N/D/H/W/C applicability |
| --- | --- |
| 3D | NWC |
| 4D | NHWC |
| 5D | NDHWC |

#### 5.5.4.1. [Bounding Box](#tensor-im2col-mode-bounding-box)[](#tensor-im2col-mode-bounding-box "Permalink to this headline")

In im2col mode, the Bounding Box is defined in DHW space. Boundaries along other dimensions are
specified by Pixels-per-Column and Channels-per-Pixel parameters as described below.

The dimensionality of the Bounding Box is two less than the tensor dimensionality.

The following properties describe how to access of the elements in im2col mode:

* Bounding-Box Lower-Corner
* Bounding-Box Upper-Corner
* Pixels-per-Column
* Channels-per-Pixel

*Bounding-box Lower-Corner* and *Bounding-box Upper-Corner* specify the two opposite corners of the
Bounding Box in the DHW space. *Bounding-box Lower-Corner* specifies the corner with the smallest
coordinate and *Bounding-box Upper-Corner* specifies the corner with the largest coordinate.

*Bounding-box Upper-* and *Lower-Corners* are 16-bit signed values whose limits varies across the
dimensions and are as shown below:

|  | 3D | 4D | 5D |
| --- | --- | --- | --- |
| Upper- / Lower- Corner sizes | [-215, 215-1] | [-27, 27-1] | [-24, 24-1] |

[Figure 11](#im2col-mode-bounding-box1) and [Figure 12](#im2col-mode-bounding-box2) show the Upper-Corners and Lower-Corners.

![_images/tensor-im2col-mode-bounding-box1.png](_images/tensor-im2col-mode-bounding-box1.png)


Figure 11 im2col mode bounding box example 1[](#im2col-mode-bounding-box1 "Permalink to this image")


![_images/tensor-im2col-mode-bounding-box2.png](_images/tensor-im2col-mode-bounding-box2.png)


Figure 12 im2col mode bounding box example 2[](#im2col-mode-bounding-box2 "Permalink to this image")

The *Bounding-box Upper-* and *Lower- Corners* specify only the boundaries and not the number of
elements to be accessed. *Pixels-per-Column* specifies the number of elements to be accessed in the
NDHW space.

*Channels-per-Pixel* specifies the number of elements to access across the C dimension.

The tensor coordinates, specified in the PTX tensor instructions, behaves differently in different
dimensions:

* Across N and C dimensions: specify the starting offsets along the dimension, similar to the tiled
  mode.
* Across DHW dimensions: specify the location of the convolution filter base in the tensor
  space. The filter corner location must be within the bounding box.

The im2col offsets, specified in the PTX tensor instructions in im2col mode, are added to the filter
base coordinates to determine the starting location in the tensor space from where the elements are
accessed.

The size of the im2col offsets varies across the dimensions and their valid ranges are as shown
below:

|  | 3D | 4D | 5D |
| --- | --- | --- | --- |
| im2col offsets range | [0, 216-1] | [0, 28-1] | [0, 25-1] |

Following are some examples of the im2col mode accesses:

* Example 1 ([Figure 13](#tensor-im2col-mode-example1)):

  ```
  Tensor Size[0] = 64

  Tensor Size[1] = 9

  Tensor Size[2] = 14

  Tensor Size[3] = 64

  Pixels-per-Column = 64

  channels-per-pixel = 8

  Bounding-Box Lower-Corner W = -1

  Bounding-Box Lower-Corner H = -1

  Bounding-Box Upper-Corner W = -1

  Bounding-Box Upper-Corner H = -1.



  tensor coordinates = (7, 7, 4, 0)

  im2col offsets : (0, 0)
  ```

  ![_images/tensor-im2col-mode-example1.png](_images/tensor-im2col-mode-example1.png)


  Figure 13 im2col mode example 1[](#tensor-im2col-mode-example1 "Permalink to this image")
* Example 2 ([Figure 14](#tensor-im2col-mode-example2)):

  ```
  Tensor Size[0] = 64

  Tensor Size[1] = 9

  Tensor Size[2] = 14

  Tensor Size[3] = 64

  Pixels-per-Column = 64

  channels-per-pixel = 8

  Bounding-Box Lower-Corner W = 0

  Bounding-Box Lower-Corner H = 0

  Bounding-Box Upper-Corner W = -2

  Bounding-Box Upper-Corner H = -2



  tensor coordinates = (7, 7, 4, 0)

  im2col offsets: (2, 2)
  ```

  ![_images/tensor-im2col-mode-example2.png](_images/tensor-im2col-mode-example2.png)


  Figure 14 im2col mode example 2[](#tensor-im2col-mode-example2 "Permalink to this image")

#### 5.5.4.2. [Traversal Stride](#tensor-im2col-mode-traversal-stride)[](#tensor-im2col-mode-traversal-stride "Permalink to this headline")

The traversal stride, in im2col mode, does not impact the total number of elements (or pixels) being
accessed unlike the tiled mode. Pixels-per-Column determines the total number of elements being
accessed, in im2col mode.

The number of elements traversed along the D, H and W dimensions is strided by the traversal stride
for that dimension.

The following example with [Figure 15](#tensor-im2col-mode-example3) illustrates accesse with traversal-strides:

```
Tensor Size[0] = 64

Tensor Size[1] = 8

Tensor Size[2] = 14

Tensor Size[3] = 64

Traversal Stride = 2

Pixels-per-Column = 32

channels-per-pixel = 16

Bounding-Box Lower-Corner W = -1

Bounding-Box Lower-Corner H = -1

Bounding-Box Upper-Corner W = -1

Bounding-Box Upper-Corner H = -1.

Tensor coordinates in the instruction = (7, 7, 5, 0)

Im2col offsets in the instruction : (1, 1)
```

![_images/tensor-im2col-mode-example3.png](_images/tensor-im2col-mode-example3.png)


Figure 15 im2col mode traversal stride example[](#tensor-im2col-mode-example3 "Permalink to this image")

#### 5.5.4.3. [Out of Boundary Access](#tensor-im2col-mode-oob-access)[](#tensor-im2col-mode-oob-access "Permalink to this headline")

In im2col mode, when the number of requested pixels in NDHW space specified by *Pixels-per-Column*
exceeds the number of available pixels in the image batch then out-of-bounds access is performed.

Similar to tiled mode, zero fill or `OOB-NaN` fill can be performed based on the Fill-Mode
specified.

### 5.5.5. [`im2col::w` and `im2col::w::128` modes](#tensor-im2col-w-w128-modes)[](#tensor-im2col-w-w128-modes "Permalink to this headline")

These modes are similar to the im2col mode with the restriction that elements are accessed across
the `W` dimension only while keeping the `H` and `D` dimension constant.

All the constraints and rules of the im2col mode apply to these modes as well.

The number of elements accessed in the `im2col::w::128` mode is fixed and is equal to 128.
The number of elements accessed in the `im2col::w` mode depends on the field Pixels-per-Column
field in the TensorMap.

#### 5.5.5.1. [Bounding Box](#tensor-im2col-w-w128-modes-bounding-box)[](#tensor-im2col-w-w128-modes-bounding-box "Permalink to this headline")

In these modes, the size of the bounding box in `D` and `H` dimensions are 1.

The `D` and `H` dimensions in the tensor coordinates argument in the PTX instruction specify
the position of the bounding box in the tensor space.

The Bounding-Box `Lower-Corner-W` and Bounding-Box `Upper-Corner-W` specify the two opposite
corners of the Bounding Box in the `W` dimension.

The `W` dimension in the tensor coordinates argument in the PTX instruction specify the location
of the first element that is to be accessed in the bounding box.

Number of pixels loaded in `im2col::w` mode is as specified by Pixels-per-Column in the TensorMap.
Number of pixels loaded in `im2col::w::128` mode is always 128. So, Pixels-per-Column is ignored
in `im2col::w::128` mode.

[Figure 16](#tensor-im2col-w-w128-modes-example) shows an example of the `im2col::w` and
`im2col::w:128` modes.

![_images/tensor-im2col-w-w128-modes-example.png](_images/tensor-im2col-w-w128-modes-example.png)


Figure 16 im2col::w and im2col::w::128 modes example[](#tensor-im2col-w-w128-modes-example "Permalink to this image")

The first element can lie outside of the Bounding Box in the W-dimension only and only on the left
side of the Bounding Box. [Figure 17](#tensor-im2col-w-w128-modes-example2) shows of an example of this.

![_images/tensor-im2col-w-w128-modes-example2.png](_images/tensor-im2col-w-w128-modes-example2.png)


Figure 17 im2col::w and im2col::w::128 modes first element outside Bounding Box example[](#tensor-im2col-w-w128-modes-example2 "Permalink to this image")

#### 5.5.5.2. [Traversal Stride](#tensor-im2col-w-w128-modes-traversal-stride)[](#tensor-im2col-w-w128-modes-traversal-stride "Permalink to this headline")

This is similar to im2col mode with the exception of that the number of elements traversed
along only the `W` dimension is strided by the traversal stride as specified in the TensorMap.

#### 5.5.5.3. [`wHalo`](#tensor-im2col-w-w128-modes-whalo)[](#tensor-im2col-w-w128-modes-whalo "Permalink to this headline")

In `im2col::w` mode, the `wHalo` argument in the PTX instruction specifies how many filter
halo elements must be loaded at the end of the image.

In `im2col::w::128` mode, the halo elements are loaded after every 32 elements in the bounding
box along the `W` dimension. The `wHalo` argument in the PTX instruction specifies how many
halo elements must be loaded after every 32 elements.

Following is an example of `.im2col::w` mode access:

```
Tensor Size [0] = 128

Tensor Size [1] = 9

Tensor Size [2] = 7

Tensor Size [3] = 64

Pixels-per-column = 128

Channels-per-pixel = 64

Bounding Box Lower Corner W = 0

Bounding Box Upper Corner W = 0



Tensor Coordinates in the instruction = (7, 2, 3, 0)

wHalo in the instruction = 2 (as 3x3 convolution filter is used)
```

A tensor copy operation with the above parameters loads 128 pixels and the two halo pixels as shown in
[Figure 18](#tensor-im2col-w-w128-modes-example3).

![_images/tensor-im2col-w-w128-modes-example3.png](_images/tensor-im2col-w-w128-modes-example3.png)


Figure 18 tensor copy operation with im2col::w mode example[](#tensor-im2col-w-w128-modes-example3 "Permalink to this image")

The halo pixels are always loaded in the shared memory next to the main row pixels as shown in
[Figure 18](#tensor-im2col-w-w128-modes-example3).

Following is an example of `.im2col::w::128` mode access:

```
Tensor Size [0] = 128

Tensor Size [1] = 9

Tensor Size [2] = 7

Tensor Size [3] = 64

Channels-per-pixel = 64

Bounding Box Lower Corner W = 0

Bounding Box Upper Corner W = 0



Tensor Coordinates in the instruction = (7, 2, 3, 0)

wHalo in the instruction = 2 (as 3x3 convolution filter is used)
```

A tensor copy operation with the above parameters loads 128 elements such that after every 32 elements,
wHalo number of elements are loaded as shown in [Figure 19](#tensor-im2col-w-w128-modes-example4).

![_images/tensor-im2col-w-w128-modes-example4.png](_images/tensor-im2col-w-w128-modes-example4.png)


Figure 19 tensor copy operation with im2col::w::128 mode example[](#tensor-im2col-w-w128-modes-example4 "Permalink to this image")

#### 5.5.5.4. [`wOffset`](#tensor-im2col-w-w128-modes-woffset)[](#tensor-im2col-w-w128-modes-woffset "Permalink to this headline")

In the convolution calculations, the same elements along the `W` dimension are reused for different
locations within the convolution filter footprint. Based on the number of times a pixel is used, the
pixels may be loaded into different shared memory buffers. Each buffer can be loaded by a separate
tensor copy operation.

The `wOffset` argument in the tensor copy and prefetch instruction adjusts the source pixel location
for each buffer. The exact position of the buffer is adjusted along the `W` dimension using the
following formula:

```
Bounding Box Lower Corner W += wOffset

Bounding Box Upper Corner W += wOffset

W += wOffset
```

Following are examples of tensor copy to multiple buffers with various `wHalo` and `wOffset` values:

Example 1:

```
Tensor Size [0] = 128

Tensor Size [1] = 9

Tensor Size [2] = 67

Tensor Size [3] = 64

Pixels-per-Column = 128

Channels-per-pixel = 64

Bounding Box Lower Corner W = -1

Bounding Box Upper Corner W = 0

Traversal Stride = 2



Tensor Coordinates in the instruction = (7, 2, -1, 0)



Shared memory buffer 1:

   wHalo = 1

   wOffset = 0



Shared memory buffer 2:

   wHalo = 0

   wOffset = 1
```

![_images/tensor-im2col-w-w128-modes-example5.png](_images/tensor-im2col-w-w128-modes-example5.png)


Figure 20 tensor copy operation to buffer 1 of Example 1[](#tensor-im2col-w-w128-modes-example5 "Permalink to this image")


![_images/tensor-im2col-w-w128-modes-example6.png](_images/tensor-im2col-w-w128-modes-example6.png)


Figure 21 tensor copy operation to buffer 2 of Example 1[](#tensor-im2col-w-w128-modes-example6 "Permalink to this image")

Example 2:

```
Tensor Size [0] = 128

Tensor Size [1] = 7

Tensor Size [2] = 7

Tensor Size [3] = 64

Pixels-per-Column = 128

Channels-per-pixel = 64

Bounding Box Lower Corner W = -1

Bounding Box Upper Corner W = -1

Traversal Stride = 3



Tensor Coordinates in the instruction = (7, 2, -1, 0)



Shared memory buffer 1:

   wHalo = 0

   wOffset = 0



Shared memory buffer 2:

   wHalo = 0

   wOffset = 1



Shared memory buffer 3:

   wHalo = 0

   wOffset = 2
```

![_images/tensor-im2col-w-w128-modes-example7.png](_images/tensor-im2col-w-w128-modes-example7.png)


Figure 22 tensor copy operation to buffer 1 of Example 2[](#tensor-im2col-w-w128-modes-example7 "Permalink to this image")


![_images/tensor-im2col-w-w128-modes-example8.png](_images/tensor-im2col-w-w128-modes-example8.png)


Figure 23 tensor copy operation to buffer 2 of Example 2[](#tensor-im2col-w-w128-modes-example8 "Permalink to this image")


![_images/tensor-im2col-w-w128-modes-example9.png](_images/tensor-im2col-w-w128-modes-example9.png)


Figure 24 tensor copy operation to buffer 3 of Example 2[](#tensor-im2col-w-w128-modes-example9 "Permalink to this image")

### 5.5.6. [Interleave layout](#tensor-interleaved-layout)[](#tensor-interleaved-layout "Permalink to this headline")

Tensor can be interleaved and the following interleave layouts are supported:

* No interleave (NDHWC)
* 8 byte interleave (NC/8DHWC8) : C8 utilizes 16 bytes in memory assuming 2B per channel.
* 16 byte interleave (NC/16HWC16) : C16 utilizes 32 bytes in memory assuming 4B per channel.

The *C* information is organized in slices where sequential C elements are grouped in 16 byte or 32
byte quantities.

If the total number of channels is not a multiple of the number of channels per slice, then the last
slice must be padded with zeros to make it complete 16B or 32B slice.

Interleaved layouts are supported only for the dimensionalities : 3D, 4D and 5D.

The interleave layout is not supported for `.im2col::w` and `.im2col::w::128` modes.

### 5.5.7. [Swizzling Modes](#tensor-swizzling-modes)[](#tensor-swizzling-modes "Permalink to this headline")

The layout of the data in the shared memory can be different to that of global memory, for access
performance reasons. The following describes various swizzling modes:

* No swizzle mode:

  There is no swizzling in this mode and the destination data layout is exactly similar to the
  source data layout.

  |  |  |  |  |  |  |  |  |
  | --- | --- | --- | --- | --- | --- | --- | --- |
  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
  | … Pattern repeats … | | | | | | | |
* 32 byte swizzle mode:

  The following table, where each elements (numbered cell) is 16 byte and the starting address is
  256 bytes aligned, shows the pattern of the destination data layout:

  |  |  |  |  |  |  |  |  |
  | --- | --- | --- | --- | --- | --- | --- | --- |
  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
  | 1 | 0 | 3 | 2 | 5 | 4 | 7 | 6 |
  | … Pattern repeats … | | | | | | | |

  An example of the 32 byte swizzle mode for NC/(32B)HWC(32B) tensor of 1x2x10x10xC16 dimension,
  with the innermost dimension holding slice of 16 channels with 2 byte/channel, is shown in
  [Figure 25](#tensor-32b-swizzle).

  ![_images/tensor-32B-swizzle.png](_images/tensor-32B-swizzle.png)


  Figure 25 32-byte swizzle mode example[](#tensor-32b-swizzle "Permalink to this image")

  [Figure 26](#tensor-32b-swizzle-frag) shows the two fragments of the tensor : one for C/(32B) = 0 and another for C/(32B) = 1.

  ![_images/tensor-32B-swizzle-frag.png](_images/tensor-32B-swizzle-frag.png)


  Figure 26 32-byte swizzle mode fragments[](#tensor-32b-swizzle-frag "Permalink to this image")

  [Figure 27](#tensor-32b-swizzle-dst) shows the destination data layout with 32 byte swizzling.

  ![_images/tensor-32B-swizzle-dst.png](_images/tensor-32B-swizzle-dst.png)


  Figure 27 32-byte swizzle mode destination data layout[](#tensor-32b-swizzle-dst "Permalink to this image")
* 64 byte swizzle mode:

  The following table, where each elements (numbered cell) is 16 byte and the starting address is
  512 bytes aligned, shows the pattern of the destination data layout:

  |  |  |  |  |  |  |  |  |
  | --- | --- | --- | --- | --- | --- | --- | --- |
  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
  | 1 | 0 | 3 | 2 | 5 | 4 | 7 | 6 |
  | 2 | 3 | 0 | 1 | 6 | 7 | 4 | 5 |
  | 3 | 2 | 1 | 0 | 7 | 6 | 5 | 4 |
  | … Pattern repeats … | | | | | | | |

  An example of the 64 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /
  channel and 32 channels, is shown in [Figure 28](#tensor-64b-swizzle).

  ![_images/tensor-64B-swizzle.png](_images/tensor-64B-swizzle.png)


  Figure 28 64-byte swizzle mode example[](#tensor-64b-swizzle "Permalink to this image")

  Each colored cell represents 8 channels. [Figure 29](#tensor-64b-swizzle-src) shows the source data layout.

  ![_images/tensor-64B-swizzle-src.png](_images/tensor-64B-swizzle-src.png)


  Figure 29 64-byte swizzle mode source data layout[](#tensor-64b-swizzle-src "Permalink to this image")

  [Figure 30](#tensor-64b-swizzle-dst) shows the destination data layout with 64 byte swizzling.

  ![_images/tensor-64B-swizzle-dst.png](_images/tensor-64B-swizzle-dst.png)


  Figure 30 64-byte swizzle mode destination data layout[](#tensor-64b-swizzle-dst "Permalink to this image")
* 96 byte swizzle mode:

  The following table where each element (numbered cell) is 16 byte shows the swizzling pattern at the destination
  data layout:

  |  |  |  |  |  |  |  |  |
  | --- | --- | --- | --- | --- | --- | --- | --- |
  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
  | 1 | 0 | 3 | 2 | 5 | 4 | 7 | 6 |
  | … Pattern repeats … | | | | | | | |

  An example of the data layout in global memory and its swizzled data layout in shared memory where each element
  (colored cell) is 16 bytes and the starting address is 256 bytes aligned is shown in [Figure 31](#tensor-96b-swizzle).

  ![_images/tensor-96B-swizzle.png](_images/tensor-96B-swizzle.png)


  Figure 31 96-byte swizzle mode example[](#tensor-96b-swizzle "Permalink to this image")
* 128 byte swizzle mode:

  The 128-byte swizzling mode supports the following sub-modes:

  + 16-byte atomicity sub-mode:

    In this sub-mode, the 16-byte of data is kept intact while swizzling.

  The following table, where each elements (numbered cell) is 16 byte and the starting address is
  1024 bytes aligned, shows the pattern of the destination data layout:

  |  |  |  |  |  |  |  |  |
  | --- | --- | --- | --- | --- | --- | --- | --- |
  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
  | 1 | 0 | 3 | 2 | 5 | 4 | 7 | 6 |
  | 2 | 3 | 0 | 1 | 6 | 7 | 4 | 5 |
  | 3 | 2 | 1 | 0 | 7 | 6 | 5 | 4 |
  | 4 | 5 | 6 | 7 | 0 | 1 | 2 | 3 |
  | 5 | 4 | 7 | 6 | 1 | 0 | 3 | 2 |
  | 6 | 7 | 4 | 5 | 2 | 3 | 0 | 1 |
  | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |
  | … Pattern repeats … | | | | | | | |

  An example of the 128 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes /
  channel and 64 channels, is shown in [Figure 32](#tensor-128b-swizzle).

  ![_images/tensor-128B-swizzle.png](_images/tensor-128B-swizzle.png)


  Figure 32 128-byte swizzle mode example[](#tensor-128b-swizzle "Permalink to this image")

  Each colored cell represents 8 channels. [Figure 33](#tensor-128b-swizzle-src) shows the source data layout.

  ![_images/tensor-128B-swizzle-src.png](_images/tensor-128B-swizzle-src.png)


  Figure 33 128-byte swizzle mode source data layout[](#tensor-128b-swizzle-src "Permalink to this image")

  [Figure 34](#tensor-128b-swizzle-dst) shows the destination data layout with 128 byte swizzling.

  ![_images/tensor-128B-swizzle-dst.png](_images/tensor-128B-swizzle-dst.png)


  Figure 34 128-byte swizzle mode destination data layout[](#tensor-128b-swizzle-dst "Permalink to this image")

  + 32-byte atomicity sub-mode:

    In this sub-mode, the 32-byte of data is kept intact while swizzling.

    The following table where each element (numbered cell) is 16 byte shows the
    swizzling pattern at the destination data layout:

    |  |  |  |  |
    | --- | --- | --- | --- |
    | 0 1 | 2 3 | 4 5 | 6 7 |
    | 2 3 | 0 1 | 6 7 | 4 5 |
    | 4 5 | 6 7 | 0 1 | 2 3 |
    | 6 7 | 4 5 | 2 3 | 0 1 |
    | … Pattern repeats … | | | |

    This sub-mode requires 32 byte alignment at shared memory.

    An example of the data layout in global memory and its swizzled data layout in shared memory
    where each element (colored cell) is 16 bytes is shown in [Figure 35](#tensor-128b-swizzle-32b-atom)

    ![_images/tensor-128B-swizzle-32B-atom.png](_images/tensor-128B-swizzle-32B-atom.png)


    Figure 35 128-byte swizzle mode example with 32-byte atomicity[](#tensor-128b-swizzle-32b-atom "Permalink to this image")
  + 32-byte atomicity with 8-byte flip sub-mode:

    The swizzling pattern for this sub-mode is similar to the 32-byte atomicity sub-mode except that
    there is a flip of adjacent 8-bytes within the 16-byte data at every alternate shared memory line.

    An example of the data layout in global memory and its swizzled data layout in shared memory where
    each element (colored cell) is 16 bytes (two 8-byte sub-elements for each 16-byte colored cell are
    shown to show the flip) is shown in [Figure 36](#tensor-128b-swizzle-32b-atom-8b-flip)

    ![_images/tensor-128B-swizzle-32B-atom-8B-flip.png](_images/tensor-128B-swizzle-32B-atom-8B-flip.png)


    Figure 36 128-byte swizzle mode example with 32-byte atomicity with 8-byte flip[](#tensor-128b-swizzle-32b-atom-8b-flip "Permalink to this image")
  + 64-byte atomicity sub-mode:

    In this sub-mode, the 64-byte of data is kept intact while swizzling.

    The following table where each element (numbered cell) is 16 byte shows the swizzling
    pattern at the destination data layout:

    |  |  |
    | --- | --- |
    | 0 1 2 3 | 4 5 6 7 |
    | 4 5 6 7 | 0 1 2 3 |
    | … Pattern repeats … | |

    This sub-mode requires 64-byte alignment at shared memory.

    An example of the data layout in global memory and its swizzled data layout
    in shared memory where each element (colored cell) is 16 bytes is shown
    in [Figure 37](#tensor-128b-swizzle-64b-atom)

    ![_images/tensor-128B-swizzle-64B-atom.png](_images/tensor-128B-swizzle-64B-atom.png)


    Figure 37 128-byte swizzle mode example with 64-byte atomicity[](#tensor-128b-swizzle-64b-atom "Permalink to this image")

[Table 14](#valid-combination-of-swizzle-atomicity-with-swizzling-mode)
lists the valid combination of swizzle-atomicity with the swizzling-mode.

Table 14 Valid combination of swizzle-atomicity with swizzling-mode[](#valid-combination-of-swizzle-atomicity-with-swizzling-mode "Permalink to this table")




| Swizzling Mode | Swizzle-Atomicity |
| --- | --- |
| No Swizzling | – |
| 32B Swizzling Mode | 16B |
| 64B Swizzling Mode | 16B |
| 96B Swizzling Mode | 16B |
| 128B Swizzling Mode | * 16B * 32B * 32B + 8B-flip * 64B |

The value of swizzle base offset is 0 when the `dstMem` shared memory address is located
at the following boundary:

| Swizzling Mode | Starting address of the repeating pattern |
| --- | --- |
| 128-Byte swizzle | 1024-Byte boundary |
| 96-Byte swizzle | 256-Byte boundary |
| 64-Byte swizzle | 512-Byte boundary |
| 32-Byte swizzle | 256-Byte boundary |

Otherwise, the swizzle base offset is a non-zero value, computed using following formula:

| Swizzling Mode | Formula |
| --- | --- |
| 128-Byte swizzle | base offset = (dstMem / 128) % 8 |
| 96-Byte swizzle | base offset = (dstMem / 128) % 2 |
| 64-Byte swizzle | base offset = (dstMem / 128) % 4 |
| 32-Byte swizzle | base offset = (dstMem / 128) % 2 |

### 5.5.8. [Tensor-map](#tensor-tensormap)[](#tensor-tensormap "Permalink to this headline")

The tensor-map is a 128-byte opaque object either in `.const` space or `.param` (kernel function
parameter) space or `.global` space which describes the tensor properties and the access properties
of the tensor data described in previous sections.

Tensor-Map can be created using CUDA APIs. Refer to *CUDA programming guide* for more details.