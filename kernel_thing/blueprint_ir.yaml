metadata:
  schema_version: 0.1
  target_sm: 75
  syntax_spec: "ptx_sm75_syntax_table.yaml"
  description: "High-level kernel blueprint IR for PTX auto-generation on sm_75."
  goals:
    - "Enable deterministic construction of runnable kernels from problem specs."
    - "Expose hooks for search/planning models to explore alternative tilings, epilogues, and instrumentation."

problem_blueprint:
  tensor_roles:
    - {name: INPUT_0, attributes: [read_only, strided], required: true}
    - {name: INPUT_1, attributes: [read_only, optional]}
    - {name: OUTPUT_0, attributes: [write_only], required: true}
    - {name: AUX_0, attributes: [read_write, shared_pool], required: false}
  dimension_symbols:
    - {name: DIM_M, type: int, description: "Primary row extent"}
    - {name: DIM_N, type: int, description: "Primary column extent"}
    - {name: DIM_K, type: int, description: "Inner reduction extent"}
    - {name: BATCH, type: int, description: "Batch or slice count"}
  parameter_pack_schema:
    ptr_fields:
      - {id: PTR_INPUT_0, dtype: b64}
      - {id: PTR_INPUT_1, dtype: b64, optional: true}
      - {id: PTR_OUTPUT_0, dtype: b64}
      - {id: PTR_AUX_0, dtype: b64, optional: true}
    scalar_fields:
      - {id: STRIDE_INPUT_0_M, dtype: s32}
      - {id: STRIDE_INPUT_0_K, dtype: s32}
      - {id: STRIDE_OUTPUT_0_M, dtype: s32}
      - {id: DIM_M, dtype: s32}
      - {id: DIM_N, dtype: s32}
      - {id: DIM_K, dtype: s32}
      - {id: BATCH, dtype: s32}
    ordering_rule: "All pointers first (8-byte aligned), then 32-bit scalars"
    notes:
      - "Schema intentionally generic; concrete problems bind symbols later"
  launch_descriptor:
    block_dim_expr: ["BLOCK_M * BLOCK_N / WARP_SIZE", "CTA_HEIGHT", "CTA_DEPTH"]
    grid_dim_expr: ["ceildiv(DIM_M, BLOCK_M)", "ceildiv(DIM_N, BLOCK_N)", "BATCH"]
    dynamic_shared_mem_expr: "shared_tile_bytes * PIPELINE_DEPTH"
    constraints:
      - "blockDim <= 1024 threads"
      - "shared memory per CTA <= 48 KB on sm_75"

host_launch:
  argument_packing:
    pointer_sequence: [PTR_INPUT_0, PTR_INPUT_1?, PTR_OUTPUT_0, PTR_AUX_0?]
    scalar_sequence: [DIM_M, DIM_N, DIM_K, BATCH, STRIDES...]
    alignment_bytes: 8
    rules:
      - "Insert padding so each field starts on alignment boundary"
  dynamic_shared_mem:
    expr: "SHARED_BYTES_PER_STAGE * PIPELINE_DEPTH"
    control: "cudaLaunchKernel third parameter"
  stream_selection:
    default_stream: 0
    policy: "One stream per benchmark scenario"

memory_blueprints:
  shared_tiles:
    schema:
      - {id: TILE_A, element_type: symbol, shape_expr: [BLOCK_M, TILE_K], double_buffered: boolean}
      - {id: TILE_B, element_type: symbol, shape_expr: [TILE_K, BLOCK_N], double_buffered: boolean}
    constraints:
      - "Sum of shared tile bytes <= shared memory limit"
  const_regions:
    schema:
      - {id: CONST_TABLE_0, element_type: symbol, length_expr: symbol}
  local_buffers:
    schema:
      - {id: LOCAL_ACCUM, size_expr: "register_count * 4"}

phase_graph:
  nodes:
    - id: PHASE_ENTRY
      kind: linear
      default_instructions: ["ld.param", "cvta", "mov", "setp", "mad.wide"]
    - id: PHASE_TILE_LOOP
      kind: loop
      loop_var: "k_tile"
      bounds_expr: "DIM_K / TILE_K"
      stages:
        - {name: LOAD, allowed_instructions: ["ld.global", "st.shared", "bar.sync"]}
        - {name: COMPUTE, allowed_instructions: ["ldmatrix", "mma.sync", "fma", "shfl.sync"]}
        - {name: UPDATE, allowed_instructions: ["add", "mad.wide", "setp", "bra"]}
    - id: PHASE_EPILOGUE
      kind: linear
      default_instructions: ["red.global", "atom.global", "st.global", "membar"]
  edges:
    - {from: PHASE_ENTRY, to: PHASE_TILE_LOOP, guard: "true"}
    - {from: PHASE_TILE_LOOP, to: PHASE_EPILOGUE, guard: "loop_end"}

optional_components:
  epilogue_variants:
    schema:
      - {id: DIRECT_STORE, sequence: ["st.global"]}
      - {id: ACCUM_REDUCTION, sequence: ["red.global", "st.global"]}
      - {id: CUSTOM, sequence: []}
    notes:
      - "Generator binds concrete instruction lists based on workload"
  instrumentation_hooks:
    schema:
      - {id: BARRIER_TRACE, insertion_point: "PHASE_TILE_LOOP.END", template: ["bar.sync 0;", "mov.u64 %t, %globaltimer;"]}
      - {id: PREDICATE_ASSERT, insertion_point: "PHASE_ENTRY", template: ["setp.eq.u32 %pX, %tid.x, 0;", "@!%pX trap;"]}

validation_and_testing:
  reference_interfaces: []
  input_generators:
    schema:
      - {id: RANDOM_NORMAL, params: {mean: 0, std: 1}}
      - {id: CONSTANT, params: {value: 0}}
  failure_tags:
    - {tag: syntax_error}
    - {tag: runtime_mismatch}
    - {tag: perf_regression}

profiling_profiles: []

search_spaces:
  tiling_parameters:
    BLOCK_M: {domain: "positive int", description: "Rows per CTA"}
    BLOCK_N: {domain: "positive int", description: "Cols per CTA"}
    TILE_K: {domain: "positive int", description: "Reduction depth per stage"}
  accumulator_layouts:
    schema:
      - {id: ROW_MAJOR_FRAG}
      - {id: COL_MAJOR_FRAG}
  pipeline_depth:
    domain: [1, 2, 3]
    description: "Number of buffered stages"

notes:
  - "All instruction identifiers referenced here must exist in the syntax spec."
  - "Extend problem_families as new kernels (e.g., conv2d) are added."
  - "This IR is intentionally architecture-agnostic apart from shared memory limits; future sm targets can override sections."
