### 9.7.15. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions 

The warpgroup level matrix multiply and accumulate operation has either of the following forms,
where matrix `D` is called accumulator:

* `D = A * B + D`
* `D = A * B`, where the input from accumulator D is disabled.

The `wgmma` instructions perform warpgroup level matrix multiply-and-accumulate operation by
having all threads in a warpgroup collectively perform the following actions:

1. Load matrices A, B and D into registers or into shared memory.
2. Perform the following `fence` operations:

   * `wgmma.fence` operations to indicate that the register/shared-memory across the warpgroup
     have been written into.
   * `fence.proxy.async` operation to make the generic proxy operations visible to the async
     proxy.
3. Issue the asynchronous matrix multiply and accumulate operations using the `wgmma.mma_async`
   operation on the input matrices. The `wgmma.mma_async` operation is performed in the async
   proxy.
4. Create a wgmma-group and commit all the prior outstanding `wgmma.mma_async` operations into the
   group, by using `wgmma.commit_group` operation.
5. Wait for the completion of the required wgmma-group.
6. Once the wgmma-group completes, all the `wgmma.mma_async` operations have been performed and
   completed.

#### 9.7.15.1. [Warpgroup](#asynchronous-warpgroup-level-matrix-instructions-warpgroup)[](#asynchronous-warpgroup-level-matrix-instructions-warpgroup "Permalink to this headline")

A warpgroup is a set of four contiguous warps such that the *warp-rank* of the first warp is a
multiple of 4.

warp-rank of a warp is defined as:

```
(%tid.x + %tid.y * %ntid.x  + %tid.z * %ntid.x * %ntid.y) / 32
```

#### 9.7.15.2. [Matrix Shape](#asynchronous-warpgroup-level-matrix-shape)[](#asynchronous-warpgroup-level-matrix-shape "Permalink to this headline")

The matrix multiply and accumulate operations support a limited set of shapes for the operand
matrices A, B and D. The shapes of all three matrix operands are collectively described by the tuple
`MxNxK`, where A is an `MxK` matrix, B is a `KxN` matrix, while D is a `MxN` matrix.

The following matrix shapes are supported for the specified types for the `wgmma.mma_async`
operation:

| Multiplicand Data type | Sparsity | Shape |
| --- | --- | --- |
| Floating-point - `.f16` | Dense | `.m64n8k16`, `.m64n16k16`, `.m64n24k16`, `.m64n32k16`, `.m64n40k16`, `.m64n48k16`, `.m64n56k16`, `.m64n64k16`, `.m64n72k16`, `.m64n80k16`, `.m64n88k16`, `.m64n96k16`, `.m64n104k16`, `.m64n112k16`, `.m64n120k16`, `.m64n128k16`, `.m64n136k16`, `.m64n144k16`, `.m64n152k16`, `.m64n160k16`, `.m64n168k16`, `.m64n176k16`, `.m64n184k16`, `.m64n192k16`, `.m64n200k16`, `.m64n208k16`, `.m64n216k16`, `.m64n224k16`, `.m64n232k16`, `.m64n240k16`, `.m64n248k16`, `.m64n256k16` |
| Alternate floating-point format - `.bf16` |
| Alternate floating-point format - `.tf32` | Sparse |
| Alternate floating-point format - `.tf32` | Dense | `.m64n8k8`, `.m64n16k8`, `.m64n24k8`, `.m64n32k8`, `.m64n40k8`, `.m64n48k8`, `.m64n56k8`, `.m64n64k8`, `.m64n72k8`, `.m64n80k8`, `.m64n88k8`, `.m64n96k8`, `.m64n104k8`, `.m64n112k8`, `.m64n120k8`, `.m64n128k8`, `.m64n136k8`, `.m64n144k8`, `.m64n152k8`, `.m64n160k8`, `.m64n168k8`, `.m64n176k8`, `.m64n184k8`, `.m64n192k8`, `.m64n200k8`, `.m64n208k8`, `.m64n216k8`, `.m64n224k8`, `.m64n232k8`, `.m64n240k8`, `.m64n248k8`, `.m64n256k8` |
| Alternate floating-point format - `.e4m3`/ `.e5m2` | Dense | `.m64n8k32`, `.m64n16k32`, `.m64n24k32`, `.m64n32k32`, `.m64n40k32`, `.m64n48k32`, `.m64n56k32`, `.m64n64k32`, `.m64n72k32`, `.m64n80k32`, `.m64n88k32`, `.m64n96k32`, `.m64n104k32`, `.m64n112k32`, `.m64n120k32`, `.m64n128k32`, `.m64n136k32`, `.m64n144k32`, `.m64n152k32`, `.m64n160k32`, `.m64n168k32`, `.m64n176k32`, `.m64n184k32`, `.m64n192k32`, `.m64n200k32`, `.m64n208k32`, `.m64n216k32`, `.m64n224k32`, `.m64n232k32`, `.m64n240k32`, `.m64n248k32`, `.m64n256k32` |
| Floating point - `.f16` | Sparse |
| Altername floating-point format - `.bf16` |
| Integer - `.u8`/`.s8` | Dense | `.m64n8k32`, `.m64n16k32`, `.m64n24k32`, `.m64n32k32`, `.m64n48k32`, `.m64n64k32`, `.m64n80k32`, `.m64n96k32`, `.m64n112k32`, `.m64n128k32`, `.m64n144k32`, `.m64n160k32`, `.m64n176k32`, `.m64n192k32`, `.m64n208k32`, `.m64n224k32`, `.m64n240k32`, `.m64n256k32` |
| Alternate floating-point format - `.e4m3`/ `.e5m2` | Sparse | `.m64n8k64`, `.m64n16k64`, `.m64n24k64`, `.m64n32k64`, `.m64n40k64`, `.m64n48k64`, `.m64n56k64`, `.m64n64k64`, `.m64n72k64`, `.m64n80k64`, `.m64n88k64`, `.m64n96k64`, `.m64n104k64`, `.m64n112k64`, `.m64n120k64`, `.m64n128k64`, `.m64n136k64`, `.m64n144k64`, `.m64n152k64`, `.m64n160k64`, `.m64n168k64`, `.m64n176k64`, `.m64n184k64`, `.m64n192k64`, `.m64n200k64`, `.m64n208k64`, `.m64n216k64`, `.m64n224k64`, `.m64n232k64`, `.m64n240k64`, `.m64n248k64`, `.m64n256k64` |
| Integer - `.u8`/`.s8` | Sparse | `.m64n8k64`, `.m64n16k64`, `.m64n24k64`, `.m64n32k64`, `.m64n48k64`, `.m64n64k64`, `.m64n80k64`, `.m64n96k64`, `.m64n112k64`, `.m64n128k64`, `.m64n144k64`, `.m64n160k64`, `.m64n176k64`, `.m64n192k64`, `.m64n208k64`, `.m64n224k64`, `.m64n240k64`, `.m64n256k64` |
| Single-bit - `.b1` | Dense | `.m64n8k256`, `.m64n16k256`, `.m64n24k256`, `.m64n32k256`, `.m64n48k256`, `.m64n64k256`, `.m64n80k256`, `.m64n96k256`, `.m64n112k256`, `.m64n128k256`, `.m64n144k256`, `.m64n160k256`, `.m64n176k256`, `.m64n192k256`, `.m64n208k256`, `.m64n224k256`, `.m64n240k256`, `.m64n256k256` |

#### 9.7.15.3. [Matrix Data-types](#asynchronous-warpgroup-level-matrix-data-types)[](#asynchronous-warpgroup-level-matrix-data-types "Permalink to this headline")

The matrix multiply and accumulate operation is supported separately on integer, floating-point,
sub-byte integer and single bit data-types. All operands must contain the same basic type kind,
i.e., integer or floating-point.

For floating-point matrix multiply and accumulate operation, different matrix operands may have
different precision, as described later.

For integer matrix multiply and accumulate operation, both multiplicand matrices (A and B) must have
elements of the same data-type, e.g. both signed integer or both unsigned integer.

| Data-type | Multiplicands (A or B) | Accumulator (D) |
| --- | --- | --- |
| Integer | both `.u8` or both `.s8` | `.s32` |
| Floating Point | `.f16` | `.f16`, `.f32` |
| Alternate floating Point | `.bf16` | `.f32` |
| Alternate floating Point | `.tf32` | `.f32` |
| Alternate floating Point | `.e4m3`, `.e5m2` | `.f16`, `.f32` |
| Single-bit integer | `.b1` | `.s32` |

#### 9.7.15.4. [Async Proxy](#asynchronous-warpgroup-level-matrix-async-proxy)[](#asynchronous-warpgroup-level-matrix-async-proxy "Permalink to this headline")

The `wgmma.mma_async` operations are performed in the asynchronous proxy (or async proxy).

Accessing the same memory location across multiple proxies needs a cross-proxy fence. For the async
proxy, `fence.proxy.async` should be used to synchronize memory between generic proxy and the
async proxy.

The completion of a `wgmma.mma_async` operation is followed by an implicit generic-async proxy
fence. So the result of the asynchronous operation is made visible to the generic proxy as soon as
its completion is observed. `wgmma.commit_group` and `wgmma.wait_group` operations must be used
to wait for the completion of the `wgmma.mma_async` instructions.

#### 9.7.15.5. [Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using `wgmma.mma_async` instruction](#asynchronous-warpgroup-level-matrix-operation-wgmma-mma-async)[](#asynchronous-warpgroup-level-matrix-operation-wgmma-mma-async "Permalink to this headline")

This section describes warpgroup level `wgmma.mma_async` instruction and the organization of
various matrices involved in this instruction.

##### 9.7.15.5.1. [Register Fragments and Shared Memory Matrix Layouts](#asynchronous-warpgroup-level-matrix-fragment)[](#asynchronous-warpgroup-level-matrix-fragment "Permalink to this headline")

The input matrix A of the warpgroup wide MMA operations can be either in registers or in the shared
memory. The input matrix B of the warpgroup wide MMA operations must be in the shared memory. This
section describes the layouts of register fragments and shared memory expected by the warpgroup MMA
instructions.

When the matrices are in shared memory, their starting addresses must be aligned to 16 bytes.

###### 9.7.15.5.1.1. [Register Fragments](#asynchronous-warpgroup-level-matrix-register-fragment)[](#asynchronous-warpgroup-level-matrix-register-fragment "Permalink to this headline")

This section describes the organization of various matrices located in register operands of the
`wgmma.mma_async` instruction.

###### 9.7.15.5.1.1.1. [Matrix Fragments for `wgmma.mma_async.m64nNk16`](#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n16)[](#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n16 "Permalink to this headline")

A warpgroup executing `wgmma.mma_async.m64nNk16` will compute an MMA operation of shape
`.m64nNk16` where N is a valid `n` dimension as listed in
[Matrix Shape](#asynchronous-warpgroup-level-matrix-shape).

Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.

* Multiplicand A in registers:

  | .atype | Fragment | Elements (low to high) |
  | --- | --- | --- |
  | `.f16`/`.bf16` | A vector expression containing four `.f16x2` registers, with each register containing two `.f16`/ `.bf16` elements from matrix A. | a0, a1, a2, a3, a4, a5, a6, a7 |

  The layout of the fragments held by different threads is shown in [Figure 148](#wgmma-64n16-a).

  ![_images/wgmma-64N16-A.png](_images/wgmma-64N16-A.png)


  Figure 148 WGMMA .m64nNk16 register fragment layout for matrix A.[](#wgmma-64n16-a "Permalink to this image")
* Accumulator D:

  | .dtype | Fragment | Elements (low to high) |
  | --- | --- | --- |
  | `.f16` | A vector expression containing N/4 number of `.f16x2` registers, with each register containing two `.f16` elements from matrix D. | d0, d1, d2, d3, …, dX, dY, dZ, dW  where `X = N/2  -  4`  `Y = N/2  -  3`  `Z = N/2  -  2`  `W = N/2  -  1`  `N = 8*i where i = {1, 2, ... , 32}` |
  | `.f32` | A vector expression containing N/2 number of `.f32` registers. |

  The layout of the fragments held by different threads is shown in [Figure 149](#wgmma-64n16-d).

  ![_images/wgmma-64N16-D.png](_images/wgmma-64N16-D.png)


  Figure 149 WGMMA .m64nNk16 register fragment layout for accumulator matrix D.[](#wgmma-64n16-d "Permalink to this image")

###### 9.7.15.5.1.1.2. [Matrix Fragments for `wgmma.mma_async.m64nNk8`](#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n8)[](#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n8 "Permalink to this headline")

A warpgroup executing `wgmma.mma_async.m64nNk8` will compute an MMA operation of shape
`.m64nNk8` where N is a valid `n` dimension as listed in [Matrix Shape](#asynchronous-warpgroup-level-matrix-shape).

Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.

* Multiplicand A in registers:

  | .atype | Fragment | Elements (low to high) |
  | --- | --- | --- |
  | `.tf32` | A vector expression containing four `.b32` registers containing four `.tf32` elements from matrix A. | a0, a1, a2, a3 |

  The layout of the fragments held by different threads is shown in [Figure 150](#wgmma-64n8-a).

  ![_images/wgmma-64N8-A.png](_images/wgmma-64N8-A.png)


  Figure 150 WGMMA .m64nNk8 register fragment layout for matrix A.[](#wgmma-64n8-a "Permalink to this image")
* Accumulator D:

  | .dtype | Fragment | Elements (low to high) |
  | --- | --- | --- |
  | `.f32` | A vector expression containing N/2 number of `.f32` registers. | d0, d1, d2, d3, …, dX, dY, dZ, dW  where `X = N/2  -  4`  `Y = N/2  -  3`  `Z = N/2  -  2`  `W = N/2  -  1`  `N = 8*i where i = {1, 2, ... , 32}` |

  The layout of the fragments held by different threads is shown in [Figure 151](#wgmma-64n8-d).

  ![_images/wgmma-64N8-D.png](_images/wgmma-64N8-D.png)


  Figure 151 WGMMA .m64nNk8 register fragment layout for accumulator matrix D.[](#wgmma-64n8-d "Permalink to this image")

###### 9.7.15.5.1.1.3. [Matrix Fragments for `wgmma.mma_async.m64nNk32`](#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n32)[](#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n32 "Permalink to this headline")

A warpgroup executing `wgmma.mma_async.m64nNk32` will compute an MMA operation of shape
`.m64nNk32` where N is a valid `n` dimension as listed in
[Matrix Shape](#asynchronous-warpgroup-level-matrix-shape).

Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.

* Multiplicand A in registers:

  | .atype | Fragment | Elements (low to high) |
  | --- | --- | --- |
  | `.s8`/`.u8` | A vector expression containing four `.b32` registers, with each register containing four `.u8`/ `.s8` elements from matrix A. | a0, a1, a2, a3, … , a14, a15 |
  | `.e4m3`/ `.e5m2` | A vector expression containing four `.b32` registers, with each register containing four `.e4m3`/ `.e5m2` elements from matrix A. |

  The layout of the fragments held by different threads is shown in [Figure 152](#wgmma-64n32-a).

  ![_images/wgmma-64N32-A.png](_images/wgmma-64N32-A.png)


  Figure 152 WGMMA .m64nNk32 register fragment layout for matrix A.[](#wgmma-64n32-a "Permalink to this image")
* Accumulator D:

  | .dtype | Fragment | Elements (low to high) | Miscellaneous Information |
  | --- | --- | --- | --- |
  | `.s32` | A vector expression containing N/2 number of `.s32` registers. | d0, d1, d2, d3, …, dX, dY, dZ, dW  where `X = N/2  -  4`  `Y = N/2  -  3`  `Z = N/2  -  2`  `W = N/2  -  1`  `N` depends on .dtype, as described in the next column. | `N = 8*i where i = {1, 2, 3, 4}`  `= 16*i where i = {3, 4, ..., 15, 16}` |
  | `.f32` | A vector expression containing N/2 number of `.f32` registers. | `N = 8*i where i = {1, 2, ... , 32}` |
  | `.f16` | A vector expression containing N/4 number of `.f16x2` registers, with each register containing two `.f16` elements from matrix D. |

  The layout of the fragments held by different threads is shown in [Figure 153](#wgmma-64n32-d).

  ![_images/wgmma-64N32-D.png](_images/wgmma-64N32-D.png)


  Figure 153 WGMMA .m64nNk32 register fragment layout for accumulator matrix D.[](#wgmma-64n32-d "Permalink to this image")

###### 9.7.15.5.1.1.4. [Matrix Fragments for `wgmma.mma_async.m64nNk256`](#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n256)[](#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n256 "Permalink to this headline")

A warpgroup executing `wgmma.mma_async.m64nNk256` will compute an MMA operation of shape
`.m64nNk256` where N is a valid `n` dimension as listed in
[Matrix Shape](#asynchronous-warpgroup-level-matrix-shape).

Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.

* Multiplicand A in registers:

  | .atype | Fragment | Elements (low to high) |
  | --- | --- | --- |
  | `.b1` | A vector expression containing four `.b32` registers, with each register containing thirty two `.b1` element from matrix A. | a0, a1, a2, …, a127 |

  The layout of the fragments held by different threads is shown in [Figure 154](#wgmma-64n256-a).

  ![_images/wgmma-64N256-A.png](_images/wgmma-64N256-A.png)


  Figure 154 WGMMA .m64nNk256 register fragment layout for matrix A.[](#wgmma-64n256-a "Permalink to this image")
* Accumulator D:

  | .dtype | Fragment | Elements (low to high) |
  | --- | --- | --- |
  | `.s32` | A vector expression containing N/2 number of `.s32` registers. | d0, d1, d2, d3, …, dX, dY, dZ, dW  where `X = N/2  -  4`  `Y = N/2  -  3`  `Z = N/2  -  2`  `W = N/2  -  1`  `N = 8*i where i = {1, 2, 3, 4}`  `= 16*i where i = {3, 4, ..., 15, 16}` |

  The layout of the fragments held by different threads is shown in [Figure 155](#wgmma-64n256-d).

  ![_images/wgmma-64N256-D.png](_images/wgmma-64N256-D.png)


  Figure 155 WGMMA .m64nNk256 register fragment layout for accumulator matrix D.[](#wgmma-64n256-d "Permalink to this image")

###### 9.7.15.5.1.2. [Shared Memory Matrix Layout](#asynchronous-warpgroup-level-matrix-shared-memory-layout)[](#asynchronous-warpgroup-level-matrix-shared-memory-layout "Permalink to this headline")

If the argument `imm-trans-a` / `imm-trans-b` of the instruction `wgmma.mma_async{.sp}`
is 0, then *K-major* is used for matrix `A` / `B` respectively. If the value of argument
`imm-trans-a` is 1 then *M-major* is used for matrix `A`. If the value of the argument
`imm-trans-b` is 1, then *N-major* is used for matrix `B`.

In a column-major default BLAS library such as cuBLAS, the matrices `A` and `B` with and
without transpose can be classified as either *K-Major* or *M-or-N-Major* as shown in the
following table:

|  | Non-Transposed | Transposed |
| --- | --- | --- |
| A | K-major | M-major |
| B | K-major | N-major |

To avoid confusion with `A`, `B`, `row-major`, `col-major`, `transpose`, and
`non-transpose`, we will use *MN-Major* and *K-Major* throughout this section.

The matrices in the shared memory are made up of one or more “swizzle layout atom”.
The exact layout of these swizzle atoms depends on the swizzling mode, swizzle-atomicity,
and the leading dimension. The layout of the swizzle are shown in
[Table 38](#asynchronous-warpgroup-level-swizzle-lead-dim).

Table 38 Various combinations of swizzling mode, leading dimension and swizzle-atom layout[](#asynchronous-warpgroup-level-swizzle-lead-dim "Permalink to this table")





| Swizzling mode | Leading Dimension / Major-ness | Swizzle atom layout (128b element) |
| --- | --- | --- |
| 128B Swizzling Mode | M/N | 8x8 |
| K | 8x8 |
| 64B Swizzling Mode | M/N | 4x8 |
| K | 8x4 |
| 32B Swizzling Mode | M/N | 2x8 |
| K | 8x2 |
| None | M/N | 1x8 |
| K | 8x1 |

The above shapes are for elements of size 128 bits. For smaller elements sizes, the same
shapes would get multiplied along the leading dimension by a factor of `128/sizeof_bits(Element)`.
For example, 128B MN major swizzle atom would have a shape of `(8*(128/32))x8 = 32x8` for
`tf32` tensor core inputs.

Examples

The following are some example layouts of *MxK* or *KxN* matrices with various swizzling modes,
and are in units of 128b elements as shown
by each colored cell as shown in
[Figure 156](#async-warpgroup-smem-layout-128b-mn),
[Figure 157](#async-warpgroup-smem-layout-128b-k),
[Figure 158](#async-warpgroup-smem-layout-64b-mn),
[Figure 159](#async-warpgroup-smem-layout-64b-k),
[Figure 160](#async-warpgroup-smem-layout-32b-mn),
[Figure 161](#async-warpgroup-smem-layout-32b-k),
[Figure 162](#async-warpgroup-smem-layout-mn-interleaved),
[Figure 163](#async-warpgroup-smem-layout-k-interleaved).

![_images/async-warpgroup-smem-layout-128B-mn.png](_images/async-warpgroup-smem-layout-128B-mn.png)


Figure 156 MN major 128B swizzling[](#async-warpgroup-smem-layout-128b-mn "Permalink to this image")


![_images/async-warpgroup-smem-layout-128B-k.png](_images/async-warpgroup-smem-layout-128B-k.png)


Figure 157 K major 128B swizzling[](#async-warpgroup-smem-layout-128b-k "Permalink to this image")


![_images/async-warpgroup-smem-layout-64B-mn.png](_images/async-warpgroup-smem-layout-64B-mn.png)


Figure 158 MN major 64B swizzling[](#async-warpgroup-smem-layout-64b-mn "Permalink to this image")


![_images/async-warpgroup-smem-layout-64B-k.png](_images/async-warpgroup-smem-layout-64B-k.png)


Figure 159 K major 64B swizzling[](#async-warpgroup-smem-layout-64b-k "Permalink to this image")


![_images/async-warpgroup-smem-layout-32B-mn.png](_images/async-warpgroup-smem-layout-32B-mn.png)


Figure 160 MN major 32B swizzling[](#async-warpgroup-smem-layout-32b-mn "Permalink to this image")


![_images/async-warpgroup-smem-layout-32B-k.png](_images/async-warpgroup-smem-layout-32B-k.png)


Figure 161 K major 32B swizzling[](#async-warpgroup-smem-layout-32b-k "Permalink to this image")


![_images/async-warpgroup-smem-layout-mn-interleaved.png](_images/async-warpgroup-smem-layout-mn-interleaved.png)


Figure 162 MN major interleaved[](#async-warpgroup-smem-layout-mn-interleaved "Permalink to this image")


![_images/async-warpgroup-smem-layout-k-interleaved.png](_images/async-warpgroup-smem-layout-k-interleaved.png)


Figure 163 K major interleaved[](#async-warpgroup-smem-layout-k-interleaved "Permalink to this image")

Following are some of the examples of the 128B swizzling layout for `tf32` element type.

* K-Major: [Figure 164](#async-warpgroup-smem-layout-128b-k-tf32)

  > ![_images/async-warpgroup-smem-layout-128B-k-tf32.png](_images/async-warpgroup-smem-layout-128B-k-tf32.png)
  >
  >
  > Figure 164 K major[](#async-warpgroup-smem-layout-128b-k-tf32 "Permalink to this image")
* MN-Major: [Figure 165](#async-warpgroup-smem-layout-128b-mn-tf32)

  > ![_images/async-warpgroup-smem-layout-128B-mn-tf32.png](_images/async-warpgroup-smem-layout-128B-mn-tf32.png)
  >
  >
  > Figure 165 MN major[](#async-warpgroup-smem-layout-128b-mn-tf32 "Permalink to this image")

###### 9.7.15.5.1.2.1. [Major-ness supported by Strides](#asynchronous-warpgroup-level-majorness-supported-by-strides)[](#asynchronous-warpgroup-level-majorness-supported-by-strides "Permalink to this headline")

There are two strides involved while accessing a matrix from shared memory:

1. Leading dimension byte offset
2. Stride dimension byte offset

###### 9.7.15.5.1.2.1.1. [Leading Dimension Byte Offset](#asynchronous-warpgroup-level-leading-dimension-byte-offset)[](#asynchronous-warpgroup-level-leading-dimension-byte-offset "Permalink to this headline")

The leading dimension byte offset is defined differently for transposed and non-transposed
matrices. The leading byte offset is defined as follows for matrices whose element types are
normalized to 128-bits:

| Major-ness | Definition |
| --- | --- |
| K-Major | * No-Swizzling: the offset from the first column to the second columns   of the 8x2 tile in the 128-bit element type normalized matrix. * Swizzled layouts: not used, assumed to be 1. |
| MN-Major | * Interleave: offset from the first 8 columns to the next 8 columns. * Swizzled layouts: offset from the first (swizzle-byte-size/16) rows   to the next (swizzle-byte-size/16) rows. |

###### 9.7.15.5.1.2.1.2. [Stride Dimension Byte Offset](#asynchronous-warpgroup-level-stride-dimension-byte-offset)[](#asynchronous-warpgroup-level-stride-dimension-byte-offset "Permalink to this headline")

The stride dimension byte offset is defined differently for transposed and non-transposed
matrices. The stride dimension byte offset is defined as follows for matrices whose element
types are normalized to 128-bits:

| Major-ness | Definition |
| --- | --- |
| K-Major | The offset from the first 8 rows to the next 8 rows. |
| MN-Major | * Interleave: offset from the first row to the next row. * Swizzled layout: offset from the first 8 columns to the next 8   columns |

###### 9.7.15.5.1.2.1.3. [Canonical Layouts](#asynchronous-warpgroup-level-canonical-layouts)[](#asynchronous-warpgroup-level-canonical-layouts "Permalink to this headline")

In terms of [CuTe layouts](https://docs.nvidia.com/cutlass/media/docs/cpp/cute/01_layout.html)
the canonical layout can be expressed as follows:

| Major- ness | Swizzling mode | Canonical Layout without swizzling | [Swizzling](https://github.com/NVIDIA/cutlass/blob/bf9da7b76c766d7ee7d536afc77880a4ef1f1156/include/cute/swizzle.hpp) on the previous column |
| --- | --- | --- | --- |
| MN- major | No-swizzling or Interleaved | ((T,1,m),(8,k)):((1,T,SBO),(1T,LBO)) | Swizzle<0, 4, 3> |
| 32B Swizzling | ((T,2,m),(8,k)):((1,T,LBO),(2T,SBO)) | Swizzle<1, 4, 3> |
| 64B Swizzling | ((T,4,m),(8,k)):((1,T,LBO),(4T,SBO)) | Swizzle<2, 4, 3> |
| 128B Swizzling | ((T,8,m),(8,k)):((1,T,LBO),(8T,SBO)) | Swizzle<3, 4, 3> |
| K- major | No-swizzling or Interleaved | ((8,m),(T,2k)):((1T,SBO),(1,LBO)) | Swizzle<0, 4, 3> |
| 32B Swizzling | ((8,m),(T,2k)):((2T,SBO),(1,T)) | Swizzle<1, 4, 3> |
| 64B Swizzling | ((8,m),(T,2k)):((4T,SBO),(1,T)) | Swizzle<2, 4, 3> |
| 128B Swizzling | ((8,m),(T,2k)):((8T,SBO),(1,T)) | Swizzle<3, 4, 3> |

where

* T = 128 / sizeof-elements-in-bits
  T represents scale factor which normalizes matrix element types to 128-bits.
* m represents the number of repeating patterns across rows.
* k represents the number of repeating patterns across columns.

Examples

* K-Major, no-swizzling and tf32 type: [Figure 166](#async-warpgroup-k-no-swizzle-tf32)

  ![_images/async-warpgroup-k-no-swizzle-tf32.png](_images/async-warpgroup-k-no-swizzle-tf32.png)


  Figure 166 K major, no-swizzling and tf32 type[](#async-warpgroup-k-no-swizzle-tf32 "Permalink to this image")

  the strides and related details are as follows:

  Exact layout : Swizzle<0,4,3> o ((8,2),(4,4)):((4,32),(1,64))

  Canonical Layout :Swizzle<0,4,3> o ((8,m),(T,2k)):((1T,SBO),(1,LBO))

  | Parameters | Value |
  | --- | --- |
  | T | 4 |
  | m | 2 |
  | k | 2 |
  | LBO | 64\*sizeof(tf32) |
  | SBO | 32\*sizeof(tf32) |
  | Encoding of LBO in descriptor | (LBO) >> 4 = 16 |
  | Encoding of SBO in descriptor | (SBO) >> 4 = 8 |
* K-Major, 32B swizzling and tf32 type: [Figure 167](#async-warpgroup-k-32b-swizzle-tf32)

  ![_images/async-warpgroup-k-32B-swizzle-tf32.png](_images/async-warpgroup-k-32B-swizzle-tf32.png)


  Figure 167 K major, 32B swizzling and tf32 type[](#async-warpgroup-k-32b-swizzle-tf32 "Permalink to this image")

  the strides and related details are as follows:

  Exact layout : Swizzle<1,4,3> o ((8,2),(4,4)):((8,64),(1,4))

  Canonical Layout :Swizzle<1,4,3> o ((8,m),(T,2k)):((2T,SBO),(1,T))

  | Parameters | Value |
  | --- | --- |
  | T | 4 |
  | m | 2 |
  | k | 2 |
  | LBO | NA |
  | SBO | 64\*sizeof(tf32) |
  | Encoding of LBO in descriptor | 1 (assumed) |
  | Encoding of SBO in descriptor | (SBO) >> 4 = 16 |
* MN-Major, no-swizzling and bf16 type: [Figure 168](#async-warpgroup-mn-no-swizzle-bf16)

  ![_images/async-warpgroup-mn-no-swizzle-bf16.png](_images/async-warpgroup-mn-no-swizzle-bf16.png)


  Figure 168 MN major, no-swizzling and bf16 type[](#async-warpgroup-mn-no-swizzle-bf16 "Permalink to this image")

  the strides and related details are as follows:

  Exact layout : Swizzle<0,4,3> o ((8,1,2),(8,2)):((1,8,64),(8,128))

  Canonical Layout :Swizzle<0,4,3> o ((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))

  | Parameters | Value |
  | --- | --- |
  | T | 8 |
  | m | 2 |
  | k | 2 |
  | LBO | 128\*sizeof(bf16) |
  | SBO | 64\*sizeof(bf16) |
  | Encoding of LBO in descriptor | (LBO) >> 4 = 16 |
  | Encoding of SBO in descriptor | (SBO) >> 4 = 8 |
* MN-Major, 32B swizzling and bf16 type: [Figure 169](#async-warpgroup-mn-32b-swizzle-bf16)

  ![_images/async-warpgroup-mn-32B-swizzle-bf16.png](_images/async-warpgroup-mn-32B-swizzle-bf16.png)


  Figure 169 MN major, 32B swizzling and bf16 type[](#async-warpgroup-mn-32b-swizzle-bf16 "Permalink to this image")

  the strides and related details are as follows:

  Exact layout : Swizzle<1,4,3> o ((8,2,2),(8,2)):((1,8,128),(16,256))

  Canonical Layout :Swizzle<1,4,3> o ((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))

  | Parameters | Value |
  | --- | --- |
  | T | 8 |
  | m | 2 |
  | k | 2 |
  | LBO | 128\*sizeof(bf16) |
  | SBO | 256\*sizeof(bf16) |
  | Encoding of LBO in descriptor | (LBO) >> 4 = 16 |
  | Encoding of SBO in descriptor | (SBO) >> 4 = 32 |
* MN-Major, 64B swizzling and bf16 type: [Figure 170](#async-warpgroup-mn-64b-swizzle-bf16)

  ![_images/async-warpgroup-mn-64B-swizzle-bf16.png](_images/async-warpgroup-mn-64B-swizzle-bf16.png)


  Figure 170 MN major, 64B swizzling and bf16 type[](#async-warpgroup-mn-64b-swizzle-bf16 "Permalink to this image")

  the strides and related details are as follows:

  Exact layout : Swizzle<2,4,3> o ((8,4,2),(8,2)):((1,8,256),(32,512))

  Canonical Layout :Swizzle<2,4,3> o ((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))

  | Parameters | Value |
  | --- | --- |
  | T | 8 |
  | m | 2 |
  | k | 2 |
  | LBO | 256\*sizeof(bf16) |
  | SBO | 512\*sizeof(bf16) |
  | Encoding of LBO in descriptor | (LBO) >> 4 = 32 |
  | Encoding of SBO in descriptor | (SBO) >> 4 = 64 |

###### 9.7.15.5.1.2.2. [Matrix Descriptor Format](#asynchronous-warpgroup-level-matrix-shared-memory-layout-matrix-descriptor)[](#asynchronous-warpgroup-level-matrix-shared-memory-layout-matrix-descriptor "Permalink to this headline")

Matrix descriptor specifies the properties of the matrix in shared memory that is a multiplicand in
the matrix multiply and accumulate operation. It is a 64-bit value contained in a register with the
following layout:

| Bit-field | Size in bits | Description |
| --- | --- | --- |
| 13–0 | 14 | matrix-descriptor-encode(Matrix start address) |
| 29–16 | 14 | matrix-descriptor-encode ([Leading dimension byte offset](#asynchronous-warpgroup-level-leading-dimension-byte-offset)) |
| 45–32 | 14 | matrix-descriptor-encode ([Stride dimension byte offset](#asynchronous-warpgroup-level-stride-dimension-byte-offset)) |
| 51–49 | 3 | Matrix base offset. This is valid for all swizzling modes except the no-swizzle mode. |
| 63–62 | 2 | Specifies the swizzling mode to be used:   * 0: No swizzle * 1: 128-Byte swizzle * 2: 64-Byte swizzle * 3: 32-Byte swizzle |

where

```
matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4
```

The value of base offset is 0 when the repeating pattern of the specified swizzling mode starts as
per the below table:

> | Swizzling mode | Starting address of the repeating pattern |
> | --- | --- |
> | 128-Byte swizzle | 1024-Byte boundary |
> | 64-Byte swizzle | 512-Byte boundary |
> | 32-Byte swizzle | 256-Byte boundary |

Otherwise, the base offset must be a non-zero value, computed using the following formula:

```
base offset = (pattern start addr >> 0x7) & 0x7
```

##### 9.7.15.5.2. [Asynchronous Multiply-and-Accumulate Instruction: `wgmma.mma_async`](#asynchronous-warpgroup-level-matrix-instructions-wgmma-mma)[](#asynchronous-warpgroup-level-matrix-instructions-wgmma-mma "Permalink to this headline")

`wgmma.mma_async`

Perform matrix multiply-and-accumulate operation across warpgroup

Syntax

Half precision floating point type:

```
wgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;



wgmma.mma_async.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;



.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,

            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,

            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,

            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,

            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,

            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,

            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,

            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};

.dtype   = {.f16, .f32};
```

Alternate floating point type :

```
.bf16 floating point type:



wgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;



wgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;



.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,

            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,

            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,

            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,

            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,

            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,

            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,

            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};

.dtype  = {.f32};



.tf32 floating point type:



wgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b;



wgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b;



.shape   = {.m64n8k8, .m64n16k8, .m64n24k8, .m64n32k8,

            .m64n40k8, .m64n48k8, .m64n56k8, .m64n64k8,

            .m64n72k8, .m64n80k8, .m64n88k8, .m64n96k8,

            .m64n104k8, .m64n112k8, .m64n120k8, .m64n128k8,

            .m64n136k8, .m64n144k8, .m64n152k8, .m64n160k8,

            .m64n168k8, .m64n176k8, .m64n184k8, .m64n192k8,

            .m64n200k8, .m64n208k8, .m64n216k8, .m64n224k8,

            .m64n232k8, .m64n240k8, .m64n248k8, .m64n256k8};

.dtype  = {.f32};



FP8 floating point type



wgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, scale-d, imm-scale-a, imm-scale-b;



wgmma.mma_async.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, scale-d, imm-scale-a, imm-scale-b;



.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,

            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,

            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,

            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,

            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,

            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,

            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,

            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};

.atype  = {.e4m3, .e5m2};

.btype  = {.e4m3, .e5m2};

.dtype  = {.f16, .f32};
```

Integer type:

```
wgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, scale-d;



wgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, scale-d;



.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,

            .m64n48k32, .m64n64k32, .m64n80k32, .m64n96k32,

            .m64n112k32, .m64n128k32, .m64n144k32, .m64n160k32,

            .m64n176k32, .m64n192k32, .m64n208k32, .m64n224k32};

.atype  = {.s8, .u8};

.btype  = {.s8, .u8};
```

Single bit:

```
wgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a-desc, b-desc, scale-d;



wgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc  d, a, b-desc, scale-d;



.shape   = {.m64n8k256, .m64n16k256, .m64n24k256, .m64n32k256,

            .m64n48k256, .m64n64k256, .m64n80k256, .m64n96k256,

            .m64n112k256, .m64n128k256, .m64n144k256, .m64n160k256,

            .m64n176k256, .m64n192k256, .m64n208k256, .m64n224k256,

            .m64n240k256, .m64n256k256};

.op  = {.and};
```

Description

Instruction `wgmma.mma_async` issues a `MxNxK` matrix multiply and accumulate operation, `D =
A*B+D`, where the A matrix is `MxK`, the B matrix is `KxN`, and the D matrix is `MxN`.

The operation of the form `D = A*B` is issued when the input predicate argument `scale-d` is
false.

`wgmma.fence` instruction must be used to fence the register accesses of `wgmma.mma_async`
instruction from their prior accesses. Otherwise, the behavior is undefined.

`wgmma.commit_group` and `wgmma.wait_group` operations must be used to wait for the completion
of the asynchronous matrix multiply and accumulate operations before the results are accessed.

Register operand `d` represents the accumulator matrix as well as the destination matrix,
distributed across the participating threads. Register operand `a` represents the multiplicand
matrix A in register distributed across the participating threads. The 64-bit register operands
`a-desc` and `b-desc` are the matrix descriptors which represent the multiplicand matrices A and
B in shared memory respectively. The contents of a matrix descriptor must be same across all the warps
in the warpgroup. The format of the matrix descriptor is described in
[Matrix Descriptor Format](#asynchronous-warpgroup-level-matrix-shared-memory-layout-matrix-descriptor).

Matrices A and B are stored in row-major and column-major format respectively. For certain floating
point variants, the input matrices A and B can be transposed by specifying the value 1 for the
immediate integer arguments `imm-trans-a` and `imm-trans-b` respectively. A value of 0 can be
used to avoid the transpose operation. The valid values of `imm-trans-a` and `imm-trans-b` are 0
and 1. The transpose operation is only supported for the `wgmma.mma_async` variants with `.f16`/
`.bf16` types on matrices accessed from shared memory using matrix descriptors.

For the floating point variants of the `wgmma.mma_async` operation, each element of the input
matrices A and B can be negated by specifying the value -1 for operands `imm-scale-a` and
`imm-scale-b` respectively. A value of 1 can be used to avoid the negate operation. The valid
values of `imm-scale-a` and `imm-scale-b` are -1 and 1.

The qualifiers `.dtype`, `.atype` and `.btype` indicate the data type of the elements in
matrices D, A and B respectively. `.atype` and `.btype` must be the same for all floating point
`wgmma.mma_async` variants except for the FP8 floating point variants. The sizes of individual
data elements of matrices A and B in alternate floating point variants of the `wgmma.mma_async`
operation are as follows:

* Matrices A and B have 8-bit data elements when `.atype`/ `.btype` is `.e4m3`/`.e5m2`.
* Matrices A and B have 16-bit data elements when `.atype`/ `.btype` is `.bf16`.
* Matrices A and B have 32-bit data elements when `.atype`/ `.btype` is `.tf32`.

Precision and rounding:

* Floating point operations:

  Element-wise multiplication of matrix A and B is performed with at least single precision. When
  `.dtype` is `.f32`, accumulation of the intermediate values is performed with at least single
  precision. When `.dtype` is `.f16`, the accumulation is performed with at least half
  precision.

  The accumulation order, rounding and handling of subnormal inputs are unspecified.
* `.bf16` and `.tf32` floating point operations:

  Element-wise multiplication of matrix A and B is performed with specified
  precision. `wgmma.mma_async` operation involving type `.tf32` will truncate lower 13 bits of
  the 32-bit input data before multiplication is issued. Accumulation of the intermediate values is
  performed with at least single precision.

  The accumulation order, rounding, and handling of subnormal inputs are unspecified.
* Integer operations:

  The integer `wgmma.mma_async` operation is performed with `.s32` accumulators. The
  `.satfinite` qualifier indicates that on overflow, the accumulated value is limited to the
  range *MIN\_INT32*.. *MAX\_INT32* (where the bounds are defined as the minimum negative signed
  32-bit integer and the maximum positive signed 32-bit integer respectively).

  If `.satfinite` is not specified, the accumulated value is wrapped instead.

The mandatory `.sync` qualifier indicates that `wgmma.mma_async` instruction causes the
executing thread to wait until all threads in the warp execute the same `wgmma.mma_async`
instruction before resuming execution.

The mandatory `.aligned` qualifier indicates that all threads in the warpgroup must execute the
same `wgmma.mma_async` instruction. In conditionally executed code, a `wgmma.mma_async`
instruction should only be used if it is known that all threads in the warpgroup evaluate the
condition identically, otherwise behavior is undefined.

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Support for `.u8.s8` and `.s8.u8` as .atype.btype introduced in PTX ISA version 8.4.

Target ISA Notes

Requires `sm_90a`.

Examples of half precision floating point type

```
.reg .f16x2 f16a<40>, f16d<40>;

.reg .f32   f32d<40>;

.reg .b64   descA, descB;

.reg .pred  scaleD;

wgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16

  {f32d0, f32d1, f32d2, f32d3},

  {f16a0, f16a1, f16a2, f16a3},

  descB,

  1, -1, -1, 1;



wgmma.mma_async.sync.aligned.m64n72k16.f16.f16.f16

  {f16d0, f16d1,  f16d2,  f16d3,  f16d4,  f16d5,  f16d6,  f16d7,  f16d8,

   f16d9, f16d10, f16d11, f16d12, f16d13, f16d14, f16d15, f16d16, f16d17},

  descA,

  descB,

  scaleD, -1, 1, 1, 0;
```

Examples of alternate floating point type

```
.reg .f32   f32d<40>;

.reg .b32   bf16a<40>

.reg .b64   descA, descB;



wgmma.mma_async.sync.aligned.m64n120k16.f32.bf16.bf16

  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7, f32d8, f32d9,

   f32d10, f32d11, f32d12, f32d13, f32d14, f32d15, f32d16, f32d17, f32d18, f32d19,

   f32d20, f32d21, f32d22, f32d23, f32d24, f32d25, f32d26, f32d27, f32d28, f32d29,

   f32d30, f32d31, f32d32, f32d33, f32d34, f32d35, f32d36, f32d37, f32d38, f32d39,

   f32d40, f32d41, f32d42, f32d43, f32d44, f32d45, f32d46, f32d47, f32d48, f32d49,

   f32d50, f32d51, f32d52, f32d53, f32d54, f32d55, f32d56, f32d57, f32d58, f32d59},

  {bf16a0, bf16a1, bf16a2, bf16a3},

  descB,

  scaleD, -1, -1, 0;



.reg .f32   f32d<40>;

.reg .b64   descA, descB;



wgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32

  {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7},

  descA,

  descB,

  0, -1, -1;



.reg .b32 f16d<8>, f16a<8>;

.reg .f32 f32d<8>;

.reg .b64   descA, descB;



wgmma.mma_async.sync.aligned.m64n8k32.f16.e4m3.e5m2

  {f16d0, f16d1},

  descA,

  descB,

  scaleD, -1, 1;



wgmma.mma_async.sync.aligned.m64n8k32.f32.e5m2.e4m3

  {f32d0, f32d1, f32d2, f32d3},

  {f16a0, f16a1, f16a2, f16a3},

  descB,

  1, -1, -1;
```

Examples of integer type

```
.reg .s32 s32d<8>, s32a<8>;

.reg .u32 u32a<8>;

.reg .pred scaleD;

.reg .b64   descA, descB;



wgmma.mma_async.sync.aligned.m64n8k32.s32.s8.s8.satfinite

  {s32d0, s32d1, s32d2, s32d3},

  {s32a0, s32a1, s32a2, s32a3},

  descB,

  1;



wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8

  {s32d0, s32d1, s32d2, s32d3},

  descA,

  descB,

  scaleD;



wgmma.mma_async.sync.aligned.m64n8k32.s32.s8.u8.satfinite

  {s32d0, s32d1, s32d2, s32d3},

  {s32a0, s32a1, s32a2, s32a3},

  descB,

  scaleD;



wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.s8

  {s32d0, s32d1, s32d2, s32d3},

  descA,

  descB,

  scaleD;
```

Examples of single bit type

```
.reg .s32 s32d<4>;

.reg .b32 b32a<4>;

.reg .pred scaleD;

.reg .b64   descA, descB;





wgmma.mma_async.sync.aligned.m64n8k256.s32.b1.b1.and.popc

  {s32d0, s32d1, s32d2, s32d3},

  {b32a0, b32a1, b32a2, b32a3},

  descB,

  scaleD;
```

#### 9.7.15.6. [Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using `wgmma.mma_async.sp` instruction](#asynchronous-warpgroup-level-matrix-instructions-for-sparse-wgmma)[](#asynchronous-warpgroup-level-matrix-instructions-for-sparse-wgmma "Permalink to this headline")

This section describes warp-level `wgmma.mma_async.sp` instruction with sparse matrix A. This
variant of the `wgmma.mma_async` operation can be used when A is a structured sparse matrix with
50% zeros in each row distributed in a shape-specific granularity. For an `MxNxK` sparse
`wgmma.mma_async.sp` operation, the `MxK` matrix A is packed into `MxK/2` elements. For each
K-wide row of matrix A, 50% elements are zeros and the remaining `K/2` non-zero elements are
packed in the operand representing matrix A. The mapping of these `K/2` elements to the
corresponding K-wide row is provided explicitly as metadata.

##### 9.7.15.6.1. [Sparse matrix storage](#asynchronous-warpgroup-level-sparse-matrix-storage)[](#asynchronous-warpgroup-level-sparse-matrix-storage "Permalink to this headline")

Granularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a
sub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the
sub-chunk is shape-specific. For example, in a `64x32` matrix A used in floating point
`wgmma.mma_async` operations, sparsity is expected to be at 2:4 granularity, i.e. each 4-element
vector (i.e. a sub-chunk of 4 consecutive elements) of a matrix row contains 2 zeros. Index of each
non-zero element in a sub-chunk is stored in the metadata operand. Values `0b0000`, `0b0101`,
`0b1010`, `0b1111` are invalid values for metadata and will result in undefined behavior. In a
group of four consecutive threads, one or more threads store the metadata for the whole group
depending upon the matrix shape. These threads are specified using an additional sparsity selector operand.

Matrix A and its corresponding input operand to the sparse wgmma is similar to the diagram shown in
[Figure 111](#sparse-mma-storage-example), with an appropriate matrix size.

Granularities for different matrix shapes and data types are described below.

Sparse `wgmma.mma_async.sp` with half-precision and `.bf16` type

For `.f16` and `.bf16` types, for all supported `64xNx32` shapes, matrix A is structured
sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of
matrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in
matrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices
in the metadata operand.

![_images/f16-metadata-example.png](_images/f16-metadata-example.png)


Figure 171 Sparse WGMMA metadata example for `.f16`/`.bf16` type.[](#f16-metadata-example-wgmma "Permalink to this image")

The sparsity selector indicates a thread-pair within a group of four consecutive threads which
contributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or
1 (threads T2, T3); any other value results in an undefined behavior.

Sparse `wgmma.mma_async.sp` with `.tf32` type

For `.tf32` type, for all supported `64xNx16` shapes, matrix A is structured sparse at a
granularity of 1:2. In other words, each chunk of two adjacent elements in a row of matrix A have
one zero and one non-zero element. Only the non-zero element is stored in operand for matrix A and
the 4-bit index in the metadata indicates the position of the non-zero element in the two-wide
chunk. 0b1110 and 0b0100 are the only meaningful values of the index, the remaining values result in
an undefined behavior.

![_images/tf32-metadata-example.png](_images/tf32-metadata-example.png)


Figure 172 Sparse WGMMA metadata example for `.tf32` type.[](#tf32-metadata-example-wgmma "Permalink to this image")

The sparsity selector indicates a thread-pair within a group of four consecutive threads which
contributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or
1 (threads T2, T3); any other value results in an undefined behavior.

Sparse `wgmma.mma_async.sp` with `.e4m3` and `.e5m2` floating point type

For `.e4m3` and `.e5m2` types, for all supported `64xNx64` shapes, matrix A is structured
sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of
matrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in
matrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices
in the metadata operand.

![_images/u8s8-metadata-example.png](_images/u8s8-metadata-example.png)


Figure 173 Sparse WGMMA metadata example for `.e4m3`/`.e5m2` type.[](#e4m3-e5m2-metadata-example-wgmma "Permalink to this image")

All threads contribute the sparsity metadata and the sparsity selector must be 0; any other value
results in an undefined behavior.

Sparse `wgmma.mma_async.sp` with integer type

For the integer type, for all supported `64xNx64` shapes, matrix A is structured sparse at a
granularity of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have
two zeroes and two non-zero elements. Only the two non-zero elements are stored in matrix A and two
2-bit indices in the metadata indicate the position of these two non-zero elements in the four-wide
chunk.

![_images/u8s8-metadata-example.png](_images/u8s8-metadata-example.png)


Figure 174 Sparse WGMMA metadata example for `.u8`/`.s8` type.[](#u8s8-metadata-example-wgmma "Permalink to this image")

All threads contribute the sparsity metadata and the sparsity selector must be 0; any other value
results in an undefined behavior.

##### 9.7.15.6.2. [Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A](#asynchronous-warpgroup-level-matrix-fragments-for-sparse-wgmma)[](#asynchronous-warpgroup-level-matrix-fragments-for-sparse-wgmma "Permalink to this headline")

In this section we describe how the contents of thread registers are associated with fragments of A
matrix and the sparsity metadata.

Each warp in the warpgroup provides sparsity information for 16 rows of matrix A. The following
table shows the assignment of warps to rows of matrix A:

| Warp | Sparsity information for rows of matrix A |
| --- | --- |
| `%warpid` % 4 = 3 | 48-63 |
| `%warpid` % 4 = 2 | 32-47 |
| `%warpid` % 4 = 1 | 16-31 |
| `%warpid` % 4 = 0 | 0-15 |

The following conventions are used throughout this section:

* For matrix A, only the layout of a fragment is described in terms of register vector sizes and
  their association with the matrix data.
* For matrix D, since the matrix dimension - data type combination is the same for all supported
  shapes, and is already covered in
  [Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma\_async instruction](#asynchronous-warpgroup-level-matrix-operation-wgmma-mma-async), the pictorial
  representations of matrix fragments are not included in this section.
* For the metadata operand, pictorial representations of the association between indices of the
  elements of matrix A and the contents of the metadata operand are included. `Tk: [m..n]` present
  in cell `[x][y..z]` indicates that bits `m` through `n` (with `m` being higher) in the
  metadata operand of thread with `%laneid=k` contains the indices of the non-zero elements from
  the chunk `[x][y]..[x][z]` of matrix A.

###### 9.7.15.6.2.1. [Matrix Fragments for sparse `wgmma.mma_async.m64nNk32`](#asynchronous-warpgroup-level-matrix-fragment-sparse-wgmma-64n32)[](#asynchronous-warpgroup-level-matrix-fragment-sparse-wgmma-64n32 "Permalink to this headline")

A warpgroup executing sparse `wgmma.mma_async.m64nNk32` will compute an MMA operation of shape
`.m64nNk32` where N is a valid n dimension as listed in
[Matrix Shape](#asynchronous-warpgroup-level-matrix-shape).

Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.

* Multiplicand A, from shared memory is documented in
  [Shared Memory Matrix Layout](#asynchronous-warpgroup-level-matrix-shared-memory-layout).
* Multiplicand A, from registers:

  > | .atype | Fragments | Elements |
  > | --- | --- | --- |
  > | `.f16` /  `.bf16` | A vector expression containing four `.b32`  registers, with each register containing two  non-zero `.f16` /`.bf16` elements out of 4  consecutive elements from matrix A. | Non-zero elements:  a0, a1, a2, a3, a4, a5, a6, a7  Mapping of the non-zero  elements is as described in  [Sparse matrix storage](#asynchronous-warpgroup-level-sparse-matrix-storage) |
  >
  > The layout of the fragments held by different threads is shown in [Figure 175](#sparse-wgmma-64n32-f16-bf16-a).
  >
  > ![_images/sparse-wgmma-64N32-f16-bf16-A.png](_images/sparse-wgmma-64N32-f16-bf16-A.png)
  >
  >
  > Figure 175 Sparse WGMMA .m64nNk32 fragment layout for matrix A with `.f16`/`.bf16` type.[](#sparse-wgmma-64n32-f16-bf16-a "Permalink to this image")
* Accumulator D:

  Matrix fragments for accumulator D are the same as in case of
  [Matrix Fragments for wgmma.mma\_async.m64nNk32](#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n32)
  for the same `.dtype` format.
* Multiplicand B:

  Shared memory layout for Matrix B is documented in
  [Shared Memory Matrix Layout](#asynchronous-warpgroup-level-matrix-shared-memory-layout).
* Metadata operand is a `.b32` register containing 16 2-bit vectors each storing the index of a
  non-zero element of a 4-wide chunk of matrix A.

  [Figure 176](#sparse-wgmma-metadata-64n32-f16bf16) shows the mapping of the metadata bits to the elements
  of matrix A for a warp. In this figure, variable `i` represents the value of the sparsity
  selector operand.

  > ![_images/sparse-mma-metadata-16832-f16bf16.png](_images/sparse-mma-metadata-16832-f16bf16.png)
  >
  >
  > Figure 176 Sparse WGMMA .m64nNk32 metadata layout for `.f16`/`.bf16` type.[](#sparse-wgmma-metadata-64n32-f16bf16 "Permalink to this image")

###### 9.7.15.6.2.2. [Matrix Fragments for sparse `wgmma.mma_async.m64nNk16`](#asynchronous-warpgroup-level-matrix-fragment-sparse-wgmma-64n16)[](#asynchronous-warpgroup-level-matrix-fragment-sparse-wgmma-64n16 "Permalink to this headline")

A warpgroup executing sparse `wgmma.mma_async.m64nNk16` will compute an MMA operation of shape
`.m64nNk16` where N is a valid n dimension as listed in
[Matrix Shape](#asynchronous-warpgroup-level-matrix-shape).

Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.

* Multiplicand A, from shared memory is documented in
  [Shared Memory Matrix Layout](#asynchronous-warpgroup-level-matrix-shared-memory-layout).
* Multiplicand A, from registers:

  > | .atype | Fragments | Elements |
  > | --- | --- | --- |
  > | `.tf32` | A vector expression containing four `.b32`  registers, containing four non-zero `.tf32`  elements out of eight consecutive elements  from matrix A. | Non-zero elements:  a0, a1, a2, a3    Mapping of the non-zero  elements is as described in  [Sparse matrix storage](#asynchronous-warpgroup-level-sparse-matrix-storage) |
  >
  > The layout of the fragments held by different threads is shown in [Figure 177](#sparse-wgmma-64n16-tf32-a).
  >
  > ![_images/sparse-wgmma-64N16-tf32-A.png](_images/sparse-wgmma-64N16-tf32-A.png)
  >
  >
  > Figure 177 Sparse WGMMA .m64nNk16 fragment layout for matrix A with `.tf32` type.[](#sparse-wgmma-64n16-tf32-a "Permalink to this image")
* Accumulator D:

  Matrix fragments for accumulator D are the same as in case of
  [Matrix Fragments for wgmma.mma\_async.m64nNk8](#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n8)
  for the same `.dtype` format.
* Multiplicand B:

  Shared memory layout for Matrix B is documented in
  [Shared Memory Matrix Layout](#asynchronous-warpgroup-level-matrix-shared-memory-layout).
* Metadata operand is a `.b32` register containing eight 4-bit vectors each storing the index of a
  non-zero element of a 2-wide chunk of matrix A.

  [Figure 178](#sparse-wgmma-metadata-64n16-tf32) shows the mapping of the metadata bits to the elements
  of matrix A for a warp. In this figure, variable `i` represents the value of the sparsity
  selector operand.

  > ![_images/sparse-mma-metadata-16816-tf32.png](_images/sparse-mma-metadata-16816-tf32.png)
  >
  >
  > Figure 178 Sparse WGMMA .m64nNk16 metadata layout for `.tf32` type.[](#sparse-wgmma-metadata-64n16-tf32 "Permalink to this image")

###### 9.7.15.6.2.3. [Matrix Fragments for sparse `wgmma.mma_async.m64nNk64`](#asynchronous-warpgroup-level-matrix-fragment-sparse-wgmma-64n64)[](#asynchronous-warpgroup-level-matrix-fragment-sparse-wgmma-64n64 "Permalink to this headline")

A warpgroup executing sparse `wgmma.mma_async.m64nNk64` will compute an MMA operation of shape
`.m64nNk64` where N is a valid n dimension as listed in
[Matrix Shape](#asynchronous-warpgroup-level-matrix-shape).

Elements of the matrix are distributed across the threads in a warpgroup so each thread of the
warpgroup holds a fragment of the matrix.

* Multiplicand A, from shared memory is documented in
  [Matrix Fragments for sparse wgmma.mma\_async.m64nNk64](#asynchronous-warpgroup-level-matrix-fragment-sparse-wgmma-64n64).
* Multiplicand A, from registers:

  > | .atype | Fragments | Elements |
  > | --- | --- | --- |
  > | `.e4m3` /  `.e5m2` | A vector expression containing four `.b32`  registers, with each register containing four  non-zero `.e4m3` /`.e5m2` elements out of  eight consecutive elements from matrix A. | Non-zero elements:  a0, a1, a2, … , a15    Mapping of the non-zero  elements is as described in  [Sparse matrix storage](#asynchronous-warpgroup-level-sparse-matrix-storage) |
  > | `.s8` /  `.u8` | A vector expression containing four `.b32`  registers, with each register containing four  non-zero `.s8` /`.u8` elements out of  eight consecutive elements from matrix A. |
  >
  > The layout of the fragments held by different threads is shown in [Figure 179](#sparse-wgmma-64n64-e4m3-e5m2-s8-u8-a).
  >
  > ![_images/sparse-wgmma-64N64-e4m3-e5m2-s8-u8-A.png](_images/sparse-wgmma-64N64-e4m3-e5m2-s8-u8-A.png)
  >
  >
  > Figure 179 Sparse WGMMA .m64nNk64 fragment layout for matrix A with `.e4m3`/ `.e5m2`/ `.s8`/ `.u8` type.[](#sparse-wgmma-64n64-e4m3-e5m2-s8-u8-a "Permalink to this image")
* Accumulator D:

  Matrix fragments for accumulator D are the same as in case of
  [Matrix Fragments for wgmma.mma\_async.m64nNk32](#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n32)
  for the same `.dtype` format.
* Multiplicand B:

  Shared memory layout for Matrix B is documented in
  [Matrix Fragments for sparse wgmma.mma\_async.m64nNk64](#asynchronous-warpgroup-level-matrix-fragment-sparse-wgmma-64n64).
* Metadata operand is a `.b32` register containing 16 4-bit vectors each storing the indices of
  two non-zero elements of a 4-wide chunk of matrix A.

  [Figure 180](#sparse-wgmma-metadata-64n64-e4m3-e5m2-s8-u8-first32col) shows the mapping of the metadata
  bits to the elements of columns 0–31 of matrix A.

  > ![_images/sparse-mma-metadata-16864-u8s8-first32col.png](_images/sparse-mma-metadata-16864-u8s8-first32col.png)
  >
  >
  > Figure 180 Sparse WGMMA .m64nNk64 metadata layout for `.e4m3`/ `.e5m2`/ `.s8`/ `.u8` type for columns 0–31[](#sparse-wgmma-metadata-64n64-e4m3-e5m2-s8-u8-first32col "Permalink to this image")

  [Figure 181](#sparse-wgmma-metadata-64n64-e4m3-e5m2-s8-u8-last32col) shows the mapping of the metadata
  bits to the elements of columns 32–63 of matrix A.

  > ![_images/sparse-mma-metadata-16864-u8s8-last32col.png](_images/sparse-mma-metadata-16864-u8s8-last32col.png)
  >
  >
  > Figure 181 Sparse WGMMA .m64nNk64 metadata layout for `.e4m3`/ `.e5m2`/ `.s8`/ `.u8` type for columns 32–63[](#sparse-wgmma-metadata-64n64-e4m3-e5m2-s8-u8-last32col "Permalink to this image")

##### 9.7.15.6.3. [Asynchronous Multiply-and-Accumulate Instruction: `wgmma.mma_async.sp`](#asynchronous-warpgroup-level-matrix-instructions-wgmma-mma-sp)[](#asynchronous-warpgroup-level-matrix-instructions-wgmma-mma-sp "Permalink to this headline")

`wgmma.mma_async.sp`

Perform matrix multiply-and-accumulate operation with sparse matrix A across warpgroup

Syntax

Half precision floating point type:

```
wgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;



wgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;



.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,

            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,

            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,

            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,

            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,

            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,

            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,

            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};

.dtype   = {.f16, .f32};
```

Alternate floating point type :

```
.bf16 floating point type:



wgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b;



wgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b;



.shape   = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32,

            .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32,

            .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32,

            .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32,

            .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32,

            .m64n168k32, .m64n176k32, .m64n184k32, .m64n192k32,

            .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32,

            .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32};

.dtype  = {.f32};



.tf32 floating point type:



wgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;



wgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;



.shape   = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16,

            .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16,

            .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16,

            .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16,

            .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16,

            .m64n168k16, .m64n176k16, .m64n184k16, .m64n192k16,

            .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16,

            .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16};

.dtype  = {.f32};



FP8 floating point type



wgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;



wgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b;



.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,

            .m64n40k64, .m64n48k64, .m64n56k64, .m64n64k64,

            .m64n72k64, .m64n80k64, .m64n88k64, .m64n96k64,

            .m64n104k64, .m64n112k64, .m64n120k64, .m64n128k64,

            .m64n136k64, .m64n144k64, .m64n152k64, .m64n160k64,

            .m64n168k64, .m64n176k64, .m64n184k64, .m64n192k64,

            .m64n200k64, .m64n208k64, .m64n216k64, .m64n224k64,

            .m64n232k64, .m64n240k64, .m64n248k64, .m64n256k64};

.atype  = {.e4m3, .e5m2};

.btype  = {.e4m3, .e5m2};

.dtype  = {.f16, .f32};
```

Integer type:

```
wgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a-desc, b-desc, sp-meta, sp-sel, scale-d;



wgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype  d, a, b-desc, sp-meta, sp-sel, scale-d;



.shape   = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64,

            .m64n48k64, .m64n64k64, .m64n80k64, .m64n96k64,

            .m64n112k64, .m64n128k64, .m64n144k64, .m64n160k64,

            .m64n176k64, .m64n192k64, .m64n208k64, .m64n224k64,

            .m64n240k64, .m64n256k64};

.atype  = {.s8, .u8};

.btype  = {.s8, .u8};
```

Description

Instruction `wgmma.mma_async` issues a `MxNxK` matrix multiply and accumulate operation, `D =
A*B+D`, where the A matrix is `MxK`, the B matrix is `KxN`, and the D matrix is `MxN`.

The matrix A is stored in the packed format Mx(K/2) as described in
[Sparse matrix storage](#asynchronous-warpgroup-level-sparse-matrix-storage).

The operation of the form `D = A*B` is issued when the input predicate argument `scale-d` is
false.

`wgmma.fence` instruction must be used to fence the register accesses of `wgmma.mma_async`
instruction from their prior accesses. Otherwise, the behavior is undefined.

`wgmma.commit_group` and `wgmma.wait_group` operations must be used to wait for the completion
of the asynchronous matrix multiply and accumulate operations before the results are accessed.

Register operand `d` represents the accumulator matrix as well as the destination matrix,
distributed across the participating threads. Register operand `a` represents the multiplicand
matrix A in register distributed across the participating threads. The 64-bit register operands
`a-desc` and `b-desc` are the matrix descriptors which represent the multiplicand matrices A and
B in shared memory respectively. The contents of a matrix descriptor must be same across all the
warps in the warpgroup. The format of the matrix descriptor is described in
[Matrix Descriptor Format](#asynchronous-warpgroup-level-matrix-shared-memory-layout-matrix-descriptor). Matrix A is
structured sparse as described in [Sparse matrix storage](#asynchronous-warpgroup-level-sparse-matrix-storage). Operands `sp-meta` and `sp-sel`
represent sparsity metadata and sparsity selector respectively. Operand `sp-meta` is a 32-bit
integer and operand `sp-sel` is a 32-bit integer constant with values in the range 0..3.

The valid values of `sp-meta` and `sp-sel` for each shape is specified in
[Sparse matrix storage](#asynchronous-warpgroup-level-sparse-matrix-storage) and are summarized here :

| Matrix shape | `.atype` | Valid values of `sp-meta` | Valid values of `sp-sel` |
| --- | --- | --- | --- |
| `.m64nNk16` | `.tf32` | 0b1110 , 0b0100 | 0 (threads T0, T1) or 1 (threads T2, T3) |
| `.m64nNk32` | `.f16`/ `.bf16` | 0b00, 0b01, 0b10, 0b11 | 0 (threads T0, T1) or 1 (threads T2, T3) |
| `.m64nNk64` | `.e4m3` / `.e5m2` / `.s8` / `.u8` | 0b00, 0b01, 0b10, 0b11 | 0 (all threads contribute) |

Matrices A and B are stored in row-major and column-major format respectively. For certain floating
point variants, the input matrices A and B can be transposed by specifying the value 1 for the
immediate integer arguments `imm-trans-a` and `imm-trans-b` respectively. A value of 0 can be
used to avoid the transpose operation. The valid values of `imm-trans-a` and `imm-trans-b` are 0
and 1. The transpose operation is only supported for the `wgmma.mma_async` variants with `.f16`/
`.bf16` types on matrices accessed from shared memory using matrix descriptors.

For the floating point variants of the `wgmma.mma_async` operation, each element of the input
matrices A and B can be negated by specifying the value -1 for operands `imm-scale-a` and
`imm-scale-b` respectively. A value of 1 can be used to avoid the negate operation. The valid
values of `imm-scale-a` and `imm-scale-b` are -1 and 1.

The qualifiers `.dtype`, `.atype` and `.btype` indicate the data type of the elements in
matrices D, A and B respectively. `.atype` and `.btype` must be the same for all floating point
`wgmma.mma_async` variants except for the FP8 floating point variants. The sizes of individual
data elements of matrices A and B in alternate floating point variants of the `wgmma.mma_async`
operation are as follows:

* Matrices A and B have 8-bit data elements when `.atype`/ `.btype` is `.e4m3`/`.e5m2`.
* Matrices A and B have 16-bit data elements when `.atype`/ `.btype` is `.bf16`.
* Matrices A and B have 32-bit data elements when `.atype`/ `.btype` is `.tf32`.

Precision and rounding:

* Floating point operations:

  Element-wise multiplication of matrix A and B is performed with at least single precision. When
  `.dtype` is `.f32`, accumulation of the intermediate values is performed with at least single
  precision. When `.dtype` is `.f16`, the accumulation is performed with at least half
  precision.

  The accumulation order, rounding and handling of subnormal inputs are unspecified.
* `.bf16` and `.tf32` floating point operations:

  Element-wise multiplication of matrix A and B is performed with specified
  precision. `wgmma.mma_async` operation involving type `.tf32` will truncate lower 13 bits of
  the 32-bit input data before multiplication is issued. Accumulation of the intermediate values is
  performed with at least single precision.

  The accumulation order, rounding, and handling of subnormal inputs are unspecified.
* Integer operations:

  The integer `wgmma.mma_async` operation is performed with `.s32` accumulators. The
  `.satfinite` qualifier indicates that on overflow, the accumulated value is limited to the
  range *MIN\_INT32*.. *MAX\_INT32* (where the bounds are defined as the minimum negative signed
  32-bit integer and the maximum positive signed 32-bit integer respectively).

  If `.satfinite` is not specified, the accumulated value is wrapped instead.

The mandatory `.sync` qualifier indicates that `wgmma.mma_async` instruction causes the
executing thread to wait until all threads in the warp execute the same `wgmma.mma_async`
instruction before resuming execution.

The mandatory `.aligned` qualifier indicates that all threads in the warpgroup must execute the
same `wgmma.mma_async` instruction. In conditionally executed code, a `wgmma.mma_async`
instruction should only be used if it is known that all threads in the warpgroup evaluate the
condition identically, otherwise behavior is undefined.

PTX ISA Notes

Introduced in PTX ISA version 8.2.

Support for `.u8.s8` and `.s8.u8` as .atype.btype introduced in PTX ISA version 8.4.

Target ISA Notes

Requires `sm_90a`.

Examples of integer type

```
wgmma.fence.sync.aligned;

wgmma.mma_async.sp.sync.aligned.m64n8k64.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},

                                                    descA, descB, spMeta, 0, scaleD;

wgmma.mma_async.sp.sync.aligned.m64n8k64.s32.s8.u8  {s32d0, s32d1, s32d2, s32d3},

                                                    descA, descB, spMeta, 0, scaleD;

wgmma.commit_group.sync.aligned;

wgmma.wait_group.sync.aligned 0;
```

#### 9.7.15.7. [Asynchronous `wgmma` Proxy Operations](#asynchronous-wgmma-proxy-operations)[](#asynchronous-wgmma-proxy-operations "Permalink to this headline")

This section describes warpgroup level `wgmma.fence`, `wgmma.commit_group` and `wgmma.wait_group` instructions.

##### 9.7.15.7.1. [Asynchronous Multiply-and-Accumulate Instruction: `wgmma.fence`](#asynchronous-warpgroup-level-matrix-instructions-wgmma-fence)[](#asynchronous-warpgroup-level-matrix-instructions-wgmma-fence "Permalink to this headline")

`wgmma.fence`

Enforce an ordering of register accesses between `wgmma.mma_async` and other operations.

Syntax

```
wgmma.fence.sync.aligned;
```

Description

`wgmma.fence` instruction establishes an ordering between prior accesses to any warpgroup
registers and subsequent accesses to the same registers by a `wgmma.mma_async` instruction. Only
the accumulator register and the input registers containing the fragments of matrix A require this
ordering.

The `wgmma.fence` instruction must be issued by all warps of the warpgroup at the following
locations:

* Before the first `wgmma.mma_async` operation in a warpgroup.
* Between a register access by a thread in the warpgroup and any `wgmma.mma_async` instruction
  that accesses the same registers, either as accumulator or input register containing fragments of
  matrix A, except when these are accumulator register accesses across multiple `wgmma.mma_async`
  instructions of the same shape. In the latter case, an ordering guarantee is provided by default.

Otherwise, the behavior is undefined.

An async proxy fence must be used to establish an ordering between prior writes to shared memory
matrices and subsequent reads of the same matrices in a `wgmma.mma_async` instruction.

The mandatory `.sync` qualifier indicates that `wgmma.fence` instruction causes the executing
thread to wait until all threads in the warp execute the same `wgmma.fence` instruction before
resuming execution.

The mandatory `.aligned` qualifier indicates that all threads in the warpgroup must execute the
same `wgmma.fence` instruction. In conditionally executed code, an `wgmma.fence` instruction
should only be used if it is known that all threads in the warpgroup evaluate the condition
identically, otherwise the behavior is undefined.

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Target ISA Notes

Requires `sm_90a`.

Examples

```
// Example 1, first use example:

wgmma.fence.sync.aligned;    // Establishes an ordering w.r.t. prior accesses to the registers s32d<0-3>

wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},

                                                  descA, descB, scaleD;

wgmma.commit_group.sync.aligned;

wgmma.wait_group.sync.aligned 0;



// Example 2, use-case with the input value updated in between:

wgmma.fence.sync.aligned;

wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},

                                                  descA, descB, scaleD;

...

mov.b32 s32d0, new_val;

wgmma.fence.sync.aligned;

wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d4, s32d5, s32d6, s32d7},

                                                 {s32d0, s32d1, s32d2, s32d3},

                                                  descB, scaleD;

wgmma.commit_group.sync.aligned;

wgmma.wait_group.sync.aligned 0;
```

##### 9.7.15.7.2. [Asynchronous Multiply-and-Accumulate Instruction: `wgmma.commit_group`](#asynchronous-warpgroup-level-matrix-instructions-wgmma-commit-group)[](#asynchronous-warpgroup-level-matrix-instructions-wgmma-commit-group "Permalink to this headline")

`wgmma.commit_group`

Commits all prior uncommitted `wgmma.mma_async` operations into a *wgmma-group*.

Syntax

```
wgmma.commit_group.sync.aligned;
```

Description

`wgmma.commit_group` instruction creates a new wgmma-group per warpgroup and batches all prior
`wgmma.mma_async` instructions initiated by the executing warp but not committed to any
wgmma-group into the new wgmma-group. If there are no uncommitted `wgmma.mma_async` instructions
then `wgmma.commit_group` results in an empty wgmma-group.

An executing thread can wait for the completion of all `wgmma.mma_async` operations in a
wgmma-group by using `wgmma.wait_group`.

The mandatory `.sync` qualifier indicates that `wgmma.commit_group` instruction causes the
executing thread to wait until all threads in the warp execute the same `wgmma.commit_group`
instruction before resuming execution.

The mandatory `.aligned` qualifier indicates that all threads in the warpgroup must execute the
same `wgmma.commit_group` instruction. In conditionally executed code, an `wgmma.commit_group`
instruction should only be used if it is known that all threads in the warpgroup evaluate the
condition identically, otherwise the behavior is undefined.

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Target ISA Notes

Requires `sm_90a`.

Examples

```
wgmma.commit_group.sync.aligned;
```

##### 9.7.15.7.3. [Asynchronous Multiply-and-Accumulate Instruction: `wgmma.wait_group`](#asynchronous-warpgroup-level-matrix-instructions-wgmma-wait-group)[](#asynchronous-warpgroup-level-matrix-instructions-wgmma-wait-group "Permalink to this headline")

`wgmma.wait_group`

Signal the completion of a preceding warpgroup operation.

Syntax

```
wgmma.wait_group.sync.aligned N;
```

Description

`wgmma.wait_group` instruction will cause the executing thread to wait until only N or fewer of
the most recent wgmma-groups are pending and all the prior wgmma-groups committed by the executing
threads are complete. For example, when N is 0, the executing thread waits on all the prior
wgmma-groups to complete. Operand N is an integer constant.

Accessing the accumulator register or the input register containing the fragments of matrix A of a
`wgmma.mma_async` instruction without first performing a `wgmma.wait_group` instruction that
waits on a *wgmma-group* including that `wgmma.mma_async` instruction is undefined behavior.

The mandatory `.sync` qualifier indicates that `wgmma.wait_group` instruction causes the
executing thread to wait until all threads in the warp execute the same `wgmma.wait_group`
instruction before resuming execution.

The mandatory `.aligned` qualifier indicates that all threads in the warpgroup must execute the
same `wgmma.wait_group` instruction. In conditionally executed code, an `wgmma.wait_group`
instruction should only be used if it is known that all threads in the warpgroup evaluate the
condition identically, otherwise the behavior is undefined.

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Target ISA Notes

Requires `sm_90a`.

Examples

```
wgmma.fence.sync.aligned;



wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8  {s32d0, s32d1, s32d2, s32d3},

                                                  descA, descB, scaleD;

wgmma.commit_group.sync.aligned;



wgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 {f32d0, f32d1, f32d2, f32d3},

                                                  {f16a0, f16a1, f16a2, f16a3},

                                                   descB, 1, -1, -1, 1;

wgmma.commit_group.sync.aligned;



wgmma.wait_group.sync.aligned 0;
```