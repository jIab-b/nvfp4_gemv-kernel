# Builder commands for python source
# Automatically generated - execute to recreate AST/CUDA
# Mode: structured

# === Full CUDA Source ===
from cuda_ast import *
from builder import CudaBuilder, ASTBuilder, reg, imm, vec, mem, sym

cb = CudaBuilder()
cb.include("cuda_fp16.h", system=True)
cb.include("cuda_runtime.h", system=True)
cb.include("cstdint", system=True)
cb.func_begin("process_tile_fused", TypeRef("void"), params=[Param("local_sum", TypeRef("float&")), Param("A0_0", TypeRef("const uint32_t")), Param("A0_1", TypeRef("const uint32_t")), Param("A0_2", TypeRef("const uint32_t")), Param("A0_3", TypeRef("const uint32_t")), Param("A1_0", TypeRef("const uint32_t")), Param("A1_1", TypeRef("const uint32_t")), Param("A1_2", TypeRef("const uint32_t")), Param("A1_3", TypeRef("const uint32_t")), Param("B0_0", TypeRef("const uint32_t")), Param("B0_1", TypeRef("const uint32_t")), Param("B0_2", TypeRef("const uint32_t")), Param("B0_3", TypeRef("const uint32_t")), Param("B1_0", TypeRef("const uint32_t")), Param("B1_1", TypeRef("const uint32_t")), Param("B1_2", TypeRef("const uint32_t")), Param("B1_3", TypeRef("const uint32_t")), Param("sfa_packed", TypeRef("const uint32_t")), Param("sfb_packed", TypeRef("const uint32_t"))], qualifier="__device__", forceinline=True)
    cb.asm_raw(['{ .reg .b16 %%sfalo, %%sfahi, %%sfblo, %%sfbhi;', '.reg .b32 %%sa01, %%sa23, %%sb01, %%sb23;', '.reg .b32 %%scale01, %%scale23;', '.reg .f32 %%s0, %%s1, %%s2, %%s3;', '.reg .b8 %%a<4>, %%b<4>;', '.reg .b32 %%fa<4>, %%fb<4>;', '.reg .b32 %%p0, %%p1, %%p2, %%p3;', '.reg .f16 %%h0, %%h1;', '.reg .f32 %%f0, %%f1, %%acc0, %%acc1, %%acc2, %%acc3, %%tile_result, %%one;', 'mov.f32 %%one, 0f3f800000;', 'mov.b32 {%%sfalo, %%sfahi}, %17;', 'mov.b32 {%%sfblo, %%sfbhi}, %18;', 'cvt.rn.f16x2.e4m3x2 %%sa01, %%sfalo;', 'cvt.rn.f16x2.e4m3x2 %%sa23, %%sfahi;', 'cvt.rn.f16x2.e4m3x2 %%sb01, %%sfblo;', 'cvt.rn.f16x2.e4m3x2 %%sb23, %%sfbhi;', 'mul.rn.f16x2 %%scale01, %%sa01, %%sb01;', 'mul.rn.f16x2 %%scale23, %%sa23, %%sb23;', 'mov.b32 {%%h0, %%h1}, %%scale01;', 'cvt.f32.f16 %%s0, %%h0;', 'cvt.f32.f16 %%s1, %%h1;', 'mov.b32 {%%h0, %%h1}, %%scale23;', 'cvt.f32.f16 %%s2, %%h0;', 'cvt.f32.f16 %%s3, %%h1;', 'mov.b32 {%%a0, %%a1, %%a2, %%a3}, %1;', 'mov.b32 {%%b0, %%b1, %%b2, %%b3}, %9;', 'cvt.rn.f16x2.e2m1x2 %%fa0, %%a0;', 'cvt.rn.f16x2.e2m1x2 %%fa1, %%a1;', 'cvt.rn.f16x2.e2m1x2 %%fa2, %%a2;', 'cvt.rn.f16x2.e2m1x2 %%fa3, %%a3;', 'cvt.rn.f16x2.e2m1x2 %%fb0, %%b0;', 'cvt.rn.f16x2.e2m1x2 %%fb1, %%b1;', 'cvt.rn.f16x2.e2m1x2 %%fb2, %%b2;', 'cvt.rn.f16x2.e2m1x2 %%fb3, %%b3;', 'mul.rn.f16x2 %%p0, %%fa0, %%fb0;', 'fma.rn.f16x2 %%p0, %%fa1, %%fb1, %%p0;', 'fma.rn.f16x2 %%p0, %%fa2, %%fb2, %%p0;', 'fma.rn.f16x2 %%p0, %%fa3, %%fb3, %%p0;', 'mov.b32 {%%a0, %%a1, %%a2, %%a3}, %2;', 'mov.b32 {%%b0, %%b1, %%b2, %%b3}, %10;', 'cvt.rn.f16x2.e2m1x2 %%fa0, %%a0;', 'cvt.rn.f16x2.e2m1x2 %%fa1, %%a1;', 'cvt.rn.f16x2.e2m1x2 %%fa2, %%a2;', 'cvt.rn.f16x2.e2m1x2 %%fa3, %%a3;', 'cvt.rn.f16x2.e2m1x2 %%fb0, %%b0;', 'cvt.rn.f16x2.e2m1x2 %%fb1, %%b1;', 'cvt.rn.f16x2.e2m1x2 %%fb2, %%b2;', 'cvt.rn.f16x2.e2m1x2 %%fb3, %%b3;', 'fma.rn.f16x2 %%p0, %%fa0, %%fb0, %%p0;', 'fma.rn.f16x2 %%p0, %%fa1, %%fb1, %%p0;', 'fma.rn.f16x2 %%p0, %%fa2, %%fb2, %%p0;', 'fma.rn.f16x2 %%p0, %%fa3, %%fb3, %%p0;', 'mov.b32 {%%h0, %%h1}, %%p0;', 'cvt.f32.f16 %%f0, %%h0;', 'cvt.f32.f16 %%f1, %%h1;', 'add.f32 %%acc0, %%f0, %%f1;', 'mul.f32 %%acc0, %%acc0, %%s0;', 'mov.b32 {%%a0, %%a1, %%a2, %%a3}, %3;', 'mov.b32 {%%b0, %%b1, %%b2, %%b3}, %11;', 'cvt.rn.f16x2.e2m1x2 %%fa0, %%a0;', 'cvt.rn.f16x2.e2m1x2 %%fa1, %%a1;', 'cvt.rn.f16x2.e2m1x2 %%fa2, %%a2;', 'cvt.rn.f16x2.e2m1x2 %%fa3, %%a3;', 'cvt.rn.f16x2.e2m1x2 %%fb0, %%b0;', 'cvt.rn.f16x2.e2m1x2 %%fb1, %%b1;', 'cvt.rn.f16x2.e2m1x2 %%fb2, %%b2;', 'cvt.rn.f16x2.e2m1x2 %%fb3, %%b3;', 'mul.rn.f16x2 %%p1, %%fa0, %%fb0;', 'fma.rn.f16x2 %%p1, %%fa1, %%fb1, %%p1;', 'fma.rn.f16x2 %%p1, %%fa2, %%fb2, %%p1;', 'fma.rn.f16x2 %%p1, %%fa3, %%fb3, %%p1;', 'mov.b32 {%%a0, %%a1, %%a2, %%a3}, %4;', 'mov.b32 {%%b0, %%b1, %%b2, %%b3}, %12;', 'cvt.rn.f16x2.e2m1x2 %%fa0, %%a0;', 'cvt.rn.f16x2.e2m1x2 %%fa1, %%a1;', 'cvt.rn.f16x2.e2m1x2 %%fa2, %%a2;', 'cvt.rn.f16x2.e2m1x2 %%fa3, %%a3;', 'cvt.rn.f16x2.e2m1x2 %%fb0, %%b0;', 'cvt.rn.f16x2.e2m1x2 %%fb1, %%b1;', 'cvt.rn.f16x2.e2m1x2 %%fb2, %%b2;', 'cvt.rn.f16x2.e2m1x2 %%fb3, %%b3;', 'fma.rn.f16x2 %%p1, %%fa0, %%fb0, %%p1;', 'fma.rn.f16x2 %%p1, %%fa1, %%fb1, %%p1;', 'fma.rn.f16x2 %%p1, %%fa2, %%fb2, %%p1;', 'fma.rn.f16x2 %%p1, %%fa3, %%fb3, %%p1;', 'mov.b32 {%%h0, %%h1}, %%p1;', 'cvt.f32.f16 %%f0, %%h0;', 'cvt.f32.f16 %%f1, %%h1;', 'add.f32 %%acc1, %%f0, %%f1;', 'fma.rn.f32 %%acc0, %%acc1, %%s1, %%acc0;', 'mov.b32 {%%a0, %%a1, %%a2, %%a3}, %5;', 'mov.b32 {%%b0, %%b1, %%b2, %%b3}, %13;', 'cvt.rn.f16x2.e2m1x2 %%fa0, %%a0;', 'cvt.rn.f16x2.e2m1x2 %%fa1, %%a1;', 'cvt.rn.f16x2.e2m1x2 %%fa2, %%a2;', 'cvt.rn.f16x2.e2m1x2 %%fa3, %%a3;', 'cvt.rn.f16x2.e2m1x2 %%fb0, %%b0;', 'cvt.rn.f16x2.e2m1x2 %%fb1, %%b1;', 'cvt.rn.f16x2.e2m1x2 %%fb2, %%b2;', 'cvt.rn.f16x2.e2m1x2 %%fb3, %%b3;', 'mul.rn.f16x2 %%p2, %%fa0, %%fb0;', 'fma.rn.f16x2 %%p2, %%fa1, %%fb1, %%p2;', 'fma.rn.f16x2 %%p2, %%fa2, %%fb2, %%p2;', 'fma.rn.f16x2 %%p2, %%fa3, %%fb3, %%p2;', 'mov.b32 {%%a0, %%a1, %%a2, %%a3}, %6;', 'mov.b32 {%%b0, %%b1, %%b2, %%b3}, %14;', 'cvt.rn.f16x2.e2m1x2 %%fa0, %%a0;', 'cvt.rn.f16x2.e2m1x2 %%fa1, %%a1;', 'cvt.rn.f16x2.e2m1x2 %%fa2, %%a2;', 'cvt.rn.f16x2.e2m1x2 %%fa3, %%a3;', 'cvt.rn.f16x2.e2m1x2 %%fb0, %%b0;', 'cvt.rn.f16x2.e2m1x2 %%fb1, %%b1;', 'cvt.rn.f16x2.e2m1x2 %%fb2, %%b2;', 'cvt.rn.f16x2.e2m1x2 %%fb3, %%b3;', 'fma.rn.f16x2 %%p2, %%fa0, %%fb0, %%p2;', 'fma.rn.f16x2 %%p2, %%fa1, %%fb1, %%p2;', 'fma.rn.f16x2 %%p2, %%fa2, %%fb2, %%p2;', 'fma.rn.f16x2 %%p2, %%fa3, %%fb3, %%p2;', 'mov.b32 {%%h0, %%h1}, %%p2;', 'cvt.f32.f16 %%f0, %%h0;', 'cvt.f32.f16 %%f1, %%h1;', 'add.f32 %%acc2, %%f0, %%f1;', 'fma.rn.f32 %%acc0, %%acc2, %%s2, %%acc0;', 'mov.b32 {%%a0, %%a1, %%a2, %%a3}, %7;', 'mov.b32 {%%b0, %%b1, %%b2, %%b3}, %15;', 'cvt.rn.f16x2.e2m1x2 %%fa0, %%a0;', 'cvt.rn.f16x2.e2m1x2 %%fa1, %%a1;', 'cvt.rn.f16x2.e2m1x2 %%fa2, %%a2;', 'cvt.rn.f16x2.e2m1x2 %%fa3, %%a3;', 'cvt.rn.f16x2.e2m1x2 %%fb0, %%b0;', 'cvt.rn.f16x2.e2m1x2 %%fb1, %%b1;', 'cvt.rn.f16x2.e2m1x2 %%fb2, %%b2;', 'cvt.rn.f16x2.e2m1x2 %%fb3, %%b3;', 'mul.rn.f16x2 %%p3, %%fa0, %%fb0;', 'fma.rn.f16x2 %%p3, %%fa1, %%fb1, %%p3;', 'fma.rn.f16x2 %%p3, %%fa2, %%fb2, %%p3;', 'fma.rn.f16x2 %%p3, %%fa3, %%fb3, %%p3;', 'mov.b32 {%%a0, %%a1, %%a2, %%a3}, %8;', 'mov.b32 {%%b0, %%b1, %%b2, %%b3}, %16;', 'cvt.rn.f16x2.e2m1x2 %%fa0, %%a0;', 'cvt.rn.f16x2.e2m1x2 %%fa1, %%a1;', 'cvt.rn.f16x2.e2m1x2 %%fa2, %%a2;', 'cvt.rn.f16x2.e2m1x2 %%fa3, %%a3;', 'cvt.rn.f16x2.e2m1x2 %%fb0, %%b0;', 'cvt.rn.f16x2.e2m1x2 %%fb1, %%b1;', 'cvt.rn.f16x2.e2m1x2 %%fb2, %%b2;', 'cvt.rn.f16x2.e2m1x2 %%fb3, %%b3;', 'fma.rn.f16x2 %%p3, %%fa0, %%fb0, %%p3;', 'fma.rn.f16x2 %%p3, %%fa1, %%fb1, %%p3;', 'fma.rn.f16x2 %%p3, %%fa2, %%fb2, %%p3;', 'fma.rn.f16x2 %%p3, %%fa3, %%fb3, %%p3;', 'mov.b32 {%%h0, %%h1}, %%p3;', 'cvt.f32.f16 %%f0, %%h0;', 'cvt.f32.f16 %%f1, %%h1;', 'add.f32 %%acc3, %%f0, %%f1;', 'fma.rn.f32 %%tile_result, %%acc3, %%s3, %%acc0;', 'fma.rn.f32 %0, %%tile_result, %%one, %0;', '}'], outputs=[('+f', 'local_sum')], inputs=[('r', 'A0_0'), ('r', 'A0_1'), ('r', 'A0_2'), ('r', 'A0_3'), ('r', 'A1_0'), ('r', 'A1_1'), ('r', 'A1_2'), ('r', 'A1_3'), ('r', 'B0_0'), ('r', 'B0_1'), ('r', 'B0_2'), ('r', 'B0_3'), ('r', 'B1_0'), ('r', 'B1_1'), ('r', 'B1_2'), ('r', 'B1_3'), ('r', 'sfa_packed'), ('r', 'sfb_packed')], clobbers=[])
cb.func_end()
cb.raw("""extern "C" __global__ __launch_bounds__(128, 8)""")
cb.func_begin("block_scaled_gemv_fp4_fp8_fp16", TypeRef("void"), params=[Param("A", TypeRef("const uint8_t* __restrict__")), Param("B", TypeRef("const uint8_t* __restrict__")), Param("SFA", TypeRef("const uint8_t* __restrict__")), Param("SFB", TypeRef("const uint8_t* __restrict__")), Param("C", TypeRef("__half* __restrict__")), Param("M", TypeRef("int")), Param("K", TypeRef("int")), Param("L", TypeRef("int"))])
    cb.stmt('constexpr int ROWS_PER_BLOCK = 8')
    cb.stmt('constexpr int THREADS_PER_ROW = 16')
    cb.stmt('constexpr int TILE_K = 64')
    cb.stmt('const int tidx = threadIdx.x')
    cb.stmt('const int tidy = threadIdx.y')
    cb.stmt('const int block_row = blockIdx.x * ROWS_PER_BLOCK')
    cb.stmt('const int batch_idx = blockIdx.z')
    cb.stmt('const int global_row = block_row + tidy')
    cb.comment('Predicate for valid row - use throughout instead of early return')
    cb.stmt('const bool valid_row = global_row < M')
    cb.stmt('const int num_k_tiles = K >> 6')
    cb.comment('Calculate iterations such that ALL threads do the same number')
    cb.comment('Process in pairs where possible')
    cb.stmt('const int num_paired_iters = (num_k_tiles / (2 * THREADS_PER_ROW))')
    cb.stmt('const int remaining_after_pairs = num_k_tiles - (num_paired_iters * 2 * THREADS_PER_ROW)')
    cb.comment("Check if there's a full iteration where all 16 threads work")
    cb.stmt('const bool has_single_iter = (remaining_after_pairs >= THREADS_PER_ROW)')
    cb.stmt('const uint8_t* B_base = B + batch_idx * (128 * (K >> 1))')
    cb.stmt('const uint8_t* SFB_base = SFB + batch_idx * (128 * (K >> 4))')
    cb.stmt('const uint8_t* A_row = A + batch_idx * (M * (K >> 1)) + global_row * (K >> 1)')
    cb.stmt('const uint8_t* SFA_row = SFA + batch_idx * (M * (K >> 4)) + global_row * (K >> 4)')
    cb.stmt('float local_sum = 0.0f')
    cb.comment('All threads execute same number of paired iterations')
    cb.for_begin('int iter = 0', 'iter < num_paired_iters', '++iter', unroll=1)
        cb.stmt('const int tile = tidx + iter * 2 * THREADS_PER_ROW')
        cb.stmt('const int k_offset_0_half = (tile * TILE_K) >> 1')
        cb.stmt('const int k_offset_1_half = ((tile + THREADS_PER_ROW) * TILE_K) >> 1')
        cb.stmt('const int k_offset_0_quarter = (tile * TILE_K) >> 4')
        cb.stmt('const int k_offset_1_quarter = ((tile + THREADS_PER_ROW) * TILE_K) >> 4')
        cb.stmt('const float4* A_ptr_0 = reinterpret_cast<const float4*>(A_row + k_offset_0_half)')
        cb.stmt('const float4* B_ptr_0 = reinterpret_cast<const float4*>(B_base + k_offset_0_half)')
        cb.stmt('const float4* A_ptr_1 = reinterpret_cast<const float4*>(A_row + k_offset_1_half)')
        cb.stmt('const float4* B_ptr_1 = reinterpret_cast<const float4*>(B_base + k_offset_1_half)')
        cb.stmt('const uint32_t* sfa_ptr_0 = reinterpret_cast<const uint32_t*>(SFA_row + k_offset_0_quarter)')
        cb.stmt('const uint32_t* sfb_ptr_0 = reinterpret_cast<const uint32_t*>(SFB_base + k_offset_0_quarter)')
        cb.stmt('const uint32_t* sfa_ptr_1 = reinterpret_cast<const uint32_t*>(SFA_row + k_offset_1_quarter)')
        cb.stmt('const uint32_t* sfb_ptr_1 = reinterpret_cast<const uint32_t*>(SFB_base + k_offset_1_quarter)')
        cb.comment('Load all data first')
        cb.stmt('uint32_t sfa_t0 = __ldg(sfa_ptr_0)')
        cb.stmt('uint32_t sfb_t0 = __ldg(sfb_ptr_0)')
        cb.stmt('uint32_t sfa_t1 = __ldg(sfa_ptr_1)')
        cb.stmt('uint32_t sfb_t1 = __ldg(sfb_ptr_1)')
        cb.stmt('float4 A_data0_t0 = __ldg(A_ptr_0)')
        cb.stmt('float4 B_data0_t0 = __ldg(B_ptr_0)')
        cb.stmt('float4 A_data1_t0 = __ldg(A_ptr_0 + 1)')
        cb.stmt('float4 B_data1_t0 = __ldg(B_ptr_0 + 1)')
        cb.stmt('float4 A_data0_t1 = __ldg(A_ptr_1)')
        cb.stmt('float4 B_data0_t1 = __ldg(B_ptr_1)')
        cb.stmt('float4 A_data1_t1 = __ldg(A_ptr_1 + 1)')
        cb.stmt('float4 B_data1_t1 = __ldg(B_ptr_1 + 1)')
        cb.stmt('const uint32_t* A0_u32_t0 = reinterpret_cast<const uint32_t*>(&A_data0_t0)')
        cb.stmt('const uint32_t* A1_u32_t0 = reinterpret_cast<const uint32_t*>(&A_data1_t0)')
        cb.stmt('const uint32_t* B0_u32_t0 = reinterpret_cast<const uint32_t*>(&B_data0_t0)')
        cb.stmt('const uint32_t* B1_u32_t0 = reinterpret_cast<const uint32_t*>(&B_data1_t0)')
        cb.stmt('const uint32_t* A0_u32_t1 = reinterpret_cast<const uint32_t*>(&A_data0_t1)')
        cb.stmt('const uint32_t* A1_u32_t1 = reinterpret_cast<const uint32_t*>(&A_data1_t1)')
        cb.stmt('const uint32_t* B0_u32_t1 = reinterpret_cast<const uint32_t*>(&B_data0_t1)')
        cb.stmt('const uint32_t* B1_u32_t1 = reinterpret_cast<const uint32_t*>(&B_data1_t1)')
        cb.stmt('process_tile_fused(\n            local_sum,\n            A0_u32_t0[0], A0_u32_t0[1], A0_u32_t0[2], A0_u32_t0[3],\n            A1_u32_t0[0], A1_u32_t0[1], A1_u32_t0[2], A1_u32_t0[3],\n            B0_u32_t0[0], B0_u32_t0[1], B0_u32_t0[2], B0_u32_t0[3],\n            B1_u32_t0[0], B1_u32_t0[1], B1_u32_t0[2], B1_u32_t0[3],\n            sfa_t0, sfb_t0)')
        cb.stmt('process_tile_fused(\n            local_sum,\n            A0_u32_t1[0], A0_u32_t1[1], A0_u32_t1[2], A0_u32_t1[3],\n            A1_u32_t1[0], A1_u32_t1[1], A1_u32_t1[2], A1_u32_t1[3],\n            B0_u32_t1[0], B0_u32_t1[1], B0_u32_t1[2], B0_u32_t1[3],\n            B1_u32_t1[0], B1_u32_t1[1], B1_u32_t1[2], B1_u32_t1[3],\n            sfa_t1, sfb_t1)')
    cb.for_end()
    cb.comment('Single iteration block - all threads execute if has work')
    cb.comment('All 16 threads participate - no divergence')
    cb.if_begin('has_single_iter')
        cb.stmt('const int tile = tidx + num_paired_iters * 2 * THREADS_PER_ROW')
        cb.stmt('const int k_offset = tile * TILE_K')
        cb.stmt('const float4* A_ptr = reinterpret_cast<const float4*>(A_row + (k_offset >> 1))')
        cb.stmt('const float4* B_ptr = reinterpret_cast<const float4*>(B_base + (k_offset >> 1))')
        cb.stmt('float4 A_data0 = __ldg(A_ptr)')
        cb.stmt('float4 B_data0 = __ldg(B_ptr)')
        cb.stmt('float4 A_data1 = __ldg(A_ptr + 1)')
        cb.stmt('float4 B_data1 = __ldg(B_ptr + 1)')
        cb.stmt('uint32_t sfa_vec = __ldg(reinterpret_cast<const uint32_t*>(SFA_row + (k_offset >> 4)))')
        cb.stmt('uint32_t sfb_vec = __ldg(reinterpret_cast<const uint32_t*>(SFB_base + (k_offset >> 4)))')
        cb.stmt('const uint32_t* A0_u32 = reinterpret_cast<const uint32_t*>(&A_data0)')
        cb.stmt('const uint32_t* A1_u32 = reinterpret_cast<const uint32_t*>(&A_data1)')
        cb.stmt('const uint32_t* B0_u32 = reinterpret_cast<const uint32_t*>(&B_data0)')
        cb.stmt('const uint32_t* B1_u32 = reinterpret_cast<const uint32_t*>(&B_data1)')
        cb.stmt('process_tile_fused(\n            local_sum,\n            A0_u32[0], A0_u32[1], A0_u32[2], A0_u32[3],\n            A1_u32[0], A1_u32[1], A1_u32[2], A1_u32[3],\n            B0_u32[0], B0_u32[1], B0_u32[2], B0_u32[3],\n            B1_u32[0], B1_u32[1], B1_u32[2], B1_u32[3],\n            sfa_vec, sfb_vec)')
    cb.if_end()
    cb.comment('Final stragglers (< 16 tiles remaining)')
    cb.comment("Accept minimal divergence here - it's unavoidable for remainder")
    cb.comment('This only happens when num_k_tiles % THREADS_PER_ROW != 0')
    cb.stmt('const int final_start = num_paired_iters * 2 * THREADS_PER_ROW + \n                            (has_single_iter ? THREADS_PER_ROW : 0)')
    cb.if_begin('tidx + final_start < num_k_tiles')
        cb.stmt('const int tile = tidx + final_start')
        cb.stmt('const int k_offset = tile * TILE_K')
        cb.stmt('const float4* A_ptr = reinterpret_cast<const float4*>(A_row + (k_offset >> 1))')
        cb.stmt('const float4* B_ptr = reinterpret_cast<const float4*>(B_base + (k_offset >> 1))')
        cb.stmt('float4 A_data0 = __ldg(A_ptr)')
        cb.stmt('float4 B_data0 = __ldg(B_ptr)')
        cb.stmt('float4 A_data1 = __ldg(A_ptr + 1)')
        cb.stmt('float4 B_data1 = __ldg(B_ptr + 1)')
        cb.stmt('uint32_t sfa_vec = __ldg(reinterpret_cast<const uint32_t*>(SFA_row + (k_offset >> 4)))')
        cb.stmt('uint32_t sfb_vec = __ldg(reinterpret_cast<const uint32_t*>(SFB_base + (k_offset >> 4)))')
        cb.stmt('const uint32_t* A0_u32 = reinterpret_cast<const uint32_t*>(&A_data0)')
        cb.stmt('const uint32_t* A1_u32 = reinterpret_cast<const uint32_t*>(&A_data1)')
        cb.stmt('const uint32_t* B0_u32 = reinterpret_cast<const uint32_t*>(&B_data0)')
        cb.stmt('const uint32_t* B1_u32 = reinterpret_cast<const uint32_t*>(&B_data1)')
        cb.stmt('process_tile_fused(\n            local_sum,\n            A0_u32[0], A0_u32[1], A0_u32[2], A0_u32[3],\n            A1_u32[0], A1_u32[1], A1_u32[2], A1_u32[3],\n            B0_u32[0], B0_u32[1], B0_u32[2], B0_u32[3],\n            B1_u32[0], B1_u32[1], B1_u32[2], B1_u32[3],\n            sfa_vec, sfb_vec)')
    cb.if_end()
    cb.for_begin('int offset = 8', 'offset > 0', 'offset >>= 1', unroll=True)
        cb.stmt('local_sum += __shfl_xor_sync(0xffff, local_sum, offset, 16)')
    cb.for_end()
    cb.if_begin('tidx == 0 && valid_row')
        cb.stmt('C[batch_idx * M + global_row] = __float2half(local_sum)')
    cb.if_end()
cb.func_end()
cb.func_begin("gemv_fp4_fp8_fp16", TypeRef("torch::Tensor"), params=[Param("A", TypeRef("torch::Tensor")), Param("B", TypeRef("torch::Tensor")), Param("SFA", TypeRef("torch::Tensor")), Param("SFB", TypeRef("torch::Tensor")), Param("C", TypeRef("torch::Tensor")), Param("M", TypeRef("int")), Param("K", TypeRef("int")), Param("L", TypeRef("int"))])
    cb.stmt('TORCH_CHECK(A.is_cuda(), "A must be a CUDA tensor")')
    cb.stmt('TORCH_CHECK(B.is_cuda(), "B must be a CUDA tensor")')
    cb.stmt('TORCH_CHECK(SFA.is_cuda(), "SFA must be a CUDA tensor")')
    cb.stmt('TORCH_CHECK(SFB.is_cuda(), "SFB must be a CUDA tensor")')
    cb.stmt('TORCH_CHECK(C.is_cuda(), "C must be a CUDA tensor")')
    cb.stmt('constexpr int ROWS_PER_BLOCK = 8')
    cb.stmt('constexpr int THREADS_PER_ROW = 16')
    cb.stmt('const dim3 grid((M + ROWS_PER_BLOCK - 1) / ROWS_PER_BLOCK, 1, L)')
    cb.stmt('const dim3 block(THREADS_PER_ROW, ROWS_PER_BLOCK, 1)')
    cb.stmt('block_scaled_gemv_fp4_fp8_fp16<<<grid, block>>>(\n        reinterpret_cast<const uint8_t*>(A.data_ptr()),\n        reinterpret_cast<const uint8_t*>(B.data_ptr()),\n        reinterpret_cast<const uint8_t*>(SFA.data_ptr()),\n        reinterpret_cast<const uint8_t*>(SFB.data_ptr()),\n        reinterpret_cast<__half*>(C.data_ptr()),\n        M, K, L)')
    cb.stmt('cudaError_t err = cudaGetLastError()')
    cb.if_begin('err != cudaSuccess')
        cb.stmt('throw std::runtime_error(cudaGetErrorString(err))')
    cb.if_end()
    cb.ret('C')
cb.func_end()

cuda_source = cb.build()
