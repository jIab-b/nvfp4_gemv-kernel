# Builder commands for python source
# Automatically generated - execute to recreate AST/CUDA
# Mode: structured

# === Full CUDA Source ===
from cuda_ast import *
from builder import CudaBuilder, ASTBuilder, reg, imm, vec, mem, sym

cb = CudaBuilder()
cb.include("assert.h", system=True)
cb.include("cuda.h", system=True)
cb.include("stdio.h", system=True)
cb.include("cuda_runtime.h", system=True)
cb.include("torch/extension.h", system=True)
cb.include("ATen/cuda/CUDAContext.h", system=True)
cb.include("c10/cuda/CUDAGuard.h", system=True)
cb.include("cuda_fp4.h", system=True)
cb.include("cuda_bf16.h", system=True)
cb.include("cuda_fp8.h", system=True)
cb.comment('---- gemv.h ----')
cb.struct_begin("Gemv_params")
    cb.using("index_t", "uint64_t")
    cb.field(Field("b", TypeRef("int")))
    cb.field(Field("m", TypeRef("int")))
    cb.field(Field("k", TypeRef("int")))
    cb.field(Field("real_k", TypeRef("int")))
    cb.field(Field("a_ptr", TypeRef("void *__restrict__")))
    cb.field(Field("b_ptr", TypeRef("void *__restrict__")))
    cb.field(Field("sfa_ptr", TypeRef("void *__restrict__")))
    cb.field(Field("sfb_ptr", TypeRef("void *__restrict__")))
    cb.field(Field("o_ptr", TypeRef("void *__restrict__")))
    cb.field(Field("a_batch_stride", TypeRef("index_t")))
    cb.field(Field("b_batch_stride", TypeRef("index_t")))
    cb.field(Field("sfa_batch_stride", TypeRef("index_t")))
    cb.field(Field("sfb_batch_stride", TypeRef("index_t")))
    cb.field(Field("o_batch_stride", TypeRef("index_t")))
    cb.field(Field("a_row_stride", TypeRef("index_t")))
    cb.field(Field("b_row_stride", TypeRef("index_t")))
    cb.field(Field("sfa_row_stride", TypeRef("index_t")))
    cb.field(Field("sfb_row_stride", TypeRef("index_t")))
    cb.field(Field("o_row_stride", TypeRef("index_t")))
cb.struct_end()
cb.constexpr("BLOCK_SIZE", TypeRef("int"), '128', storage="static")
cb.comment('128')
cb.comment('---- load helpers ----')
cb.func_begin("load_block_16x2fp4_generic", TypeRef("void"), params=[Param("rowA", TypeRef("const __nv_fp4x2_e2m1*")), Param("vecB", TypeRef("const __nv_fp4x2_e2m1*")), Param("rowS_u16", TypeRef("const uint16_t*")), Param("vecS_u16", TypeRef("const uint16_t*")), Param("elem_base", TypeRef("int")), Param("block_base", TypeRef("int")), Param("(&a_regs)[2]", TypeRef("uint64_t")), Param("(&b_regs)[2]", TypeRef("uint64_t")), Param("&sfa_regs", TypeRef("uint16_t")), Param("&sfb_regs", TypeRef("uint16_t"))], qualifier="__device__", forceinline=True)
    cb.stmt('uint64_t rowA_addr = reinterpret_cast<uint64_t>(rowA + elem_base)')
    cb.stmt('uint64_t vecB_addr = reinterpret_cast<uint64_t>(vecB + elem_base)')
    cb.stmt('uint64_t rowS_addr = reinterpret_cast<uint64_t>(rowS_u16 + block_base)')
    cb.stmt('uint64_t vecS_addr = reinterpret_cast<uint64_t>(vecS_u16 + block_base)')
    cb.asm_raw(['ld.global.u64.v2 {%0, %1}, [%4];', 'ld.global.u64.v2 {%2, %3}, [%5];'], outputs=[('=l', 'a_regs[0]'), ('=l', 'a_regs[1]'), ('=l', 'b_regs[0]'), ('=l', 'b_regs[1]')], inputs=[('l', 'rowA_addr'), ('l', 'vecB_addr')], clobbers=[])
    cb.asm_raw(['ld.global.u16 %0, [%2];', 'ld.global.u16 %1, [%3];'], outputs=[('=h', 'sfa_regs'), ('=h', 'sfb_regs')], inputs=[('l', 'rowS_addr'), ('l', 'vecS_addr')], clobbers=[])
cb.func_end()
cb.comment('k = 3584')
cb.func_begin("load_block_16x2fp4_k3584", TypeRef("void"), params=[Param("rowA", TypeRef("const __nv_fp4x2_e2m1*")), Param("vecB", TypeRef("const __nv_fp4x2_e2m1*")), Param("rowS_u16", TypeRef("const uint16_t*")), Param("vecS_u16", TypeRef("const uint16_t*")), Param("elem_base", TypeRef("int")), Param("block_base", TypeRef("int")), Param("(&a_regs)[2]", TypeRef("uint64_t")), Param("(&b_regs)[2]", TypeRef("uint64_t")), Param("&sfa_regs", TypeRef("uint16_t")), Param("&sfb_regs", TypeRef("uint16_t"))], qualifier="__device__", forceinline=True)
    cb.stmt('uint64_t rowA_addr = reinterpret_cast<uint64_t>(rowA + elem_base)')
    cb.stmt('uint64_t vecB_addr = reinterpret_cast<uint64_t>(vecB + elem_base)')
    cb.stmt('uint64_t rowS_addr = reinterpret_cast<uint64_t>(rowS_u16 + block_base)')
    cb.stmt('uint64_t vecS_addr = reinterpret_cast<uint64_t>(vecS_u16 + block_base)')
    cb.asm_raw(['ld.global.cs.u64.v2 {%0, %1}, [%4];', 'ld.global.L2::128B.u64.v2 {%2, %3}, [%5];'], outputs=[('=l', 'a_regs[0]'), ('=l', 'a_regs[1]'), ('=l', 'b_regs[0]'), ('=l', 'b_regs[1]')], inputs=[('l', 'rowA_addr'), ('l', 'vecB_addr')], clobbers=[])
    cb.asm_raw(['ld.global.cs.u16 %0, [%2];', 'ld.global.L2::128B.u16 %1, [%3];'], outputs=[('=h', 'sfa_regs'), ('=h', 'sfb_regs')], inputs=[('l', 'rowS_addr'), ('l', 'vecS_addr')], clobbers=[])
cb.func_end()
cb.comment('k = 8192')
cb.func_begin("load_block_16x2fp4_k8192", TypeRef("void"), params=[Param("rowA", TypeRef("const __nv_fp4x2_e2m1*")), Param("vecB", TypeRef("const __nv_fp4x2_e2m1*")), Param("rowS_u16", TypeRef("const uint16_t*")), Param("vecS_u16", TypeRef("const uint16_t*")), Param("elem_base", TypeRef("int")), Param("block_base", TypeRef("int")), Param("(&a_regs)[2]", TypeRef("uint64_t")), Param("(&b_regs)[2]", TypeRef("uint64_t")), Param("&sfa_regs", TypeRef("uint16_t")), Param("&sfb_regs", TypeRef("uint16_t"))], qualifier="__device__", forceinline=True)
    cb.stmt('uint64_t rowA_addr = reinterpret_cast<uint64_t>(rowA + elem_base)')
    cb.stmt('uint64_t vecB_addr = reinterpret_cast<uint64_t>(vecB + elem_base)')
    cb.stmt('uint64_t rowS_addr = reinterpret_cast<uint64_t>(rowS_u16 + block_base)')
    cb.stmt('uint64_t vecS_addr = reinterpret_cast<uint64_t>(vecS_u16 + block_base)')
    cb.asm_raw(['ld.global.cs.u64.v2 {%0, %1}, [%4];', 'ld.global.u64.v2 {%2, %3}, [%5];'], outputs=[('=l', 'a_regs[0]'), ('=l', 'a_regs[1]'), ('=l', 'b_regs[0]'), ('=l', 'b_regs[1]')], inputs=[('l', 'rowA_addr'), ('l', 'vecB_addr')], clobbers=[])
    cb.asm_raw(['ld.global.lu.u16 %0, [%2];', 'ld.global.u16 %1, [%3];'], outputs=[('=h', 'sfa_regs'), ('=h', 'sfb_regs')], inputs=[('l', 'rowS_addr'), ('l', 'vecS_addr')], clobbers=[])
cb.func_end()
cb.comment('k = 1024')
cb.func_begin("load_block_16x2fp4_k1024", TypeRef("void"), params=[Param("rowA", TypeRef("const __nv_fp4x2_e2m1*")), Param("vecB", TypeRef("const __nv_fp4x2_e2m1*")), Param("rowS_u16", TypeRef("const uint16_t*")), Param("vecS_u16", TypeRef("const uint16_t*")), Param("elem_base", TypeRef("int")), Param("block_base", TypeRef("int")), Param("(&a_regs)[2]", TypeRef("uint64_t")), Param("(&b_regs)[2]", TypeRef("uint64_t")), Param("&sfa_regs", TypeRef("uint16_t")), Param("&sfb_regs", TypeRef("uint16_t"))], qualifier="__device__", forceinline=True)
    cb.stmt('uint64_t rowA_addr = reinterpret_cast<uint64_t>(rowA + elem_base)')
    cb.stmt('uint64_t vecB_addr = reinterpret_cast<uint64_t>(vecB + elem_base)')
    cb.stmt('uint64_t rowS_addr = reinterpret_cast<uint64_t>(rowS_u16 + block_base)')
    cb.stmt('uint64_t vecS_addr = reinterpret_cast<uint64_t>(vecS_u16 + block_base)')
    cb.asm_raw(['ld.global.cs.u64.v2 {%0, %1}, [%4];', 'ld.global.u64.v2 {%2, %3}, [%5];'], outputs=[('=l', 'a_regs[0]'), ('=l', 'a_regs[1]'), ('=l', 'b_regs[0]'), ('=l', 'b_regs[1]')], inputs=[('l', 'rowA_addr'), ('l', 'vecB_addr')], clobbers=[])
    cb.asm_raw(['ld.global.cs.u16 %0, [%2];', 'ld.global.u16 %1, [%3];'], outputs=[('=h', 'sfa_regs'), ('=h', 'sfb_regs')], inputs=[('l', 'rowS_addr'), ('l', 'vecS_addr')], clobbers=[])
cb.func_end()
cb.comment('Compile-time dispatcher')
cb.func_begin("load_block_16x2fp4", TypeRef("void"), params=[Param("rowA", TypeRef("const __nv_fp4x2_e2m1*")), Param("vecB", TypeRef("const __nv_fp4x2_e2m1*")), Param("rowS_u16", TypeRef("const uint16_t*")), Param("vecS_u16", TypeRef("const uint16_t*")), Param("elem_base", TypeRef("int")), Param("block_base", TypeRef("int")), Param("(&a_regs)[2]", TypeRef("uint64_t")), Param("(&b_regs)[2]", TypeRef("uint64_t")), Param("&sfa_regs", TypeRef("uint16_t")), Param("&sfb_regs", TypeRef("uint16_t"))], qualifier="__device__", forceinline=True, template=['int K'])
    cb.if_begin('K == 3584', constexpr=True)
        cb.stmt('load_block_16x2fp4_k3584(\n            rowA, vecB, rowS_u16, vecS_u16,\n            elem_base, block_base, a_regs, b_regs, sfa_regs, sfb_regs)')
    cb.else_begin()
        cb.if_begin('K == 8192', constexpr=True)
        cb.stmt('load_block_16x2fp4_k8192(\n            rowA, vecB, rowS_u16, vecS_u16,\n            elem_base, block_base, a_regs, b_regs, sfa_regs, sfb_regs)')
    cb.else_begin()
        cb.if_begin('K == 1024', constexpr=True)
        cb.stmt('load_block_16x2fp4_k1024(\n            rowA, vecB, rowS_u16, vecS_u16,\n            elem_base, block_base, a_regs, b_regs, sfa_regs, sfb_regs)')
    cb.else_begin()
        cb.comment('generic / fallback')
        cb.stmt('load_block_16x2fp4_generic(\n            rowA, vecB, rowS_u16, vecS_u16,\n            elem_base, block_base, a_regs, b_regs, sfa_regs, sfb_regs)')
    cb.if_end()
    cb.if_end()
    cb.if_end()
cb.func_end()
cb.func_begin("load_block_32x2fp4", TypeRef("void"), params=[Param("rowA", TypeRef("const __nv_fp4x2_e2m1*")), Param("vecB", TypeRef("const __nv_fp4x2_e2m1*")), Param("rowS_u16", TypeRef("const uint16_t*")), Param("vecS_u16", TypeRef("const uint16_t*")), Param("elem_base", TypeRef("int")), Param("block_base", TypeRef("int")), Param("(&a_regs)[4]", TypeRef("uint64_t")), Param("(&b_regs)[4]", TypeRef("uint64_t")), Param("(&sfa_regs)[2]", TypeRef("uint16_t")), Param("(&sfb_regs)[2]", TypeRef("uint16_t"))], qualifier="__device__", forceinline=True)
    cb.stmt('uint64_t rowA_addr = reinterpret_cast<uint64_t>(rowA + elem_base)')
    cb.stmt('uint64_t vecB_addr = reinterpret_cast<uint64_t>(vecB + elem_base)')
    cb.asm_raw(['ld.global.L1::no_allocate.L2::evict_first.L2::256B.v4.u64 {%0, %1, %2, %3}, [%8];', 'ld.global.L1::evict_last.L2::evict_last.v4.u64 {%4, %5, %6, %7}, [%9];'], outputs=[('=l', 'a_regs[0]'), ('=l', 'a_regs[1]'), ('=l', 'a_regs[2]'), ('=l', 'a_regs[3]'), ('=l', 'b_regs[0]'), ('=l', 'b_regs[1]'), ('=l', 'b_regs[2]'), ('=l', 'b_regs[3]')], inputs=[('l', 'rowA_addr'), ('l', 'vecB_addr')], clobbers=[])
    cb.stmt('uint64_t rowS_addr = reinterpret_cast<uint64_t>(rowS_u16 + block_base * 2)')
    cb.stmt('uint64_t vecS_addr = reinterpret_cast<uint64_t>(vecS_u16 + block_base * 2)')
    cb.asm_raw(['ld.global.L1::no_allocate.v2.u16 {%0, %1}, [%4];', 'ld.global.L1::evict_last.v2.u16 {%2, %3}, [%5];'], outputs=[('=h', 'sfa_regs[0]'), ('=h', 'sfa_regs[1]'), ('=h', 'sfb_regs[0]'), ('=h', 'sfb_regs[1]')], inputs=[('l', 'rowS_addr'), ('l', 'vecS_addr')], clobbers=[])
cb.func_end()
cb.func_begin("block_scaled_fma_32x2fp4", TypeRef("__half"), params=[Param("(&a_regs)[4]", TypeRef("const uint64_t")), Param("(&b_regs)[4]", TypeRef("const uint64_t")), Param("(&sfa_regs)[2]", TypeRef("const uint16_t")), Param("(&sfb_regs)[2]", TypeRef("const uint16_t"))], qualifier="__device__", forceinline=True)
    cb.stmt('const uint32_t* a_regs_packed = reinterpret_cast<const uint32_t*>(&a_regs)')
    cb.stmt('const uint32_t* b_regs_packed = reinterpret_cast<const uint32_t*>(&b_regs)')
    cb.stmt('uint16_t out_half_bits')
    cb.asm_raw(['{', '.reg .b8 a0, a1, a2, a3, a4, a5, a6, a7;', '.reg .b8 b0, b1, b2, b3, b4, b5, b6, b7;', '.reg .f16x2 sfa_f16x2_0, sfa_f16x2_1, sfb_f16x2_0, sfb_f16x2_1;', '.reg .f16x2 sf_f16x2_0, sf_f16x2_1;', '.reg .f16x2 scale0, scale1, scale2, scale3;', '.reg .f16x2 accum_total, accum_group;', '.reg .f16x2 cvt_a0, cvt_a1, cvt_a2, cvt_a3, cvt_a4, cvt_a5, cvt_a6, cvt_a7;', '.reg .f16x2 cvt_b0, cvt_b1, cvt_b2, cvt_b3, cvt_b4, cvt_b5, cvt_b6, cvt_b7;', '.reg .f16 lane0, lane1, result_f16;', 'cvt.rn.f16x2.e4m3x2 sfa_f16x2_0, %17;', 'cvt.rn.f16x2.e4m3x2 sfa_f16x2_1, %18;', 'cvt.rn.f16x2.e4m3x2 sfb_f16x2_0, %19;', 'cvt.rn.f16x2.e4m3x2 sfb_f16x2_1, %20;', 'mul.rn.f16x2 sf_f16x2_0, sfa_f16x2_0, sfb_f16x2_0;', 'mul.rn.f16x2 sf_f16x2_1, sfa_f16x2_1, sfb_f16x2_1;', 'mov.b32 {lane0, lane1}, sf_f16x2_0;', 'mov.b32 scale0, {lane0, lane0};', 'mov.b32 scale1, {lane1, lane1};', 'mov.b32 {lane0, lane1}, sf_f16x2_1;', 'mov.b32 scale2, {lane0, lane0};', 'mov.b32 scale3, {lane1, lane1};', 'mov.b32 accum_total, 0;', 'mov.b32 {a0, a1, a2, a3}, %1; mov.b32 {a4, a5, a6, a7}, %2;', 'mov.b32 {b0, b1, b2, b3}, %9; mov.b32 {b4, b5, b6, b7}, %10;', 'cvt.rn.f16x2.e2m1x2 cvt_a0, a0; cvt.rn.f16x2.e2m1x2 cvt_a1, a1; cvt.rn.f16x2.e2m1x2 cvt_a2, a2; cvt.rn.f16x2.e2m1x2 cvt_a3, a3; cvt.rn.f16x2.e2m1x2 cvt_a4, a4; cvt.rn.f16x2.e2m1x2 cvt_a5, a5; cvt.rn.f16x2.e2m1x2 cvt_a6, a6; cvt.rn.f16x2.e2m1x2 cvt_a7, a7;', 'cvt.rn.f16x2.e2m1x2 cvt_b0, b0; cvt.rn.f16x2.e2m1x2 cvt_b1, b1; cvt.rn.f16x2.e2m1x2 cvt_b2, b2; cvt.rn.f16x2.e2m1x2 cvt_b3, b3; cvt.rn.f16x2.e2m1x2 cvt_b4, b4; cvt.rn.f16x2.e2m1x2 cvt_b5, b5; cvt.rn.f16x2.e2m1x2 cvt_b6, b6; cvt.rn.f16x2.e2m1x2 cvt_b7, b7;', 'mov.b32 accum_group, 0; fma.rn.f16x2 accum_group, cvt_a0, cvt_b0, accum_group; fma.rn.f16x2 accum_group, cvt_a1, cvt_b1, accum_group; fma.rn.f16x2 accum_group, cvt_a2, cvt_b2, accum_group; fma.rn.f16x2 accum_group, cvt_a3, cvt_b3, accum_group; fma.rn.f16x2 accum_group, cvt_a4, cvt_b4, accum_group; fma.rn.f16x2 accum_group, cvt_a5, cvt_b5, accum_group; fma.rn.f16x2 accum_group, cvt_a6, cvt_b6, accum_group; fma.rn.f16x2 accum_group, cvt_a7, cvt_b7, accum_group;', 'mul.rn.f16x2 accum_group, scale0, accum_group; add.rn.f16x2 accum_total, accum_total, accum_group;', 'mov.b32 {a0, a1, a2, a3}, %3; mov.b32 {a4, a5, a6, a7}, %4;', 'mov.b32 {b0, b1, b2, b3}, %11; mov.b32 {b4, b5, b6, b7}, %12;', 'cvt.rn.f16x2.e2m1x2 cvt_a0, a0; cvt.rn.f16x2.e2m1x2 cvt_a1, a1; cvt.rn.f16x2.e2m1x2 cvt_a2, a2; cvt.rn.f16x2.e2m1x2 cvt_a3, a3; cvt.rn.f16x2.e2m1x2 cvt_a4, a4; cvt.rn.f16x2.e2m1x2 cvt_a5, a5; cvt.rn.f16x2.e2m1x2 cvt_a6, a6; cvt.rn.f16x2.e2m1x2 cvt_a7, a7;', 'cvt.rn.f16x2.e2m1x2 cvt_b0, b0; cvt.rn.f16x2.e2m1x2 cvt_b1, b1; cvt.rn.f16x2.e2m1x2 cvt_b2, b2; cvt.rn.f16x2.e2m1x2 cvt_b3, b3; cvt.rn.f16x2.e2m1x2 cvt_b4, b4; cvt.rn.f16x2.e2m1x2 cvt_b5, b5; cvt.rn.f16x2.e2m1x2 cvt_b6, b6; cvt.rn.f16x2.e2m1x2 cvt_b7, b7;', 'mov.b32 accum_group, 0; fma.rn.f16x2 accum_group, cvt_a0, cvt_b0, accum_group; fma.rn.f16x2 accum_group, cvt_a1, cvt_b1, accum_group; fma.rn.f16x2 accum_group, cvt_a2, cvt_b2, accum_group; fma.rn.f16x2 accum_group, cvt_a3, cvt_b3, accum_group; fma.rn.f16x2 accum_group, cvt_a4, cvt_b4, accum_group; fma.rn.f16x2 accum_group, cvt_a5, cvt_b5, accum_group; fma.rn.f16x2 accum_group, cvt_a6, cvt_b6, accum_group; fma.rn.f16x2 accum_group, cvt_a7, cvt_b7, accum_group;', 'mul.rn.f16x2 accum_group, scale1, accum_group; add.rn.f16x2 accum_total, accum_total, accum_group;', 'mov.b32 {a0, a1, a2, a3}, %5; mov.b32 {a4, a5, a6, a7}, %6;', 'mov.b32 {b0, b1, b2, b3}, %13; mov.b32 {b4, b5, b6, b7}, %14;', 'cvt.rn.f16x2.e2m1x2 cvt_a0, a0; cvt.rn.f16x2.e2m1x2 cvt_a1, a1; cvt.rn.f16x2.e2m1x2 cvt_a2, a2; cvt.rn.f16x2.e2m1x2 cvt_a3, a3; cvt.rn.f16x2.e2m1x2 cvt_a4, a4; cvt.rn.f16x2.e2m1x2 cvt_a5, a5; cvt.rn.f16x2.e2m1x2 cvt_a6, a6; cvt.rn.f16x2.e2m1x2 cvt_a7, a7;', 'cvt.rn.f16x2.e2m1x2 cvt_b0, b0; cvt.rn.f16x2.e2m1x2 cvt_b1, b1; cvt.rn.f16x2.e2m1x2 cvt_b2, b2; cvt.rn.f16x2.e2m1x2 cvt_b3, b3; cvt.rn.f16x2.e2m1x2 cvt_b4, b4; cvt.rn.f16x2.e2m1x2 cvt_b5, b5; cvt.rn.f16x2.e2m1x2 cvt_b6, b6; cvt.rn.f16x2.e2m1x2 cvt_b7, b7;', 'mov.b32 accum_group, 0; fma.rn.f16x2 accum_group, cvt_a0, cvt_b0, accum_group; fma.rn.f16x2 accum_group, cvt_a1, cvt_b1, accum_group; fma.rn.f16x2 accum_group, cvt_a2, cvt_b2, accum_group; fma.rn.f16x2 accum_group, cvt_a3, cvt_b3, accum_group; fma.rn.f16x2 accum_group, cvt_a4, cvt_b4, accum_group; fma.rn.f16x2 accum_group, cvt_a5, cvt_b5, accum_group; fma.rn.f16x2 accum_group, cvt_a6, cvt_b6, accum_group; fma.rn.f16x2 accum_group, cvt_a7, cvt_b7, accum_group;', 'mul.rn.f16x2 accum_group, scale2, accum_group; add.rn.f16x2 accum_total, accum_total, accum_group;', 'mov.b32 {a0, a1, a2, a3}, %7; mov.b32 {a4, a5, a6, a7}, %8;', 'mov.b32 {b0, b1, b2, b3}, %15; mov.b32 {b4, b5, b6, b7}, %16;', 'cvt.rn.f16x2.e2m1x2 cvt_a0, a0; cvt.rn.f16x2.e2m1x2 cvt_a1, a1; cvt.rn.f16x2.e2m1x2 cvt_a2, a2; cvt.rn.f16x2.e2m1x2 cvt_a3, a3; cvt.rn.f16x2.e2m1x2 cvt_a4, a4; cvt.rn.f16x2.e2m1x2 cvt_a5, a5; cvt.rn.f16x2.e2m1x2 cvt_a6, a6; cvt.rn.f16x2.e2m1x2 cvt_a7, a7;', 'cvt.rn.f16x2.e2m1x2 cvt_b0, b0; cvt.rn.f16x2.e2m1x2 cvt_b1, b1; cvt.rn.f16x2.e2m1x2 cvt_b2, b2; cvt.rn.f16x2.e2m1x2 cvt_b3, b3; cvt.rn.f16x2.e2m1x2 cvt_b4, b4; cvt.rn.f16x2.e2m1x2 cvt_b5, b5; cvt.rn.f16x2.e2m1x2 cvt_b6, b6; cvt.rn.f16x2.e2m1x2 cvt_b7, b7;', 'mov.b32 accum_group, 0; fma.rn.f16x2 accum_group, cvt_a0, cvt_b0, accum_group; fma.rn.f16x2 accum_group, cvt_a1, cvt_b1, accum_group; fma.rn.f16x2 accum_group, cvt_a2, cvt_b2, accum_group; fma.rn.f16x2 accum_group, cvt_a3, cvt_b3, accum_group; fma.rn.f16x2 accum_group, cvt_a4, cvt_b4, accum_group; fma.rn.f16x2 accum_group, cvt_a5, cvt_b5, accum_group; fma.rn.f16x2 accum_group, cvt_a6, cvt_b6, accum_group; fma.rn.f16x2 accum_group, cvt_a7, cvt_b7, accum_group;', 'mul.rn.f16x2 accum_group, scale3, accum_group; add.rn.f16x2 accum_total, accum_total, accum_group;', 'mov.b32 {lane0, lane1}, accum_total;', 'add.rn.f16 result_f16, lane0, lane1;', 'mov.b16 %0, result_f16;', '}'], outputs=[('=h', 'out_half_bits')], inputs=[('r', 'a_regs_packed[0]'), ('r', 'a_regs_packed[1]'), ('r', 'a_regs_packed[2]'), ('r', 'a_regs_packed[3]'), ('r', 'a_regs_packed[4]'), ('r', 'a_regs_packed[5]'), ('r', 'a_regs_packed[6]'), ('r', 'a_regs_packed[7]'), ('r', 'b_regs_packed[0]'), ('r', 'b_regs_packed[1]'), ('r', 'b_regs_packed[2]'), ('r', 'b_regs_packed[3]'), ('r', 'b_regs_packed[4]'), ('r', 'b_regs_packed[5]'), ('r', 'b_regs_packed[6]'), ('r', 'b_regs_packed[7]'), ('h', 'sfa_regs[0]'), ('h', 'sfa_regs[1]'), ('h', 'sfb_regs[0]'), ('h', 'sfb_regs[1]')], clobbers=['memory'])
    cb.stmt('union { uint16_t u; __half h; } conv')
    cb.stmt('conv.u = out_half_bits')
    cb.ret('conv.h')
cb.func_end()
cb.func_begin("block_scaled_fma_16x2fp4", TypeRef("float"), params=[Param("(&a_regs)[2]", TypeRef("const uint64_t")), Param("(&b_regs)[2]", TypeRef("const uint64_t")), Param("sfa_regs", TypeRef("uint16_t")), Param("sfb_regs", TypeRef("uint16_t"))], qualifier="__device__", forceinline=True)
    cb.stmt('uint32_t const* a_regs_packed = reinterpret_cast<uint32_t const*>(&a_regs)')
    cb.stmt('uint32_t const* b_regs_packed = reinterpret_cast<uint32_t const*>(&b_regs)')
    cb.stmt('float out_f32')
    cb.asm_raw(['{', '.reg .b8 a0_0, a0_1, a0_2, a0_3;', '.reg .b8 a0_4, a0_5, a0_6, a0_7;', '.reg .b8 b0_0, b0_1, b0_2, b0_3;', '.reg .b8 b0_4, b0_5, b0_6, b0_7;', '.reg .f16x2 sfa_f16x2, sfb_f16x2, sf_f16x2;', '.reg .f16x2 scale0_f16x2, scale1_f16x2;', '.reg .f16x2 accum_total, accum_group;', '.reg .f16x2 cvt_0_0, cvt_0_1, cvt_0_2, cvt_0_3;', '.reg .f16x2 cvt_0_4, cvt_0_5, cvt_0_6, cvt_0_7;', '.reg .f16x2 cvt_1_0, cvt_1_1, cvt_1_2, cvt_1_3;', '.reg .f16x2 cvt_1_4, cvt_1_5, cvt_1_6, cvt_1_7;', '.reg .f16 lane0, lane1, result_f16;', '.reg .f32 result_f32;', 'cvt.rn.f16x2.e4m3x2 sfa_f16x2, %5;', 'cvt.rn.f16x2.e4m3x2 sfb_f16x2, %6;', 'mul.rn.f16x2 sf_f16x2, sfa_f16x2, sfb_f16x2;', 'mov.b32 {lane0, lane1}, sf_f16x2;', 'mov.b32 scale0_f16x2, {lane0, lane0};', 'mov.b32 scale1_f16x2, {lane1, lane1};', 'mov.b32 accum_total, 0;', 'mov.b32 {a0_0, a0_1, a0_2, a0_3}, %1;', 'mov.b32 {a0_4, a0_5, a0_6, a0_7}, %2;', 'mov.b32 {b0_0, b0_1, b0_2, b0_3}, %3;', 'mov.b32 {b0_4, b0_5, b0_6, b0_7}, %4;', 'cvt.rn.f16x2.e2m1x2 cvt_0_0, a0_0;', 'cvt.rn.f16x2.e2m1x2 cvt_1_0, b0_0;', 'cvt.rn.f16x2.e2m1x2 cvt_0_1, a0_1;', 'cvt.rn.f16x2.e2m1x2 cvt_1_1, b0_1;', 'cvt.rn.f16x2.e2m1x2 cvt_0_2, a0_2;', 'cvt.rn.f16x2.e2m1x2 cvt_1_2, b0_2;', 'cvt.rn.f16x2.e2m1x2 cvt_0_3, a0_3;', 'cvt.rn.f16x2.e2m1x2 cvt_1_3, b0_3;', 'cvt.rn.f16x2.e2m1x2 cvt_0_4, a0_4;', 'cvt.rn.f16x2.e2m1x2 cvt_1_4, b0_4;', 'cvt.rn.f16x2.e2m1x2 cvt_0_5, a0_5;', 'cvt.rn.f16x2.e2m1x2 cvt_1_5, b0_5;', 'cvt.rn.f16x2.e2m1x2 cvt_0_6, a0_6;', 'cvt.rn.f16x2.e2m1x2 cvt_1_6, b0_6;', 'cvt.rn.f16x2.e2m1x2 cvt_0_7, a0_7;', 'cvt.rn.f16x2.e2m1x2 cvt_1_7, b0_7;', 'mov.b32 accum_group, 0;', 'fma.rn.f16x2 accum_group, cvt_0_0, cvt_1_0, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_1, cvt_1_1, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_2, cvt_1_2, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_3, cvt_1_3, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_4, cvt_1_4, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_5, cvt_1_5, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_6, cvt_1_6, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_7, cvt_1_7, accum_group;', 'mul.rn.f16x2 accum_group, scale0_f16x2, accum_group;', 'add.rn.f16x2 accum_total, accum_total, accum_group;', 'mov.b32 {a0_0, a0_1, a0_2, a0_3}, %7;', 'mov.b32 {a0_4, a0_5, a0_6, a0_7}, %8;', 'mov.b32 {b0_0, b0_1, b0_2, b0_3}, %9;', 'mov.b32 {b0_4, b0_5, b0_6, b0_7}, %10;', 'cvt.rn.f16x2.e2m1x2 cvt_0_0, a0_0;', 'cvt.rn.f16x2.e2m1x2 cvt_1_0, b0_0;', 'cvt.rn.f16x2.e2m1x2 cvt_0_1, a0_1;', 'cvt.rn.f16x2.e2m1x2 cvt_1_1, b0_1;', 'cvt.rn.f16x2.e2m1x2 cvt_0_2, a0_2;', 'cvt.rn.f16x2.e2m1x2 cvt_1_2, b0_2;', 'cvt.rn.f16x2.e2m1x2 cvt_0_3, a0_3;', 'cvt.rn.f16x2.e2m1x2 cvt_1_3, b0_3;', 'cvt.rn.f16x2.e2m1x2 cvt_0_4, a0_4;', 'cvt.rn.f16x2.e2m1x2 cvt_1_4, b0_4;', 'cvt.rn.f16x2.e2m1x2 cvt_0_5, a0_5;', 'cvt.rn.f16x2.e2m1x2 cvt_1_5, b0_5;', 'cvt.rn.f16x2.e2m1x2 cvt_0_6, a0_6;', 'cvt.rn.f16x2.e2m1x2 cvt_1_6, b0_6;', 'cvt.rn.f16x2.e2m1x2 cvt_0_7, a0_7;', 'cvt.rn.f16x2.e2m1x2 cvt_1_7, b0_7;', 'mov.b32 accum_group, 0;', 'fma.rn.f16x2 accum_group, cvt_0_0, cvt_1_0, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_1, cvt_1_1, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_2, cvt_1_2, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_3, cvt_1_3, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_4, cvt_1_4, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_5, cvt_1_5, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_6, cvt_1_6, accum_group;', 'fma.rn.f16x2 accum_group, cvt_0_7, cvt_1_7, accum_group;', 'mul.rn.f16x2 accum_group, scale1_f16x2, accum_group;', 'add.rn.f16x2 accum_total, accum_total, accum_group;', 'mov.b32 {lane0, lane1}, accum_total;', 'add.rn.f16 result_f16, lane0, lane1;', 'cvt.f32.f16 result_f32, result_f16;', 'mov.b32 %0, result_f32;', '}'], outputs=[('=f', 'out_f32')], inputs=[('r', 'a_regs_packed[0]'), ('r', 'a_regs_packed[1]'), ('r', 'b_regs_packed[0]'), ('r', 'b_regs_packed[1]'), ('h', 'sfa_regs'), ('h', 'sfb_regs'), ('r', 'a_regs_packed[2]'), ('r', 'a_regs_packed[3]'), ('r', 'b_regs_packed[2]'), ('r', 'b_regs_packed[3]')], clobbers=['memory'])
    cb.ret('out_f32')
cb.func_end()
cb.func_begin("gemv_kernel", TypeRef("void"), params=[Param("params", TypeRef("const __grid_constant__ Gemv_params"))], qualifier="__global__", template=['int ROWS_PER_BLOCK', 'int THREADS_PER_ROW', 'int ITERS', 'int K_SPECIAL', 'bool USE_32X2'], launch_bounds=LaunchBounds('ROWS_PER_BLOCK * THREADS_PER_ROW', 8))
    cb.stmt('const int tid   = threadIdx.x')
    cb.stmt('const int rib   = tid / THREADS_PER_ROW')
    cb.stmt('const int lane  = tid % THREADS_PER_ROW')
    cb.stmt('const int batch = blockIdx.z')
    cb.stmt('const int row   = blockIdx.x * ROWS_PER_BLOCK + rib')
    cb.stmt('const size_t A_batch_base   = static_cast<size_t>(batch) * params.a_batch_stride')
    cb.stmt('const size_t SFA_batch_base = static_cast<size_t>(batch) * params.sfa_batch_stride')
    cb.stmt('const size_t B_batch_base   = static_cast<size_t>(batch) * params.b_batch_stride')
    cb.stmt('const size_t SFB_batch_base = static_cast<size_t>(batch) * params.sfb_batch_stride')
    cb.stmt('const size_t C_batch_base   = static_cast<size_t>(batch) * params.o_batch_stride')
    cb.stmt('const __nv_fp4x2_e2m1* rowA = static_cast<const __nv_fp4x2_e2m1*>(params.a_ptr) + A_batch_base   + row * params.a_row_stride')
    cb.stmt('const __nv_fp8_e4m3*   rowS = static_cast<const __nv_fp8_e4m3*>(params.sfa_ptr) + SFA_batch_base + row * params.sfa_row_stride')
    cb.stmt('const __nv_fp4x2_e2m1* vecB = static_cast<const __nv_fp4x2_e2m1*>(params.b_ptr) + B_batch_base')
    cb.stmt('const __nv_fp8_e4m3*   vecS = static_cast<const __nv_fp8_e4m3*>(params.sfb_ptr) + SFB_batch_base')
    cb.stmt('const uint16_t* rowS_u16 = reinterpret_cast<const uint16_t*>(rowS)')
    cb.stmt('const uint16_t* vecS_u16 = reinterpret_cast<const uint16_t*>(vecS)')
    cb.stmt('float sum = 0.f')
    cb.if_begin('USE_32X2', constexpr=True)
        cb.comment('---- old gemv_kernel_v2 body ----')
        cb.for_begin('int idx = 0', 'idx < 2', '++idx', unroll=True)
        cb.comment('8192-special: 2 iters of 32 * 128 = 8192')
        cb.stmt('int block_base = idx * THREADS_PER_ROW + lane')
        cb.stmt('int elem_base  = block_base * 32')
        cb.stmt('uint64_t a_regs[4], b_regs[4]')
        cb.stmt('uint16_t sfa_regs[2], sfb_regs[2]')
        cb.stmt('load_block_32x2fp4(\n                rowA, vecB,\n                rowS_u16, vecS_u16,\n                elem_base, block_base,\n                a_regs, b_regs,\n                sfa_regs, sfb_regs)')
        cb.stmt('__half h = block_scaled_fma_32x2fp4(a_regs, b_regs, sfa_regs, sfb_regs)')
        cb.stmt('sum += __half2float(h)')
    cb.for_end()
        cb.stmt('__shared__ float sdata[THREADS_PER_ROW]')
        cb.stmt('sdata[lane] = sum')
        cb.stmt('__syncthreads()')
        cb.comment('THREADS_PER_ROW == 128 here')
        cb.if_begin('tid < 64')
        cb.stmt('sdata[lane] += sdata[lane + 64]')
    cb.if_end()
        cb.stmt('__syncthreads()')
        cb.if_begin('lane < 32')
        cb.stmt('float val = sdata[lane] + sdata[lane + 32]')
        cb.stmt('val += __shfl_down_sync(0xffffffff, val, 16)')
        cb.stmt('val += __shfl_down_sync(0xffffffff, val, 8)')
        cb.stmt('val += __shfl_down_sync(0xffffffff, val, 4)')
        cb.stmt('val += __shfl_down_sync(0xffffffff, val, 2)')
        cb.stmt('val += __shfl_down_sync(0xffffffff, val, 1)')
        cb.if_begin('lane == 0')
        cb.stmt('__half* out = (__half*)params.o_ptr + C_batch_base + row')
        cb.stmt('out[0] = __float2half(val)')
    cb.if_end()
    cb.if_end()
    cb.else_begin()
        cb.comment('---- old gemv_kernel body ----')
        cb.stmt('auto body = [&](int idx) {\n            int block_base = idx * THREADS_PER_ROW + lane;\n            int elem_base  = block_base * 16;\n\n            uint64_t a_regs[2], b_regs[2];\n            uint16_t sfa_regs, sfb_regs;\n\n            load_block_16x2fp4<K_SPECIAL>(\n                rowA, vecB,\n                rowS_u16, vecS_u16,\n                elem_base, block_base,\n                a_regs, b_regs,\n                sfa_regs, sfb_regs);\n            sum += block_scaled_fma_16x2fp4(a_regs, b_regs, sfa_regs, sfb_regs);\n        }')
        cb.if_begin('ITERS > 0', constexpr=True)
        cb.for_begin('int idx = 0', 'idx < ITERS', '++idx', unroll=True)
        cb.stmt('body(idx)')
    cb.for_end()
    cb.else_begin()
        cb.stmt('int iters = params.k / (THREADS_PER_ROW * 16)')
        cb.for_begin('int idx = 0', 'idx < iters', '++idx')
        cb.stmt('body(idx)')
    cb.for_end()
    cb.if_end()
        cb.for_begin('int offset = THREADS_PER_ROW / 2', 'offset > 0', 'offset /= 2', unroll=True)
        cb.stmt('sum += __shfl_down_sync(0xffffffffu, sum, offset, THREADS_PER_ROW)')
    cb.for_end()
        cb.if_begin('lane == 0')
        cb.stmt('__half* out = (__half*)params.o_ptr + C_batch_base + row')
        cb.stmt('out[0] = __float2half(sum)')
    cb.if_end()
    cb.if_end()
cb.func_end()
cb.func_begin("cuda_nvfp4_gemv", TypeRef("torch::Tensor"), params=[Param("A", TypeRef("torch::Tensor")), Param("B", TypeRef("torch::Tensor")), Param("C", TypeRef("torch::Tensor")), Param("SFA", TypeRef("torch::Tensor")), Param("SFB", TypeRef("torch::Tensor"))])
    cb.stmt('const auto sizes = A.sizes()')
    cb.stmt('const int M = sizes[0]')
    cb.stmt('const int K = sizes[1]')
    cb.stmt('const int L = sizes[2]')
    cb.stmt('Gemv_params params{}')
    cb.stmt('params.b = L')
    cb.stmt('params.m = M')
    cb.stmt('params.k = K')
    cb.stmt('params.a_ptr  = A.data_ptr()')
    cb.stmt('params.b_ptr  = B.data_ptr()')
    cb.stmt('params.sfa_ptr= SFA.data_ptr()')
    cb.stmt('params.sfb_ptr= SFB.data_ptr()')
    cb.stmt('params.o_ptr  = C.data_ptr()')
    cb.stmt('params.a_batch_stride  = A.stride(2)')
    cb.stmt('params.b_batch_stride  = B.stride(2)')
    cb.stmt('params.sfa_batch_stride= SFA.stride(2)')
    cb.stmt('params.sfb_batch_stride= SFB.stride(2)')
    cb.stmt('params.o_batch_stride  = C.stride(2)')
    cb.stmt('params.a_row_stride  = A.stride(0)')
    cb.stmt('params.b_row_stride  = B.stride(0)')
    cb.stmt('params.sfa_row_stride= SFA.stride(0)')
    cb.stmt('params.sfb_row_stride= SFB.stride(0)')
    cb.stmt('params.o_row_stride  = C.stride(0)')
    cb.if_begin('params.k <= 256')
        cb.comment('<= 512 FP4 values')
        cb.stmt('dim3 grid(params.m / 16, 1, params.b)')
        cb.stmt('dim3 block(128, 1, 1)')
        cb.comment('generic loader (K_SPECIAL = 0), 16x2 path')
        cb.stmt('gemv_kernel<16, 8, 0, 0, false><<<grid, block>>>(params)')
    cb.else_begin()
        cb.if_begin('params.k == 3584')
        cb.stmt('dim3 block(128, 1, 1)')
        cb.stmt('dim3 grid(params.m / 4, 1, params.b)')
        cb.comment('K_SPECIAL = 3584 -> uses k3584 loader, 16x2 path')
        cb.stmt('gemv_kernel<4, 32, 7, 3584, false><<<grid, block>>>(params)')
    cb.else_begin()
        cb.if_begin('params.k == 8192')
        cb.stmt('dim3 block(128, 1, 1)')
        cb.stmt('dim3 grid(params.m, 1, params.b)')
        cb.comment('8192 special case -> 32x2 path (former v2)')
        cb.stmt('gemv_kernel<1, 128, 0, 8192, true><<<grid, block>>>(params)')
    cb.else_begin()
        cb.if_begin('params.k == 1024')
        cb.stmt('dim3 block(128, 1, 1)')
        cb.stmt('dim3 grid(params.m / 8, 1, params.b)')
        cb.comment('K_SPECIAL = 1024 -> uses k1024 loader, 16x2 path')
        cb.stmt('gemv_kernel<8, 16, 4, 1024, false><<<grid, block>>>(params)')
    cb.else_begin()
        cb.stmt('dim3 block(128, 1, 1)')
        cb.stmt('dim3 grid(params.m / 8, 1, params.b)')
        cb.comment('generic loader again, 16x2 path')
        cb.stmt('gemv_kernel<8, 16, 0, 0, false><<<grid, block>>>(params)')
    cb.if_end()
    cb.if_end()
    cb.if_end()
    cb.if_end()
    cb.ret('C')
cb.func_end()

cuda_source = cb.build()
