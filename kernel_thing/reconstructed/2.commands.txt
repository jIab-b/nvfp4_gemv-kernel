# Builder commands for python source
# Automatically generated - execute to recreate AST/CUDA
# Mode: structured

# === Full CUDA Source ===
from cuda_ast import *
from builder import CudaBuilder, ASTBuilder, reg, imm, vec, mem, sym

cb = CudaBuilder()
cb.include("cuda_fp16.h", system=True)
cb.include("cuda_fp8.h", system=True)
cb.include("torch/library.h", system=True)
cb.include("ATen/ATen.h", system=True)
cb.include("ATen/core/Tensor.h", system=True)
cb.include("ATen/cuda/CUDAUtils.h", system=True)
cb.include("ATen/cuda/CUDAContext.h", system=True)
cb.constexpr("WARP_SIZE", TypeRef("int"), '32')
cb.func_begin("fp4x8_to_fp16x2x4", TypeRef("void"), params=[Param("*out", TypeRef("int")), Param("in", TypeRef("int"))], qualifier="__device__")
    cb.asm_raw(['{', '.reg .b8 tmp0, tmp1, tmp2, tmp3;', 'mov.b32 {tmp0, tmp1, tmp2, tmp3}, %4; // unpack 32-bit register to 4x fp4x2;', 'cvt.rn.f16x2.e2m1x2 %0, tmp0;', 'cvt.rn.f16x2.e2m1x2 %1, tmp1;', 'cvt.rn.f16x2.e2m1x2 %2, tmp2;', 'cvt.rn.f16x2.e2m1x2 %3, tmp3;', '}'], outputs=[('=r', 'out[0]'), ('=r', 'out[1]'), ('=r', 'out[2]'), ('=r', 'out[3]')], inputs=[('r', 'in')], clobbers=[])
cb.func_end()
cb.func_begin("ldcs_i16", TypeRef("void"), params=[Param("*dst", TypeRef("int16_t")), Param("*src", TypeRef("const void"))], qualifier="__device__")
    cb.asm_raw(['ld.global.L1::no_allocate.b16 %0, [%1];'], outputs=[('=h', 'dst[0]')], inputs=[('l', 'src')], clobbers=[])
cb.func_end()
cb.func_begin("ldca_i16", TypeRef("void"), params=[Param("*dst", TypeRef("int16_t")), Param("*src", TypeRef("const void"))], qualifier="__device__")
    cb.asm_raw(['ld.global.L1::evict_last.b16 %0, [%1];'], outputs=[('=h', 'dst[0]')], inputs=[('l', 'src')], clobbers=[])
cb.func_end()
cb.func_begin("ldcs_i16x2", TypeRef("void"), params=[Param("*dst", TypeRef("int16_t")), Param("*src", TypeRef("const void"))], qualifier="__device__")
    cb.asm_raw(['ld.global.L1::no_allocate.v2.b16 {%0, %1}, [%2];'], outputs=[('=h', 'dst[0]'), ('=h', 'dst[1]')], inputs=[('l', 'src')], clobbers=[])
cb.func_end()
cb.func_begin("ldca_i16x2", TypeRef("void"), params=[Param("*dst", TypeRef("int16_t")), Param("*src", TypeRef("const void"))], qualifier="__device__")
    cb.asm_raw(['ld.global.L1::evict_last.v2.b16 {%0, %1}, [%2];'], outputs=[('=h', 'dst[0]'), ('=h', 'dst[1]')], inputs=[('l', 'src')], clobbers=[])
cb.func_end()
cb.func_begin("ldcs_i32x4", TypeRef("void"), params=[Param("*dst", TypeRef("int")), Param("*src", TypeRef("const void"))], qualifier="__device__")
    cb.asm_raw(['ld.global.L1::no_allocate.v4.b32 {%0, %1, %2, %3}, [%4];'], outputs=[('=r', 'dst[0]'), ('=r', 'dst[1]'), ('=r', 'dst[2]'), ('=r', 'dst[3]')], inputs=[('l', 'src')], clobbers=[])
cb.func_end()
cb.func_begin("ldca_i32x4", TypeRef("void"), params=[Param("*dst", TypeRef("int")), Param("*src", TypeRef("const void"))], qualifier="__device__")
    cb.asm_raw(['ld.global.L1::evict_last.v4.b32 {%0, %1, %2, %3}, [%4];'], outputs=[('=r', 'dst[0]'), ('=r', 'dst[1]'), ('=r', 'dst[2]'), ('=r', 'dst[3]')], inputs=[('l', 'src')], clobbers=[])
cb.func_end()
cb.func_begin("ldcs_i32x8", TypeRef("void"), params=[Param("*dst", TypeRef("int")), Param("*src", TypeRef("const void"))], qualifier="__device__")
    cb.asm_raw(['ld.global.L1::no_allocate.L2::evict_first.v8.b32 {%0, %1, %2, %3, %4, %5, %6, %7}, [%8];'], outputs=[('=r', 'dst[0]'), ('=r', 'dst[1]'), ('=r', 'dst[2]'), ('=r', 'dst[3]'), ('=r', 'dst[4]'), ('=r', 'dst[5]'), ('=r', 'dst[6]'), ('=r', 'dst[7]')], inputs=[('l', 'src')], clobbers=[])
cb.func_end()
cb.func_begin("ldca_i32x8", TypeRef("void"), params=[Param("*dst", TypeRef("int")), Param("*src", TypeRef("const void"))], qualifier="__device__")
    cb.asm_raw(['ld.global.L1::evict_last.L2::evict_last.v8.b32 {%0, %1, %2, %3, %4, %5, %6, %7}, [%8];'], outputs=[('=r', 'dst[0]'), ('=r', 'dst[1]'), ('=r', 'dst[2]'), ('=r', 'dst[3]'), ('=r', 'dst[4]'), ('=r', 'dst[5]'), ('=r', 'dst[6]'), ('=r', 'dst[7]')], inputs=[('l', 'src')], clobbers=[])
cb.func_end()
cb.func_begin("fp8x2_to_fp16x2", TypeRef("void"), params=[Param("*out", TypeRef("half2")), Param("in", TypeRef("int16_t"))], qualifier="__device__")
    cb.stmt('int *out_i32 = reinterpret_cast<int *>(out)')
    cb.asm_raw(['cvt.rn.f16x2.e4m3x2 %0, %1;'], outputs=[('=r', 'out_i32[0]')], inputs=[('h', 'in')], clobbers=[])
cb.func_end()
cb.comment("to make our calculations simple, let's treat fp4x2 as a unit.")
cb.comment('hence, K = number of fp4x2 elements, and 8 elements share')
cb.comment('the same scale.')
cb.func_begin("kernel_v2h", TypeRef("void"), params=[Param("*A_ptr", TypeRef("const char")), Param("[L", TypeRef("//")), Param("M", TypeRef("void")), Param("*B_ptr", TypeRef("K] const char")), Param("[L", TypeRef("//")), Param("128", TypeRef("void")), Param("*SFA_ptr", TypeRef("K] const char")), Param("[L", TypeRef("//")), Param("M", TypeRef("void")), Param("*SFB_ptr", TypeRef("K/8] const char")), Param("[L", TypeRef("//")), Param("128", TypeRef("void")), Param("*C_ptr", TypeRef("K/8] half")), Param("[L", TypeRef("//")), Param("L", TypeRef("M] int")), Param("M", TypeRef("int"))], qualifier="__global__", template=['int BLOCK_M', 'int BLOCK_K', 'int K', 'int NUM_WARPS', 'int CP_SIZE'], launch_bounds=LaunchBounds('NUM_WARPS * WARP_SIZE'))
    cb.stmt('static_assert(BLOCK_K % CP_SIZE == 0)')
    cb.comment('each thread reads 16 bytes')
    cb.stmt('constexpr int TB_SIZE = NUM_WARPS * WARP_SIZE')
    cb.stmt('constexpr int SF_BLOCK_K = BLOCK_K / 8')
    cb.stmt('const int tid = threadIdx.x')
    cb.stmt('const int bid = blockIdx.x')
    cb.stmt('const int batch_id = blockIdx.y')
    cb.stmt('constexpr int num_cols = BLOCK_K / CP_SIZE')
    cb.comment('each thread reads 16-byte at a time')
    cb.stmt('static_assert(num_cols <= TB_SIZE)')
    cb.stmt('constexpr int num_rows = TB_SIZE / num_cols')
    cb.stmt('const int t_col = tid % num_cols')
    cb.stmt('const int t_row = tid / num_cols')
    cb.block_begin()
        cb.stmt('int off_m = bid * BLOCK_M')
        cb.stmt('int off_k = t_col * CP_SIZE')
        cb.stmt('A_ptr += batch_id * M * K + off_m * K + off_k')
        cb.stmt('B_ptr += batch_id * 128 * K + off_k')
        cb.stmt('C_ptr += batch_id * M + off_m')
        cb.stmt('SFA_ptr += (batch_id * M * K + off_m * K + off_k) / 8')
        cb.stmt('SFB_ptr += (batch_id * 128 * K + off_k) / 8')
    cb.block_end()
    cb.comment('load A & B')
    cb.stmt('int A_rmem[BLOCK_M / num_rows][CP_SIZE / 4]')
    cb.stmt('int B_rmem[CP_SIZE / 4]')
    cb.stmt('int16_t SFA_rmem[BLOCK_M / num_rows][CP_SIZE / 16]')
    cb.stmt('int16_t SFB_rmem[CP_SIZE / 16]')
    cb.stmt('half2 A_fp16x2[BLOCK_M / num_rows][CP_SIZE / 16][16]')
    cb.stmt('half2 B_fp16x2[CP_SIZE / 16][16]')
    cb.stmt('half2 SFA_fp16x2[BLOCK_M / num_rows][CP_SIZE / 16]')
    cb.stmt('half2 SFB_fp16x2[CP_SIZE / 16]')
    cb.stmt('half2 acc[BLOCK_M / num_rows][CP_SIZE / 16][2]')
    cb.stmt('float master_acc[BLOCK_M / num_rows] = {}')
    cb.stmt('const int num_iters = K / BLOCK_K')
    cb.for_begin('int iter_k = 0', 'iter_k < num_iters', 'iter_k++')
        cb.comment('load')
        cb.if_begin('CP_SIZE == 16', constexpr=True)
        cb.stmt('ldca_i16(SFB_rmem, SFB_ptr)')
        cb.stmt('ldca_i32x4(B_rmem, B_ptr)')
        cb.for_begin('int m = 0', 'm < BLOCK_M / num_rows', 'm++')
        cb.stmt('const int row = m * num_rows + t_row')
        cb.stmt('ldcs_i16(SFA_rmem[m], SFA_ptr + row * K / 8)')
        cb.stmt('ldcs_i32x4(A_rmem[m], A_ptr + row * K)')
    cb.for_end()
    cb.else_begin()
        cb.if_begin('CP_SIZE == 32', constexpr=True)
        cb.stmt('ldca_i16x2(SFB_rmem, SFB_ptr)')
        cb.stmt('ldca_i32x8(B_rmem, B_ptr)')
        cb.for_begin('int m = 0', 'm < BLOCK_M / num_rows', 'm++')
        cb.stmt('const int row = m * num_rows + t_row')
        cb.stmt('ldcs_i16x2(SFA_rmem[m], SFA_ptr + row * K / 8)')
        cb.stmt('ldcs_i32x8(A_rmem[m], A_ptr + row * K)')
    cb.for_end()
    cb.if_end()
    cb.if_end()
        cb.stmt('A_ptr += BLOCK_K')
        cb.stmt('B_ptr += BLOCK_K')
        cb.stmt('SFA_ptr += SF_BLOCK_K')
        cb.stmt('SFB_ptr += SF_BLOCK_K')
        cb.comment('unpack B')
        cb.for_begin('int i = 0', 'i < CP_SIZE / 16', 'i++')
        cb.stmt('SFB_fp16x2[i] = static_cast<half2>(reinterpret_cast<__nv_fp8x2_e4m3 *>(&SFB_rmem)[i])')
        cb.for_begin('int j = 0', 'j < 4', 'j++')
        cb.stmt('fp4x8_to_fp16x2x4(reinterpret_cast<int *>(&B_fp16x2[i][j * 4]), B_rmem[i * 4 + j])')
    cb.for_end()
    cb.for_end()
        cb.comment('unpack A')
        cb.for_begin('int m = 0', 'm < BLOCK_M / num_rows', 'm++')
        cb.raw("""for (int i = 0; i < CP_SIZE / 16; i++) {
            SFA_fp16x2[m][i] = static_cast<half2>(reinterpret_cast<__nv_fp8x2_e4m3 *>(&SFA_rmem[m])[i]);
            for (int j = 0; j < 4; j++)
              fp4x8_to_fp16x2x4(reinterpret_cast<int *>(&A_fp16x2[m][i][j * 4]), A_rmem[m][i * 4 + j]);
            SFA_fp16x2[m][i] = __hmul2(SFA_fp16x2[m][i], SFB_fp16x2[i]);  // pre-multiply scale
          }
    
        // compute
        for (int m = 0; m < BLOCK_M / num_rows; m++)
          for (int i = 0; i < CP_SIZE / 16; i++) {
          acc[m][i][0] = __hmul2(A_fp16x2[m][i][0], B_fp16x2[i][0]);
          acc[m][i][1] = __hmul2(A_fp16x2[m][i][8], B_fp16x2[i][8]);
          for (int j = 1; j < 8; j++) {
            acc[m][i][0] = __hfma2(A_fp16x2[m][i][0 + j], B_fp16x2[i][0 + j], acc[m][i][0]);
            acc[m][i][1] = __hfma2(A_fp16x2[m][i][8 + j], B_fp16x2[i][8 + j], acc[m][i][1]);
          }
        }
    
        for (int m = 0; m < BLOCK_M / num_rows; m++)
          for (int i = 0; i < CP_SIZE / 16; i++) {
            __half2_raw scales = SFA_fp16x2[m][i];
            __half_raw group0 = __hadd(acc[m][i][0].x, acc[m][i][0].y);
            __half_raw group1 = __hadd(acc[m][i][1].x, acc[m][i][1].y);
            asm volatile(
            "fma.rn.f32.f16 %0, %1, %2, %0;"
            : "+f"(master_acc[m])
            : "h"(group0.x), "h"(scales.x)
        )
            asm volatile(
            "fma.rn.f32.f16 %0, %1, %2, %0;"
            : "+f"(master_acc[m])
            : "h"(group1.x), "h"(scales.y)
        )
          }""")
    cb.for_end()
    cb.for_end()
    cb.if_begin('NUM_WARPS % 2 == 0', constexpr=True)
        cb.if_begin('num_cols > WARP_SIZE', constexpr=True)
        cb.stmt('__shared__ float smem[BLOCK_M / num_rows][TB_SIZE]')
        cb.for_begin('int m = 0', 'm < BLOCK_M / num_rows', 'm++')
        cb.stmt('smem[m][tid] = master_acc[m]')
    cb.for_end()
        cb.stmt('__syncthreads()')
        cb.for_begin('int stride = num_cols / 2', 'stride >= WARP_SIZE * 2', 'stride /= 2')
        cb.if_begin('t_col < stride')
        cb.stmt('for (int m = 0; m < BLOCK_M / num_rows; m++) {\n            master_acc[m] += smem[m][tid + stride];\n            smem[m][tid] = master_acc[m];\n          }\n        __syncthreads()')
    cb.if_end()
    cb.for_end()
        cb.if_begin('t_col < WARP_SIZE')
        cb.stmt('for (int m = 0; m < BLOCK_M / num_rows; m++)\n          master_acc[m] += smem[m][tid + WARP_SIZE]')
    cb.if_end()
    cb.if_end()
        cb.stmt('constexpr int start_stride = std::min(num_cols, WARP_SIZE) / 2')
        cb.for_begin('int stride = start_stride', 'stride > 0', 'stride /= 2')
        cb.stmt("for (int m = 0; m < BLOCK_M / num_rows; m++)\n        master_acc[m] += __shfl_down_sync(0xFFFF'FFFF, master_acc[m], stride)")
    cb.for_end()
        cb.if_begin('t_col == 0')
        cb.stmt('for (int m = 0; m < BLOCK_M / num_rows; m++)\n        C_ptr[m * num_rows + t_row] = __float2half(master_acc[m])')
    cb.if_end()
    cb.else_begin()
        cb.comment('this is for benchmark.2, when NUM_WARPS = 7')
        cb.stmt('__shared__ float smem[BLOCK_M / num_rows][(NUM_WARPS - 1) * WARP_SIZE]')
        cb.stmt('const int warp_id = tid / WARP_SIZE')
        cb.if_begin('warp_id > 0')
        cb.stmt('for (int m = 0; m < BLOCK_M / num_rows; m++)\n        smem[m][tid - WARP_SIZE] = master_acc[m]')
    cb.if_end()
        cb.stmt('__syncthreads()')
        cb.if_begin('warp_id == 0')
        cb.for_begin('int w = 0', 'w < NUM_WARPS - 1', 'w++')
        cb.stmt('for (int m = 0; m < BLOCK_M; m++)\n          master_acc[m] += smem[m][tid + w * WARP_SIZE]')
    cb.for_end()
        cb.stmt('constexpr int start_stride = std::min(num_cols, WARP_SIZE) / 2')
        cb.for_begin('int stride = start_stride', 'stride > 0', 'stride /= 2')
        cb.stmt("for (int m = 0; m < BLOCK_M / num_rows; m++)\n          master_acc[m] += __shfl_down_sync(0xFFFF'FFFF, master_acc[m], stride)")
    cb.for_end()
        cb.if_begin('t_col == 0')
        cb.stmt('for (int m = 0; m < BLOCK_M / num_rows; m++)\n          C_ptr[m * num_rows + t_row] = __float2half(master_acc[m])')
    cb.if_end()
    cb.if_end()
    cb.if_end()
cb.func_end()
cb.comment("to make our calculations simple, let's treat fp4x2 as a unit.")
cb.comment('hence, K = number of fp4x2 elements, and 8 elements share')
cb.comment('the same scale.')
cb.func_begin("kernel_v2fp", TypeRef("void"), params=[Param("*A_ptr", TypeRef("const char")), Param("[L", TypeRef("//")), Param("M", TypeRef("void")), Param("*B_ptr", TypeRef("K] const char")), Param("[L", TypeRef("//")), Param("128", TypeRef("void")), Param("*SFA_ptr", TypeRef("K] const char")), Param("[L", TypeRef("//")), Param("M", TypeRef("void")), Param("*SFB_ptr", TypeRef("K/8] const char")), Param("[L", TypeRef("//")), Param("128", TypeRef("void")), Param("*C_ptr", TypeRef("K/8] half")), Param("[L", TypeRef("//")), Param("L", TypeRef("M] int")), Param("M", TypeRef("int")), Param("K", TypeRef("int"))], qualifier="__global__", template=['int BLOCK_M', 'int BLOCK_K', 'int TB_WIDTH', 'int NUM_WARPS'], launch_bounds=LaunchBounds('NUM_WARPS * WARP_SIZE'))
    cb.stmt('static_assert(BLOCK_K % 16 == 0)')
    cb.comment('each thread reads 16 bytes')
    cb.stmt('constexpr int TB_SIZE = NUM_WARPS * WARP_SIZE')
    cb.stmt('constexpr int SF_BLOCK_K = BLOCK_K / 8')
    cb.stmt('const int tid = threadIdx.x')
    cb.stmt('const int bid = blockIdx.x')
    cb.stmt('const int batch_id = blockIdx.y')
    cb.stmt('const int lane_id = tid % WARP_SIZE')
    cb.stmt('const int warp_id = tid / WARP_SIZE')
    cb.stmt('int off_m = bid * BLOCK_M')
    cb.stmt('A_ptr += (batch_id * M * K) + off_m * K')
    cb.stmt('B_ptr += (batch_id * 128 * K)')
    cb.stmt('C_ptr += (batch_id * M) + off_m')
    cb.stmt('SFA_ptr += (batch_id *   M * (K / 8)) + off_m * (K / 8)')
    cb.stmt('SFB_ptr += (batch_id * 128 * (K / 8))')
    cb.stmt('constexpr int num_cols = BLOCK_K / 16')
    cb.comment('each thread reads 16-byte at a time')
    cb.stmt('constexpr int TB_HEIGHT = TB_SIZE / TB_WIDTH')
    cb.comment('for gmem->rmem')
    cb.stmt('int A_rmem[BLOCK_M / TB_HEIGHT][num_cols / TB_WIDTH][4]')
    cb.stmt('int B_rmem[num_cols / TB_WIDTH][4]')
    cb.stmt('int16_t SFA_rmem[BLOCK_M / TB_HEIGHT][num_cols / TB_WIDTH]')
    cb.stmt('int16_t SFB_rmem[num_cols / TB_WIDTH]')
    cb.comment('for unpacking to fp16x2')
    cb.stmt('half2 A_fp16x2[BLOCK_M / TB_HEIGHT][num_cols / TB_WIDTH][16]')
    cb.stmt('half2 B_fp16x2[num_cols / TB_WIDTH][16]')
    cb.stmt('half2 SFA_fp16x2[BLOCK_M / TB_HEIGHT][num_cols / TB_WIDTH]')
    cb.stmt('half2 SFB_fp16x2[num_cols / TB_WIDTH]')
    cb.comment('for accumulation')
    cb.stmt('half2 acc[BLOCK_M / TB_HEIGHT][num_cols / TB_WIDTH][2]')
    cb.stmt('float master_acc[BLOCK_M / TB_HEIGHT] = {}')
    cb.stmt('auto gmem_to_rmem = [&]() {\n    for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n      const int col = k * TB_WIDTH + (tid % TB_WIDTH);\n      ldca_i16(SFB_rmem + k, SFB_ptr + (col * 2));\n      ldca_i32x4(B_rmem[k], B_ptr + (col * 16));\n    }\n    for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n      for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n        const int row = m * TB_HEIGHT + (tid / TB_WIDTH);\n        const int col = k * TB_WIDTH + (tid % TB_WIDTH);\n        ldcs_i16(SFA_rmem[m] + k, SFA_ptr + row * (K / 8) + (col * 2));\n        ldcs_i32x4(A_rmem[m][k], A_ptr + row * K + (col * 16));\n      }\n  }')
    cb.stmt('auto unpack = [&]() {\n    for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n      for (int i = 0; i < 4; i++)\n        fp4x8_to_fp16x2x4(reinterpret_cast<int *>(B_fp16x2[k] + i * 4), B_rmem[k][i]);\n      //fp8x2_to_fp16x2(reinterpret_cast<int *>(&SFB_fp16x2[k]), SFB_rmem[k]);\n      SFB_fp16x2[k] = static_cast<half2>(reinterpret_cast<__nv_fp8x2_e4m3 *>(SFB_rmem)[k]);\n    }\n    for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n      for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n        for (int i = 0; i < 4; i++)\n          fp4x8_to_fp16x2x4(reinterpret_cast<int *>(A_fp16x2[m][k] + i * 4), A_rmem[m][k][i]);\n        //fp8x2_to_fp16x2(reinterpret_cast<int *>(&SFA_fp16x2[m][k]), SFA_rmem[m][k]);\n        SFA_fp16x2[m][k] = static_cast<half2>(reinterpret_cast<__nv_fp8x2_e4m3 *>(SFA_rmem[m])[k]);\n        //SFA_fp16x2[m][k] = __hmul2(SFA_fp16x2[m][k], SFB_fp16x2[k]);\n      }\n  }')
    cb.stmt('auto compute = [&]() {\n    for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n      for (int k = 0; k < num_cols / TB_WIDTH; k++)\n        SFA_fp16x2[m][k] = __hmul2(SFA_fp16x2[m][k], SFB_fp16x2[k]);\n\n    for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n      for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n        acc[m][k][0] = __hmul2(A_fp16x2[m][k][0], B_fp16x2[k][0]);  // 1st group\n        acc[m][k][1] = __hmul2(A_fp16x2[m][k][8], B_fp16x2[k][8]);  // 2nd group\n\n        for (int i = 1; i < 8; i++) {\n          acc[m][k][0] = __hfma2(A_fp16x2[m][k][0 + i], B_fp16x2[k][0 + i], acc[m][k][0]);  // 1st group\n          acc[m][k][1] = __hfma2(A_fp16x2[m][k][8 + i], B_fp16x2[k][8 + i], acc[m][k][1]);  // 2nd group\n        }\n      }\n\n    for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n      for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n        __half2_raw scales = SFA_fp16x2[m][k];\n        __half_raw group0 = __hadd(acc[m][k][0].x, acc[m][k][0].y);\n        __half_raw group1 = __hadd(acc[m][k][1].x, acc[m][k][1].y);\n        asm volatile(\n        "fma.rn.f32.f16 %0, %1, %2, %0;"\n        : "+f"(master_acc[m])\n        : "h"(group0.x), "h"(scales.x)\n    )\n        asm volatile(\n        "fma.rn.f32.f16 %0, %1, %2, %0;"\n        : "+f"(master_acc[m])\n        : "h"(group1.x), "h"(scales.y)\n    )\n      }\n  }')
    cb.stmt('const int num_iters = K / BLOCK_K')
    cb.for_begin('int iter_k = 0', 'iter_k < num_iters', 'iter_k++')
        cb.asm_raw(['// start of main loop'], outputs=[], inputs=[], clobbers=[])
        cb.stmt('gmem_to_rmem()')
        cb.stmt('A_ptr += BLOCK_K')
        cb.stmt('B_ptr += BLOCK_K')
        cb.stmt('SFA_ptr += SF_BLOCK_K')
        cb.stmt('SFB_ptr += SF_BLOCK_K')
        cb.stmt('unpack()')
        cb.stmt('compute()')
    cb.for_end()
    cb.stmt("auto final_epilogue = [&]() {\n    constexpr int start_stride = std::min(TB_WIDTH, WARP_SIZE) / 2;\n    for (int stride = start_stride; stride > 0; stride /= 2) {\n      for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++) {\n        float tmp = __shfl_down_sync(0xFFFF'FFFF, master_acc[m], stride);\n        master_acc[m] += tmp;\n      }\n    }\n\n    if (tid % TB_WIDTH == 0) {\n      for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++) {\n        const int row = m * TB_HEIGHT + (tid / TB_WIDTH);\n        C_ptr[row] = __float2half(master_acc[m]);\n      }\n    }\n  }")
    cb.comment('benchmark.0')
    cb.comment("don't think this is faster in a meaningful way, but just for the lolz.")
    cb.if_begin('TB_WIDTH == WARP_SIZE * 2', constexpr=True)
        cb.stmt('__shared__ float smem[BLOCK_M / TB_HEIGHT][NUM_WARPS / 2][WARP_SIZE]')
        cb.if_begin('warp_id % 2 == 1')
        cb.stmt('for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n        smem[m][warp_id / 2][lane_id] = master_acc[m]')
    cb.if_end()
        cb.stmt('__syncthreads()')
        cb.if_begin('warp_id % 2 == 0')
        cb.for_begin('int m = 0', 'm < BLOCK_M / TB_HEIGHT', 'm++')
        cb.stmt('master_acc[m] += smem[m][warp_id / 2][lane_id]')
    cb.for_end()
        cb.stmt('final_epilogue()')
    cb.if_end()
    cb.else_begin()
        cb.if_begin('TB_WIDTH > WARP_SIZE', constexpr=True)
        cb.stmt('__shared__ float smem[BLOCK_M / TB_HEIGHT][TB_SIZE]')
        cb.for_begin('int m = 0', 'm < BLOCK_M / TB_HEIGHT', 'm++')
        cb.stmt('smem[m][tid] = master_acc[m]')
    cb.for_end()
        cb.stmt('__syncthreads()')
        cb.for_begin('int stride = TB_WIDTH / 2', 'stride >= WARP_SIZE', 'stride /= 2')
        cb.if_begin('(tid % TB_WIDTH) < stride')
        cb.for_begin('int m = 0', 'm < BLOCK_M / TB_HEIGHT', 'm++')
        cb.stmt('float tmp = smem[m][tid + stride]')
        cb.stmt('master_acc[m] += tmp')
        cb.stmt('smem[m][tid] = master_acc[m]')
    cb.for_end()
    cb.if_end()
        cb.stmt('__syncthreads()')
    cb.for_end()
    cb.if_end()
        cb.stmt('final_epilogue()')
    cb.if_end()
cb.func_end()
cb.comment("to make our calculations simple, let's treat fp4x2 as a unit.")
cb.comment('hence, K = number of fp4x2 elements, and 8 elements share')
cb.comment('the same scale.')
cb.func_begin("kernel_v2f", TypeRef("void"), params=[Param("*A_ptr", TypeRef("const char")), Param("[L", TypeRef("//")), Param("M", TypeRef("void")), Param("*B_ptr", TypeRef("K] const char")), Param("[L", TypeRef("//")), Param("128", TypeRef("void")), Param("*SFA_ptr", TypeRef("K] const char")), Param("[L", TypeRef("//")), Param("M", TypeRef("void")), Param("*SFB_ptr", TypeRef("K/8] const char")), Param("[L", TypeRef("//")), Param("128", TypeRef("void")), Param("*C_ptr", TypeRef("K/8] half")), Param("[L", TypeRef("//")), Param("L", TypeRef("M] int")), Param("M", TypeRef("int")), Param("K", TypeRef("int"))], qualifier="__global__", template=['int BLOCK_M', 'int BLOCK_K', 'int NUM_WARPS'], launch_bounds=LaunchBounds('NUM_WARPS * WARP_SIZE'))
    cb.stmt('static_assert(BLOCK_K % 16 == 0)')
    cb.comment('each thread reads 16 bytes')
    cb.stmt('constexpr int TB_SIZE = NUM_WARPS * WARP_SIZE')
    cb.stmt('constexpr int SF_BLOCK_K = BLOCK_K / 8')
    cb.stmt('const int tid = threadIdx.x')
    cb.stmt('const int bid = blockIdx.x')
    cb.stmt('const int batch_id = blockIdx.y')
    cb.stmt('const int lane_id = tid % WARP_SIZE')
    cb.stmt('const int warp_id = tid / WARP_SIZE')
    cb.stmt('int off_m = bid * BLOCK_M')
    cb.stmt('A_ptr += (batch_id * M * K) + off_m * K')
    cb.stmt('B_ptr += (batch_id * 128 * K)')
    cb.stmt('C_ptr += (batch_id * M) + off_m')
    cb.stmt('SFA_ptr += (batch_id *   M * (K / 8)) + off_m * (K / 8)')
    cb.stmt('SFB_ptr += (batch_id * 128 * (K / 8))')
    cb.stmt('constexpr int num_cols = BLOCK_K / 16')
    cb.comment('each thread reads 16-byte at a time')
    cb.stmt('constexpr int TB_WIDTH = std::min(num_cols, TB_SIZE)')
    cb.stmt('constexpr int TB_HEIGHT = TB_SIZE / TB_WIDTH')
    cb.comment('for gmem->rmem')
    cb.stmt('int A_rmem[BLOCK_M / TB_HEIGHT][num_cols / TB_WIDTH][4]')
    cb.stmt('int B_rmem[num_cols / TB_WIDTH][4]')
    cb.stmt('int16_t SFA_rmem[BLOCK_M / TB_HEIGHT][num_cols / TB_WIDTH]')
    cb.stmt('int16_t SFB_rmem[num_cols / TB_WIDTH]')
    cb.comment('for unpacking to fp16x2')
    cb.stmt('half2 A_fp16x2[BLOCK_M / TB_HEIGHT][num_cols / TB_WIDTH][16]')
    cb.stmt('half2 B_fp16x2[num_cols / TB_WIDTH][16]')
    cb.stmt('half2 SFA_fp16x2[BLOCK_M / TB_HEIGHT][num_cols / TB_WIDTH]')
    cb.stmt('half2 SFB_fp16x2[num_cols / TB_WIDTH]')
    cb.comment('for accumulation')
    cb.stmt('half2 acc[BLOCK_M / TB_HEIGHT][num_cols / TB_WIDTH][2]')
    cb.stmt('float master_acc[BLOCK_M / TB_HEIGHT] = {}')
    cb.stmt('auto gmem_to_rmem = [&]() {\n    for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n      for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n        const int row = m * TB_HEIGHT + (tid / TB_WIDTH);\n        const int col = k * TB_WIDTH + (tid % TB_WIDTH);\n        ldcs_i32x4(A_rmem[m][k], A_ptr + row * K + (col * 16));\n        ldcs_i16(SFA_rmem[m] + k, SFA_ptr + row * (K / 8) + (col * 2));\n      }\n\n    for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n      const int col = k * TB_WIDTH + (tid % TB_WIDTH);\n      ldca_i32x4(B_rmem[k], B_ptr + (col * 16));\n      ldca_i16(SFB_rmem + k, SFB_ptr + (col * 2));\n    }\n  }')
    cb.stmt('auto unpack = [&]() {\n    for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n      for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n        for (int i = 0; i < 4; i++)\n          fp4x8_to_fp16x2x4(reinterpret_cast<int *>(A_fp16x2[m][k] + i * 4), A_rmem[m][k][i]);\n        fp8x2_to_fp16x2(SFA_fp16x2[m] + k, SFA_rmem[m][k]);\n      }\n\n    for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n      for (int i = 0; i < 4; i++)\n        fp4x8_to_fp16x2x4(reinterpret_cast<int *>(B_fp16x2[k] + i * 4), B_rmem[k][i]);\n      fp8x2_to_fp16x2(SFB_fp16x2 + k, SFB_rmem[k]);\n    }\n  }')
    cb.stmt('auto compute = [&]() {\n    for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n      for (int k = 0; k < num_cols / TB_WIDTH; k++)\n        SFA_fp16x2[m][k] = __hmul2(SFA_fp16x2[m][k], SFB_fp16x2[k]);\n\n    for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n      for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n        acc[m][k][0] = __hmul2(A_fp16x2[m][k][0], B_fp16x2[k][0]);  // 1st group\n        acc[m][k][1] = __hmul2(A_fp16x2[m][k][8], B_fp16x2[k][8]);  // 2nd group\n\n        for (int i = 1; i < 8; i++) {\n          acc[m][k][0] = __hfma2(A_fp16x2[m][k][0 + i], B_fp16x2[k][0 + i], acc[m][k][0]);  // 1st group\n          acc[m][k][1] = __hfma2(A_fp16x2[m][k][8 + i], B_fp16x2[k][8 + i], acc[m][k][1]);  // 2nd group\n        }\n      }\n\n    for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n      for (int k = 0; k < num_cols / TB_WIDTH; k++) {\n        half2 tmp;\n        tmp.x = __hadd(acc[m][k][0].x, acc[m][k][0].y);  // 1st group\n        tmp.y = __hadd(acc[m][k][1].x, acc[m][k][1].y);  // 2nd group\n\n        // apply scaling\n        tmp = __hmul2(tmp, SFA_fp16x2[m][k]);\n\n        // add 2 groups together\n        float2 tmp2 = __half22float2(tmp);\n        master_acc[m] += tmp2.x + tmp2.y;\n        //master_acc[m] += __half2float(tmp.x) + __half2float(tmp.y);\n      }\n  }')
    cb.stmt('const int num_iters = K / BLOCK_K')
    cb.for_begin('int iter_k = 0', 'iter_k < num_iters', 'iter_k++')
        cb.stmt('gmem_to_rmem()')
        cb.stmt('A_ptr += BLOCK_K')
        cb.stmt('B_ptr += BLOCK_K')
        cb.stmt('SFA_ptr += SF_BLOCK_K')
        cb.stmt('SFB_ptr += SF_BLOCK_K')
        cb.stmt('unpack()')
        cb.stmt('compute()')
    cb.for_end()
    cb.stmt("auto final_epilogue = [&]() {\n    constexpr int start_stride = std::min(TB_WIDTH, WARP_SIZE) / 2;\n    for (int stride = start_stride; stride > 0; stride /= 2) {\n      for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++) {\n        master_acc[m] += __shfl_down_sync(0xFFFF'FFFF, master_acc[m], stride);\n      }\n    }\n\n    if (tid % TB_WIDTH == 0) {\n      for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++) {\n        const int row = m * TB_HEIGHT + (tid / TB_WIDTH);\n        C_ptr[row] = __float2half(master_acc[m]);\n      }\n    }\n  }")
    cb.comment('benchmark.0')
    cb.comment("don't think this is faster in a meaningful way, but just for the lolz.")
    cb.if_begin('TB_WIDTH == WARP_SIZE * 2', constexpr=True)
        cb.stmt('__shared__ float smem[BLOCK_M / TB_HEIGHT][NUM_WARPS / 2][WARP_SIZE]')
        cb.if_begin('warp_id % 2 == 1')
        cb.stmt('for (int m = 0; m < BLOCK_M / TB_HEIGHT; m++)\n        smem[m][warp_id / 2][lane_id] = master_acc[m]')
    cb.if_end()
        cb.stmt('__syncthreads()')
        cb.if_begin('warp_id % 2 == 0')
        cb.for_begin('int m = 0', 'm < BLOCK_M / TB_HEIGHT', 'm++')
        cb.stmt('master_acc[m] = my_add(master_acc[m], smem[m][warp_id / 2][lane_id])')
    cb.for_end()
        cb.stmt('final_epilogue()')
    cb.if_end()
    cb.else_begin()
        cb.if_begin('TB_WIDTH > WARP_SIZE', constexpr=True)
        cb.stmt('__shared__ float smem[BLOCK_M / TB_HEIGHT][TB_SIZE]')
        cb.for_begin('int m = 0', 'm < BLOCK_M / TB_HEIGHT', 'm++')
        cb.stmt('smem[m][tid] = master_acc[m]')
    cb.for_end()
        cb.stmt('__syncthreads()')
        cb.for_begin('int stride = TB_WIDTH / 2', 'stride >= WARP_SIZE', 'stride /= 2')
        cb.if_begin('(tid % TB_WIDTH) < stride')
        cb.for_begin('int m = 0', 'm < BLOCK_M / TB_HEIGHT', 'm++')
        cb.stmt('master_acc[m] += smem[m][tid + stride]')
        cb.stmt('smem[m][tid] = master_acc[m]')
    cb.for_end()
    cb.if_end()
        cb.stmt('__syncthreads()')
    cb.for_end()
    cb.if_end()
        cb.stmt('final_epilogue()')
    cb.if_end()
cb.func_end()
cb.func_begin("gemv", TypeRef("void"), params=[Param("A", TypeRef("const at::Tensor&")), Param("B", TypeRef("const at::Tensor&")), Param("SFA", TypeRef("const at::Tensor&")), Param("SFB", TypeRef("const at::Tensor&")), Param("C", TypeRef("at::Tensor&"))])
    cb.stmt('const int M = A.size(0)')
    cb.stmt('const int K = A.size(1)')
    cb.stmt('const int L = A.size(2)')
    cb.stmt('auto A_ptr = reinterpret_cast<const char *>(A.data_ptr())')
    cb.stmt('auto B_ptr = reinterpret_cast<const char *>(B.data_ptr())')
    cb.stmt('auto SFA_ptr = reinterpret_cast<const char *>(SFA.data_ptr())')
    cb.stmt('auto SFB_ptr = reinterpret_cast<const char *>(SFB.data_ptr())')
    cb.stmt('auto C_ptr = reinterpret_cast<half *>(C.data_ptr())')
    cb.define("launch", value='else if (K == K_) {  dim3 grid(M / BLOCK_M, L);  auto this_kernel = kernel_v2h<BLOCK_M, BLOCK_K, K_, NUM_WARPS, CP_SIZE>;  this_kernel<<<grid, NUM_WARPS * WARP_SIZE>>>(A_ptr, B_ptr, SFA_ptr, SFB_ptr, C_ptr, L, M);  }', params=['K_', 'BLOCK_M', 'BLOCK_K', 'NUM_WARPS', 'CP_SIZE'])
    cb.if_begin('false')
    cb.else_begin()
        cb.if_begin('K == 1024')
        cb.stmt('dim3 grid(M / 8, L)')
        cb.stmt('kernel_v2fp<8, 512, 32, 4><<<grid, 4 * WARP_SIZE>>>(A_ptr, B_ptr, SFA_ptr, SFB_ptr, C_ptr, L, M, K)')
        cb.comment('kernel_v2f<8, 512, 4><<<grid, 4 * WARP_SIZE>>>(A_ptr, B_ptr, SFA_ptr, SFB_ptr, C_ptr, L, M, K);')
    cb.if_end()
    cb.if_end()
    cb.raw("""launch(8192, 1, 8192, 8, 32)   // benchmark.0
      launch(3584, 2, 3584, 7, 16)   // benchmark.1
      //launch(3584, 8,  512, 4, 16)   // benchmark.1 - without using 7 warps LMAO
      //launch(1024, 8,  512, 4, 16)   // benchmark.2
      else {
        dim3 grid(M / 32, L);
        kernel_v2fp<32, 128, 8, 4><<<grid, 4 * WARP_SIZE>>>(A_ptr, B_ptr, SFA_ptr, SFB_ptr, C_ptr, L, M, K);
      }
    
    #undef launch""")
cb.func_end()
cb.func_begin("TORCH_LIBRARY", TypeRef(""), params=[Param("my_module, m", TypeRef(""))])
    cb.stmt('m.def("gemv(Tensor A, Tensor B, Tensor SFA, Tensor SFB, Tensor(a!) C) -> ()")')
    cb.stmt('m.impl("gemv", &gemv)')
cb.func_end()

cuda_source = cb.build()
