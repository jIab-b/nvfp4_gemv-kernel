# Builder commands for python source
# Automatically generated - execute to recreate AST/CUDA
# Mode: structured

# === Full CUDA Source ===
from cuda_ast import *
from builder import CudaBuilder, ASTBuilder, reg, imm, vec, mem, sym

cb = CudaBuilder()
cb.include("cuda_fp4.h", system=True)
cb.include("cuda_fp8.h", system=True)
cb.include("cuda_fp16.h", system=True)
cb.include("cuda_pipeline.h", system=True)
cb.include("cuda/ptx", system=True)
cb.include("cuda_awbarrier_primitives.h", system=True)
cb.raw("""namespace ptx = cuda::ptx;""")
cb.define("FULL_MASK", value='0xffffffff')
cb.raw("""__inline__ __device__ void multiply_and_accumulate(
    int4 a_packed,
    int4 b_packed,
    __nv_fp8x2_storage_t sfa_fp8x2,
    __nv_fp8x2_storage_t sfb_fp8x2,
    int* result_0,
    int* result_1,
    int* result_2,
    int* result_3
) {""")
cb.raw("""asm volatile(
        "{\\n.reg .b8 byte0_0, byte0_1, byte0_2, byte0_3;\\n.reg .b8 byte0_4, byte0_5, byte0_6, byte0_7;\\n.reg .b8 byte1_0, byte1_1, byte1_2, byte1_3;\\n.reg .b8 byte1_4, byte1_5, byte1_6, byte1_7;\\n.reg .b8 byte2_0, byte2_1, byte2_2, byte2_3;\\n.reg .b8 byte2_4, byte2_5, byte2_6, byte2_7;\\n.reg .b8 byte3_0, byte3_1, byte3_2, byte3_3;\\n.reg .b8 byte3_4, byte3_5, byte3_6, byte3_7;\\n.reg .f16x2 accum_0_0, accum_0_1, accum_0_2, accum_0_3;\\n.reg .f16x2 accum_1_0, accum_1_1, accum_1_2, accum_1_3;\\n.reg .f16x2 accum_2_0, accum_2_1, accum_2_2, accum_2_3;\\n.reg .f16x2 accum_3_0, accum_3_1, accum_3_2, accum_3_3;\\n.reg .f16x2 sfa_f16x2;\\n.reg .f16x2 sfb_f16x2;\\n.reg .f16x2 sf_f16x2;\\n.reg .f16x2 cvt_0_0, cvt_0_1, cvt_0_2, cvt_0_3;\\n.reg .f16x2 cvt_0_4, cvt_0_5, cvt_0_6, cvt_0_7;\\n.reg .f16x2 cvt_1_0, cvt_1_1, cvt_1_2, cvt_1_3;\\n.reg .f16x2 cvt_1_4, cvt_1_5, cvt_1_6, cvt_1_7;\\n.reg .f16x2 cvt_2_0, cvt_2_1, cvt_2_2, cvt_2_3;\\n.reg .f16x2 cvt_2_4, cvt_2_5, cvt_2_6, cvt_2_7;\\n.reg .f16x2 cvt_3_0, cvt_3_1, cvt_3_2, cvt_3_3;\\n.reg .f16x2 cvt_3_4, cvt_3_5, cvt_3_6, cvt_3_7;\\n.reg .f16 result_f16, lane0, lane1;\\n.reg .f16x2 mul_f16x2_0, mul_f16x2_1;\\ncvt.rn.f16x2.e4m3x2 sfa_f16x2, %4;\\ncvt.rn.f16x2.e4m3x2 sfb_f16x2, %5;\\nmov.b32 accum_0_0, 0;\\nmov.b32 accum_0_1, 0;\\nmov.b32 accum_0_2, 0;\\nmov.b32 accum_0_3, 0;\\nmov.b32 accum_1_0, 0;\\nmov.b32 accum_1_1, 0;\\nmov.b32 accum_1_2, 0;\\nmov.b32 accum_1_3, 0;\\nmov.b32 accum_2_0, 0;\\nmov.b32 accum_2_1, 0;\\nmov.b32 accum_2_2, 0;\\nmov.b32 accum_2_3, 0;\\nmov.b32 accum_3_0, 0;\\nmov.b32 accum_3_1, 0;\\nmov.b32 accum_3_2, 0;\\nmov.b32 accum_3_3, 0;\\nmul.rn.f16x2 sf_f16x2, sfa_f16x2, sfb_f16x2;\\nmov.b32 {lane0, lane1}, sf_f16x2;\\nmov.b32 mul_f16x2_0, {lane0, lane0};\\nmov.b32 mul_f16x2_1, {lane1, lane1};\\nmov.b32 {byte0_0, byte0_1, byte0_2, byte0_3}, %6;\\nmov.b32 {byte0_4, byte0_5, byte0_6, byte0_7}, %7;\\nmov.b32 {byte1_0, byte1_1, byte1_2, byte1_3}, %8;\\nmov.b32 {byte1_4, byte1_5, byte1_6, byte1_7}, %9;\\nmov.b32 {byte2_0, byte2_1, byte2_2, byte2_3}, %10;\\nmov.b32 {byte2_4, byte2_5, byte2_6, byte2_7}, %11;\\nmov.b32 {byte3_0, byte3_1, byte3_2, byte3_3}, %12;\\nmov.b32 {byte3_4, byte3_5, byte3_6, byte3_7}, %13;\\ncvt.rn.f16x2.e2m1x2 cvt_0_0, byte0_0;\\ncvt.rn.f16x2.e2m1x2 cvt_0_1, byte0_1;\\ncvt.rn.f16x2.e2m1x2 cvt_0_2, byte0_2;\\ncvt.rn.f16x2.e2m1x2 cvt_0_3, byte0_3;\\ncvt.rn.f16x2.e2m1x2 cvt_0_4, byte0_4;\\ncvt.rn.f16x2.e2m1x2 cvt_0_5, byte0_5;\\ncvt.rn.f16x2.e2m1x2 cvt_0_6, byte0_6;\\ncvt.rn.f16x2.e2m1x2 cvt_0_7, byte0_7;\\ncvt.rn.f16x2.e2m1x2 cvt_1_0, byte1_0;\\ncvt.rn.f16x2.e2m1x2 cvt_1_1, byte1_1;\\ncvt.rn.f16x2.e2m1x2 cvt_1_2, byte1_2;\\ncvt.rn.f16x2.e2m1x2 cvt_1_3, byte1_3;\\ncvt.rn.f16x2.e2m1x2 cvt_1_4, byte1_4;\\ncvt.rn.f16x2.e2m1x2 cvt_1_5, byte1_5;\\ncvt.rn.f16x2.e2m1x2 cvt_1_6, byte1_6;\\ncvt.rn.f16x2.e2m1x2 cvt_1_7, byte1_7;\\ncvt.rn.f16x2.e2m1x2 cvt_2_0, byte2_0;\\ncvt.rn.f16x2.e2m1x2 cvt_2_1, byte2_1;\\ncvt.rn.f16x2.e2m1x2 cvt_2_2, byte2_2;\\ncvt.rn.f16x2.e2m1x2 cvt_2_3, byte2_3;\\ncvt.rn.f16x2.e2m1x2 cvt_2_4, byte2_4;\\ncvt.rn.f16x2.e2m1x2 cvt_2_5, byte2_5;\\ncvt.rn.f16x2.e2m1x2 cvt_2_6, byte2_6;\\ncvt.rn.f16x2.e2m1x2 cvt_2_7, byte2_7;\\ncvt.rn.f16x2.e2m1x2 cvt_3_0, byte3_0;\\ncvt.rn.f16x2.e2m1x2 cvt_3_1, byte3_1;\\ncvt.rn.f16x2.e2m1x2 cvt_3_2, byte3_2;\\ncvt.rn.f16x2.e2m1x2 cvt_3_3, byte3_3;\\ncvt.rn.f16x2.e2m1x2 cvt_3_4, byte3_4;\\ncvt.rn.f16x2.e2m1x2 cvt_3_5, byte3_5;\\ncvt.rn.f16x2.e2m1x2 cvt_3_6, byte3_6;\\ncvt.rn.f16x2.e2m1x2 cvt_3_7, byte3_7;\\nfma.rn.f16x2 accum_0_0, cvt_0_0, cvt_0_4, accum_0_0;\\nfma.rn.f16x2 accum_0_1, cvt_0_1, cvt_0_5, accum_0_1;\\nfma.rn.f16x2 accum_0_2, cvt_0_2, cvt_0_6, accum_0_2;\\nfma.rn.f16x2 accum_0_3, cvt_0_3, cvt_0_7, accum_0_3;\\nfma.rn.f16x2 accum_1_0, cvt_1_0, cvt_1_4, accum_1_0;\\nfma.rn.f16x2 accum_1_1, cvt_1_1, cvt_1_5, accum_1_1;\\nfma.rn.f16x2 accum_1_2, cvt_1_2, cvt_1_6, accum_1_2;\\nfma.rn.f16x2 accum_1_3, cvt_1_3, cvt_1_7, accum_1_3;\\nfma.rn.f16x2 accum_2_0, cvt_2_0, cvt_2_4, accum_2_0;\\nfma.rn.f16x2 accum_2_1, cvt_2_1, cvt_2_5, accum_2_1;\\nfma.rn.f16x2 accum_2_2, cvt_2_2, cvt_2_6, accum_2_2;\\nfma.rn.f16x2 accum_2_3, cvt_2_3, cvt_2_7, accum_2_3;\\nfma.rn.f16x2 accum_3_0, cvt_3_0, cvt_3_4, accum_3_0;\\nfma.rn.f16x2 accum_3_1, cvt_3_1, cvt_3_5, accum_3_1;\\nfma.rn.f16x2 accum_3_2, cvt_3_2, cvt_3_6, accum_3_2;\\nfma.rn.f16x2 accum_3_3, cvt_3_3, cvt_3_7, accum_3_3;\\nadd.rn.f16x2 accum_0_0, accum_0_0, accum_0_1;\\nadd.rn.f16x2 accum_0_2, accum_0_2, accum_0_3;\\nadd.rn.f16x2 accum_1_0, accum_1_0, accum_1_1;\\nadd.rn.f16x2 accum_1_2, accum_1_2, accum_1_3;\\nadd.rn.f16x2 accum_2_0, accum_2_0, accum_2_1;\\nadd.rn.f16x2 accum_2_2, accum_2_2, accum_2_3;\\nadd.rn.f16x2 accum_3_0, accum_3_0, accum_3_1;\\nadd.rn.f16x2 accum_3_2, accum_3_2, accum_3_3;\\nfma.rn.f16x2 %0, accum_0_0, mul_f16x2_0, %0;\\nfma.rn.f16x2 %1, accum_0_2, mul_f16x2_0, %1;\\nfma.rn.f16x2 %2, accum_1_0, mul_f16x2_0, %2;\\nfma.rn.f16x2 %3, accum_1_2, mul_f16x2_0, %3;\\nfma.rn.f16x2 %0, accum_2_0, mul_f16x2_1, %0;\\nfma.rn.f16x2 %1, accum_2_2, mul_f16x2_1, %1;\\nfma.rn.f16x2 %2, accum_3_0, mul_f16x2_1, %2;\\nfma.rn.f16x2 %3, accum_3_2, mul_f16x2_1, %3;\\n}"
        : "+r"(*result_0), "+r"(*result_1), "+r"(*result_2), "+r"(*result_3)
        : "h"(sfa_fp8x2), "h"(sfb_fp8x2), "r"(a_packed.x), "r"(b_packed.x), "r"(a_packed.y), "r"(b_packed.y), "r"(a_packed.z), "r"(b_packed.z), "r"(a_packed.w), "r"(b_packed.w)
    )
}


__global__ void gemv_kernel_4096_7168(
    const __nv_fp4x2_storage_t* __restrict__ a,
    const __nv_fp4x2_storage_t* __restrict__ b,
    const __nv_fp8_e4m3* __restrict__ sfa,
    const __nv_fp8_e4m3* __restrict__ sfb,
    __half* __restrict__ c
)""")
cb.raw("""{
    const int M = 4096;
    const int K = 7168;

    extern __shared__ unsigned char shared_storage[];
    auto* b_shared = reinterpret_cast<__nv_fp4x2_storage_t*>(shared_storage);
    auto* sfb_shared = reinterpret_cast<__nv_fp8_e4m3*>(b_shared + (K / 2));
    __shared__ __half c_shared[32];

    b += blockIdx.y * (K / 2) * 128;
    sfb += blockIdx.y * (K / 16) * 128;

    for (int i = threadIdx.y * 32 + threadIdx.x; i < K / 32; i += blockDim.y * blockDim.x) {
        reinterpret_cast<int4*>(b_shared)[i] = reinterpret_cast<const int4*>(b)[i];
    }
    for (int i = threadIdx.y * 32 + threadIdx.x; i < K / 256; i += blockDim.y * blockDim.x) {
        reinterpret_cast<int4*>(sfb_shared)[i] = reinterpret_cast<const int4*>(sfb)[i];
    }
    __syncthreads();

    // Each warp computes one result and saves it to shared memory
    int result_0 = 0;
    int result_1 = 0;
    int result_2 = 0;
    int result_3 = 0;
    int offset = blockIdx.y * (K * M / 2) + (blockIdx.x * 32 + threadIdx.y) * (K / 2);
    a += offset;
    sfa += offset / 8;
    
    for (int i = threadIdx.x; i < K / 32; i += 32) {
        int4 a_packed = reinterpret_cast<const int4*>(a)[i];
        int4 b_packed = reinterpret_cast<int4*>(b_shared)[i];
        
        __nv_fp8x2_storage_t sfa_fp8x2 = reinterpret_cast<const __nv_fp8x2_storage_t*>(sfa)[i];
        __nv_fp8x2_storage_t sfb_fp8x2 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfb_shared)[i];

        multiply_and_accumulate(a_packed, b_packed, sfa_fp8x2, sfb_fp8x2, &result_0, &result_1, &result_2, &result_3);
    }


    // Reduce the result and store it in shared memory
    __half2 reduction_result_0 = __hadd2(reinterpret_cast<const __half2&>(result_0),
            reinterpret_cast<const __half2&>(result_1));
    __half2 reduction_result_1 = __hadd2(reinterpret_cast<const __half2&>(result_2),
            reinterpret_cast<const __half2&>(result_3));
    reduction_result_0 = __hadd2(reduction_result_0, reduction_result_1);
    float final_result_f = __half22float2(reduction_result_0).x + __half22float2(reduction_result_0).y;
    for (int offset = 16; offset > 0; offset /= 2) {
        final_result_f += __shfl_down_sync(FULL_MASK, final_result_f, offset);
    }
    if (threadIdx.x == 0) {
        int c_offset = blockIdx.y * M + blockIdx.x * 32 + threadIdx.y;
        c[c_offset] = __float2half_rn(final_result_f);
    }
}""")
cb.func_begin("gemv_kernel_4096_7168_L8", TypeRef("void"), params=[Param("a", TypeRef("const int4* __restrict__")), Param("b", TypeRef("const int4* __restrict__")), Param("sfa", TypeRef("const int* __restrict__")), Param("sfb", TypeRef("const int* __restrict__")), Param("c", TypeRef("__half* __restrict__"))], qualifier="__global__", launch_bounds=LaunchBounds(1024))
    cb.stmt('const int M = 4096')
    cb.stmt('const int K = 7168')
    cb.stmt('const int K_TILES = 7')
    cb.comment('7 tiles of 1024 each')
    cb.stmt('const int TILE_K = 1024')
    cb.comment('elements per tile')
    cb.stmt('const int Q_SIZE = 2')
    cb.comment('Row distribution: 10 blocks handle 228 rows, 8 blocks handle 227 rows')
    cb.comment('10 * 228 + 8 * 227 = 2280 + 1816 = 4096')
    cb.stmt('const int rows_before = (blockIdx.x < 10) ? (blockIdx.x * 228) : (10 * 228 + (blockIdx.x - 10) * 227)')
    cb.stmt('const int rows_in_block = (blockIdx.x < 10) ? 228 : 227')
    cb.comment('Shared memory for B (all 7 tiles loaded upfront)')
    cb.comment('Each tile: 1024/32 = 32 int4s, 1024/16 = 64 fp8s = 16 ints')
    cb.stmt('__shared__ int4 b_shared[K_TILES][32]')
    cb.comment('[tile][lane]')
    cb.stmt('__shared__ int sfb_shared[K_TILES][16]')
    cb.comment('[tile][16 ints = 64 fp8s]')
    cb.comment('Prefetch buffers for A (Q_SIZE + 1 = 3 slots, each warp has its own)')
    cb.stmt('__shared__ int4 a_shared[Q_SIZE + 1][31][32]')
    cb.comment('[buffer][warp][lane]')
    cb.stmt('__shared__ int sfa_shared[Q_SIZE + 1][31][16]')
    cb.comment('[buffer][warp][16 ints]')
    cb.stmt('__shared__ __mbarrier_t bar[K_TILES]')
    cb.if_begin('threadIdx.y == 0 && threadIdx.x == 0')
        cb.for_begin('int i = 0', 'i < K_TILES', 'i++', unroll=True)
        cb.stmt('__mbarrier_init(&bar[i], 32)')
    cb.for_end()
    cb.if_end()
    cb.stmt('__syncthreads()')
    cb.comment('B is laid out as [L, K/2, 128] with 128-padding')
    cb.stmt('b += blockIdx.y * (K / 2) * 128 / 16')
    cb.stmt('sfb += blockIdx.y * (K / 16) * 128 / 4')
    cb.comment('A is laid out as [L, M, K/2]')
    cb.stmt('const int4* a_base = a + blockIdx.y * (M * K / 32)')
    cb.stmt('const int* sfa_base = sfa + blockIdx.y * (M * K / 64)')
    cb.if_begin('threadIdx.y == 0')
        cb.comment('========== WARP 0: Load all B tiles ==========')
        cb.for_begin('int tile = 0', 'tile < K_TILES', 'tile++', unroll=True)
        cb.comment('Each tile: 32 int4s for B, 4 int4s for SFB')
        cb.stmt('__pipeline_memcpy_async(&b_shared[tile][threadIdx.x], \n                                     &b[tile * 32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('tile == K_TILES - 1')
        cb.if_begin('threadIdx.x < 4')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfb_shared[tile])[threadIdx.x], \n                                            &reinterpret_cast<const int4*>(sfb)[tile * 4 + threadIdx.x], sizeof(int4))')
    cb.if_end()
    cb.else_begin()
        cb.if_begin('tile % 2 == 0 && threadIdx.x < 8')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfb_shared[tile])[threadIdx.x], \n                                        &reinterpret_cast<const int4*>(sfb)[tile * 4 + threadIdx.x], sizeof(int4))')
    cb.if_end()
    cb.if_end()
        cb.stmt('__pipeline_arrive_on(&bar[tile])')
        cb.stmt('__mbarrier_arrive(&bar[tile])')
    cb.for_end()
    cb.else_begin()
        cb.comment('========== COMPUTE WARPS (1-31): Process rows in pairs ==========')
        cb.stmt('const int warp_id = threadIdx.y - 1')
        cb.comment('0 to 30')
        cb.comment('Calculate which rows this warp handles')
        cb.stmt('const int base_rows_per_warp = rows_in_block / 31')
        cb.stmt('const int extra_rows = rows_in_block % 31')
        cb.stmt('const int my_rows = base_rows_per_warp + (warp_id < extra_rows ? 1 : 0)')
        cb.stmt('const int my_first_row = (warp_id < extra_rows) ? \n                                  (warp_id * (base_rows_per_warp + 1)) :\n                                  (extra_rows * (base_rows_per_warp + 1) + (warp_id - extra_rows) * base_rows_per_warp)')
        cb.comment('Process rows in pairs (following 7168_16384 pattern)')
        cb.for_begin('int row_pair = 0', 'row_pair < my_rows', 'row_pair += 2')
        cb.stmt('const int local_row0 = row_pair')
        cb.stmt('const int local_row1 = row_pair + 1')
        cb.stmt('const bool has_row1 = (local_row1 < my_rows)')
        cb.stmt('const int global_row0 = rows_before + my_first_row + local_row0')
        cb.stmt('const int global_row1 = has_row1 ? (rows_before + my_first_row + local_row1) : 0')
        cb.stmt('int result[2][4] = {0}')
        cb.comment('Pointers to row data (each row is K/32 = 224 int4s, each tile is 32 int4s)')
        cb.stmt('const int4* a_row0 = a_base + global_row0 * (K / 32)')
        cb.stmt('const int* sfa_row0 = sfa_base + global_row0 * (K / 64)')
        cb.stmt('const int4* a_row1 = has_row1 ? (a_base + global_row1 * (K / 32)) : a_row0')
        cb.stmt('const int* sfa_row1 = has_row1 ? (sfa_base + global_row1 * (K / 64)) : sfa_row0')
        cb.comment('Prologue: prefetch tile 0 for both rows')
        cb.stmt('__pipeline_memcpy_async(&a_shared[0][warp_id][threadIdx.x], \n                                     &a_row0[0 * 32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('threadIdx.x < 4')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfa_shared[0][warp_id])[threadIdx.x], \n                                         &reinterpret_cast<const int4*>(sfa_row0)[0 * 4 + threadIdx.x], sizeof(int4))')
    cb.if_end()
        cb.stmt('__pipeline_commit()')
        cb.if_begin('has_row1')
        cb.stmt('__pipeline_memcpy_async(&a_shared[1][warp_id][threadIdx.x], \n                                         &a_row1[0 * 32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('threadIdx.x < 4')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfa_shared[1][warp_id])[threadIdx.x], \n                                             &reinterpret_cast<const int4*>(sfa_row1)[0 * 4 + threadIdx.x], sizeof(int4))')
    cb.if_end()
    cb.if_end()
        cb.stmt('__pipeline_commit()')
        cb.comment('Main loop: tiles 0 to K_TILES-2')
        cb.for_begin('int tile = 0', 'tile < K_TILES - 1', 'tile++', unroll=True)
        cb.stmt('const int next_tile = tile + 1')
        cb.comment('Prefetch row 0 for next tile')
        cb.stmt('__pipeline_memcpy_async(&a_shared[(tile * 2 + 2) % (Q_SIZE + 1)][warp_id][threadIdx.x], \n                                         &a_row0[next_tile * 32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('threadIdx.x < 4')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfa_shared[(tile * 2 + 2) % (Q_SIZE + 1)][warp_id])[threadIdx.x], \n                                             &reinterpret_cast<const int4*>(sfa_row0)[next_tile * 4 + threadIdx.x], sizeof(int4))')
    cb.if_end()
        cb.stmt('__pipeline_commit()')
        cb.comment('Wait for B[tile]')
        cb.while_begin('!ptx::mbarrier_try_wait_parity(&bar[tile], 0)')
    cb.while_end()
        cb.stmt('int4 b_packed = b_shared[tile][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfb_fp8x2 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfb_shared[tile])[threadIdx.x]')
        cb.comment('Wait for A[tile][row0], compute')
        cb.stmt('__pipeline_wait_prior(Q_SIZE)')
        cb.stmt('int4 a_packed_r0 = a_shared[(tile * 2) % (Q_SIZE + 1)][warp_id][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r0 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(tile * 2) % (Q_SIZE + 1)][warp_id])[threadIdx.x]')
        cb.stmt('multiply_and_accumulate(a_packed_r0, b_packed, sfa_fp8x2_r0, sfb_fp8x2, \n                                        &result[0][0], &result[0][1], &result[0][2], &result[0][3])')
        cb.comment('Prefetch row 1 for next tile')
        cb.if_begin('has_row1')
        cb.stmt('__pipeline_memcpy_async(&a_shared[(tile * 2 + 3) % (Q_SIZE + 1)][warp_id][threadIdx.x], \n                                             &a_row1[next_tile * 32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('threadIdx.x < 4')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfa_shared[(tile * 2 + 3) % (Q_SIZE + 1)][warp_id])[threadIdx.x], \n                                                 &reinterpret_cast<const int4*>(sfa_row1)[next_tile * 4 + threadIdx.x], sizeof(int4))')
    cb.if_end()
    cb.if_end()
        cb.stmt('__pipeline_commit()')
        cb.comment('Wait for A[tile][row1], compute')
        cb.if_begin('has_row1')
        cb.stmt('__pipeline_wait_prior(Q_SIZE)')
        cb.stmt('int4 a_packed_r1 = a_shared[(tile * 2 + 1) % (Q_SIZE + 1)][warp_id][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r1 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(tile * 2 + 1) % (Q_SIZE + 1)][warp_id])[threadIdx.x]')
        cb.stmt('multiply_and_accumulate(a_packed_r1, b_packed, sfa_fp8x2_r1, sfb_fp8x2, \n                                            &result[1][0], &result[1][1], &result[1][2], &result[1][3])')
    cb.if_end()
    cb.for_end()
        cb.comment('Epilogue: last tile (tile = K_TILES - 1)')
        cb.stmt('const int tile = K_TILES - 1')
        cb.while_begin('!ptx::mbarrier_try_wait_parity(&bar[tile], 0)')
    cb.while_end()
        cb.stmt('int4 b_packed = b_shared[tile][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfb_fp8x2 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfb_shared[tile])[threadIdx.x]')
        cb.comment('Row 0')
        cb.stmt('__pipeline_wait_prior(1)')
        cb.stmt('int4 a_packed_r0 = a_shared[(tile * 2) % (Q_SIZE + 1)][warp_id][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r0 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(tile * 2) % (Q_SIZE + 1)][warp_id])[threadIdx.x]')
        cb.stmt('multiply_and_accumulate(a_packed_r0, b_packed, sfa_fp8x2_r0, sfb_fp8x2, \n                                        &result[0][0], &result[0][1], &result[0][2], &result[0][3])')
        cb.comment('Row 1')
        cb.if_begin('has_row1')
        cb.stmt('__pipeline_wait_prior(0)')
        cb.stmt('int4 a_packed_r1 = a_shared[(tile * 2 + 1) % (Q_SIZE + 1)][warp_id][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r1 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(tile * 2 + 1) % (Q_SIZE + 1)][warp_id])[threadIdx.x]')
        cb.stmt('multiply_and_accumulate(a_packed_r1, b_packed, sfa_fp8x2_r1, sfb_fp8x2, \n                                            &result[1][0], &result[1][1], &result[1][2], &result[1][3])')
    cb.if_end()
        cb.comment('Reduction and store for both rows')
        cb.stmt('float final_result_f[2]')
        cb.for_begin('int i = 0', 'i < 2', 'i++', unroll=True)
        cb.stmt('__half2 reduction_result_0 = __hadd2(reinterpret_cast<const __half2&>(result[i][0]),\n                        reinterpret_cast<const __half2&>(result[i][1]))')
        cb.stmt('__half2 reduction_result_1 = __hadd2(reinterpret_cast<const __half2&>(result[i][2]),\n                        reinterpret_cast<const __half2&>(result[i][3]))')
        cb.stmt('reduction_result_0 = __hadd2(reduction_result_0, reduction_result_1)')
        cb.stmt('final_result_f[i] = __half22float2(reduction_result_0).x + __half22float2(reduction_result_0).y')
        cb.for_begin('int offset = 16', 'offset > 0', 'offset /= 2')
        cb.stmt('final_result_f[i] += __shfl_down_sync(FULL_MASK, final_result_f[i], offset)')
    cb.for_end()
    cb.for_end()
        cb.if_begin('threadIdx.x == 0')
        cb.stmt('int c_offset = blockIdx.y * M + global_row0')
        cb.stmt('c[c_offset] = __float2half_rn(final_result_f[0])')
        cb.if_begin('has_row1')
        cb.stmt('c[blockIdx.y * M + global_row1] = __float2half_rn(final_result_f[1])')
    cb.if_end()
    cb.if_end()
    cb.for_end()
    cb.if_end()
cb.func_end()
cb.func_begin("gemv_kernel_7168_2048_L4", TypeRef("void"), params=[Param("a", TypeRef("const int4* __restrict__")), Param("b", TypeRef("const int4* __restrict__")), Param("sfa", TypeRef("const int* __restrict__")), Param("sfb", TypeRef("const int* __restrict__")), Param("c", TypeRef("__half* __restrict__"))], qualifier="__global__", launch_bounds=LaunchBounds(1024))
    cb.stmt('const int M = 7168')
    cb.stmt('const int K = 2048')
    cb.comment('Row distribution: 27 blocks handle 194 rows, 10 blocks handle 193 rows')
    cb.comment('27 * 194 + 10 * 193 = 5238 + 1930 = 7168')
    cb.stmt('const int rows_before = (blockIdx.x < 27) ? (blockIdx.x * 194) : (27 * 194 + (blockIdx.x - 27) * 193)')
    cb.stmt('const int rows_in_block = (blockIdx.x < 27) ? 194 : 193')
    cb.comment('Shared memory for B and SFB (loaded once, used by all warps)')
    cb.comment('K/32 = 64 int4s for B, K/16 = 128 fp8s = 32 ints for SFB')
    cb.stmt('__shared__ int4 b_shared[2][32]')
    cb.comment('[half][lane]')
    cb.stmt('__shared__ int sfb_shared[32]')
    cb.comment('128 fp8s as 32 ints')
    cb.comment('Prefetch buffers for A (each warp has its own buffer, double buffered)')
    cb.comment('31 compute warps (warps 1-31)')
    cb.stmt('__shared__ int4 a_shared[2][31][2][32]')
    cb.comment('[buffer][warp][half][lane]')
    cb.stmt('__shared__ int sfa_shared[2][31][32]')
    cb.comment('[buffer][warp][32 ints = 64 fp8x2]')
    cb.stmt('__shared__ __mbarrier_t bar')
    cb.if_begin('threadIdx.y == 0 && threadIdx.x == 0')
        cb.stmt('__mbarrier_init(&bar, 32)')
    cb.if_end()
    cb.stmt('__syncthreads()')
    cb.comment('B is laid out as [L, K/2, 128] with 128-padding')
    cb.comment('Adjust pointers for this L slice (blockIdx.y)')
    cb.comment('Stride in L dimension: (K/2) * 128 bytes = K * 64 bytes')
    cb.comment('As int4 (16 bytes): stride = K * 4')
    cb.stmt('b += blockIdx.y * (K / 2) * 128 / 16')
    cb.comment('SFB stride: (K/16) * 128 bytes = K * 8 bytes')
    cb.comment('As int (4 bytes): stride = K * 2')
    cb.stmt('sfb += blockIdx.y * (K / 16) * 128 / 4')
    cb.comment('A is laid out as [L, M, K/2]')
    cb.comment('Stride in L dimension: M * K/2 bytes')
    cb.comment('As int4: stride = M * K / 32')
    cb.stmt('const int4* a_base = a + blockIdx.y * (M * K / 32)')
    cb.stmt('const int* sfa_base = sfa + blockIdx.y * (M * K / 64)')
    cb.if_begin('threadIdx.y == 0')
        cb.comment('========== WARP 0: Load B and SFB (one-shot, no loop) ==========')
        cb.comment('K/32 = 64 int4s, 32 threads each load 2')
        cb.stmt('__pipeline_memcpy_async(&b_shared[0][threadIdx.x], &b[threadIdx.x], sizeof(int4))')
        cb.stmt('__pipeline_memcpy_async(&b_shared[1][threadIdx.x], &b[32 + threadIdx.x], sizeof(int4))')
        cb.comment('K/256 = 8 int4s for SFB')
        cb.if_begin('threadIdx.x < 8')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfb_shared)[threadIdx.x], \n                                     &reinterpret_cast<const int4*>(sfb)[threadIdx.x], sizeof(int4))')
    cb.if_end()
        cb.stmt('__pipeline_arrive_on(&bar)')
        cb.stmt('__mbarrier_arrive(&bar)')
    cb.else_begin()
        cb.comment('========== COMPUTE WARPS (warps 1-31): Load A/SFA and compute ==========')
        cb.stmt('const int warp_id = threadIdx.y - 1')
        cb.comment('0 to 30')
        cb.comment('Prologue: prefetch first row for this warp')
        cb.stmt('int row = warp_id')
        cb.if_begin('row < rows_in_block')
        cb.stmt('int global_row = rows_before + row')
        cb.stmt('const int4* a_row = a_base + global_row * (K / 32)')
        cb.comment('K/32 int4s per row')
        cb.stmt('const int* sfa_row = sfa_base + global_row * (K / 64)')
        cb.comment('K/64 ints per row')
        cb.stmt('__pipeline_memcpy_async(&a_shared[0][warp_id][0][threadIdx.x], &a_row[threadIdx.x], sizeof(int4))')
        cb.stmt('__pipeline_memcpy_async(&a_shared[0][warp_id][1][threadIdx.x], &a_row[32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('threadIdx.x < 8')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfa_shared[0][warp_id])[threadIdx.x], \n                                         &reinterpret_cast<const int4*>(sfa_row)[threadIdx.x], sizeof(int4))')
    cb.if_end()
    cb.if_end()
        cb.stmt('__pipeline_commit()')
        cb.comment('Wait for B to be ready')
        cb.while_begin('!ptx::mbarrier_try_wait_parity(&bar, 0)')
    cb.while_end()
        cb.comment('Load B and SFB from shared to registers (reused for all rows)')
        cb.stmt('int4 b_packed_0 = b_shared[0][threadIdx.x]')
        cb.stmt('int4 b_packed_1 = b_shared[1][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfb_fp8x2_0 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfb_shared)[threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfb_fp8x2_1 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfb_shared)[threadIdx.x + 32]')
        cb.comment('Main loop: iterate over assigned rows with stride 31')
        cb.for_begin('int row = warp_id', 'row < rows_in_block', 'row += 31')
        cb.stmt('int next_row = row + 31')
        cb.stmt('int buf = (row / 31) & 1')
        cb.stmt('int next_buf = 1 - buf')
        cb.comment('Prefetch next row if exists')
        cb.if_begin('next_row < rows_in_block')
        cb.stmt('int global_next_row = rows_before + next_row')
        cb.stmt('const int4* a_row = a_base + global_next_row * (K / 32)')
        cb.stmt('const int* sfa_row = sfa_base + global_next_row * (K / 64)')
        cb.stmt('__pipeline_memcpy_async(&a_shared[next_buf][warp_id][0][threadIdx.x], &a_row[threadIdx.x], sizeof(int4))')
        cb.stmt('__pipeline_memcpy_async(&a_shared[next_buf][warp_id][1][threadIdx.x], &a_row[32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('threadIdx.x < 8')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfa_shared[next_buf][warp_id])[threadIdx.x], \n                                             &reinterpret_cast<const int4*>(sfa_row)[threadIdx.x], sizeof(int4))')
    cb.if_end()
    cb.if_end()
        cb.stmt('__pipeline_commit()')
        cb.comment("Wait for current row's A data")
        cb.stmt('__pipeline_wait_prior(1)')
        cb.comment('Load A from shared')
        cb.stmt('int4 a_packed_0 = a_shared[buf][warp_id][0][threadIdx.x]')
        cb.stmt('int4 a_packed_1 = a_shared[buf][warp_id][1][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_0 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[buf][warp_id])[threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_1 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[buf][warp_id])[threadIdx.x + 32]')
        cb.comment('Compute: multiply_and_accumulate for both halves')
        cb.stmt('int result[4] = {0, 0, 0, 0}')
        cb.stmt('multiply_and_accumulate(a_packed_0, b_packed_0, sfa_fp8x2_0, sfb_fp8x2_0, \n                                    &result[0], &result[1], &result[2], &result[3])')
        cb.stmt('multiply_and_accumulate(a_packed_1, b_packed_1, sfa_fp8x2_1, sfb_fp8x2_1, \n                                    &result[0], &result[1], &result[2], &result[3])')
        cb.comment('Reduce and store')
        cb.stmt('__half2 reduction_result_0 = __hadd2(reinterpret_cast<const __half2&>(result[0]),\n                    reinterpret_cast<const __half2&>(result[1]))')
        cb.stmt('__half2 reduction_result_1 = __hadd2(reinterpret_cast<const __half2&>(result[2]),\n                    reinterpret_cast<const __half2&>(result[3]))')
        cb.stmt('reduction_result_0 = __hadd2(reduction_result_0, reduction_result_1)')
        cb.stmt('float final_result_f = __half22float2(reduction_result_0).x + __half22float2(reduction_result_0).y')
        cb.for_begin('int offset = 16', 'offset > 0', 'offset /= 2', unroll=True)
        cb.stmt('final_result_f += __shfl_down_sync(FULL_MASK, final_result_f, offset)')
    cb.for_end()
        cb.if_begin('threadIdx.x == 0')
        cb.stmt('int global_row = rows_before + row')
        cb.stmt('int c_offset = blockIdx.y * M + global_row')
        cb.stmt('c[c_offset] = __float2half_rn(final_result_f)')
    cb.if_end()
    cb.for_end()
    cb.if_end()
cb.func_end()
cb.func_begin("gemv_kernel_7168_16384", TypeRef("void"), params=[Param("a", TypeRef("const int4* __restrict__")), Param("b", TypeRef("const int4* __restrict__")), Param("sfa", TypeRef("const int* __restrict__")), Param("sfb", TypeRef("const int* __restrict__")), Param("c", TypeRef("__half* __restrict__"))], qualifier="__global__", launch_bounds=LaunchBounds(832))
    cb.stmt('const int M = 7168')
    cb.stmt('const int K = 16384')
    cb.stmt('const int Q_SIZE = 2')
    cb.stmt('const int active_warps = (blockIdx.x < 32) ? 26 : 25')
    cb.stmt('__shared__ int4 a_shared[Q_SIZE + 1][25][2][32]')
    cb.stmt('__shared__ int sfa_shared[Q_SIZE + 1][25][32]')
    cb.comment('We will load all b and sfb, because we can, it simplifies the logic')
    cb.stmt('__shared__ int4 b_shared[8][2][32]')
    cb.stmt('__shared__ int sfb_shared[8][32]')
    cb.stmt('__shared__ __mbarrier_t bar[8]')
    cb.if_begin('threadIdx.y == 0 && threadIdx.x == 0')
        cb.for_begin('int i = 0', 'i < 8', 'i++', unroll=True)
        cb.stmt('__mbarrier_init(&bar[i], 32)')
    cb.for_end()
    cb.if_end()
    cb.stmt('__syncthreads()')
    cb.if_begin('threadIdx.y == 0')
        cb.comment('========== WARP 0: Load b and sfb for all columns ==========')
        cb.for_begin('int col_idx = 0', 'col_idx < 8', 'col_idx++', unroll=True)
        cb.stmt('__pipeline_memcpy_async(&b_shared[col_idx][0][threadIdx.x], &b[col_idx * 64 + threadIdx.x], sizeof(int4))')
        cb.stmt('__pipeline_memcpy_async(&b_shared[col_idx][1][threadIdx.x], &b[col_idx * 64 + 32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('threadIdx.x < 8')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfb_shared[col_idx])[threadIdx.x], &reinterpret_cast<const int4*>(sfb)[col_idx * 8 + threadIdx.x], sizeof(int4))')
    cb.if_end()
        cb.stmt('__pipeline_arrive_on(&bar[col_idx])')
        cb.stmt('__mbarrier_arrive(&bar[col_idx])')
    cb.for_end()
    cb.else_begin()
        cb.if_begin('threadIdx.y < active_warps')
        cb.comment('========== COMPUTE WARPS: Load a/sfa and compute ==========')
        cb.stmt('int offset = (blockIdx.x * 24 + min(blockIdx.x, 32) + threadIdx.y - 1) * 2 * (K / 2)')
        cb.stmt('a += offset / 16')
        cb.stmt('sfa += offset / 32')
        cb.stmt('int result[2][4] = {0}')
        cb.comment('Prologue: prefetch col 0 (both rows)')
        cb.stmt('__pipeline_memcpy_async(&a_shared[0][threadIdx.y - 1][0][threadIdx.x], &a[0 * (K / 32) + 0 * 64 + threadIdx.x], sizeof(int4))')
        cb.stmt('__pipeline_memcpy_async(&a_shared[0][threadIdx.y - 1][1][threadIdx.x], &a[0 * (K / 32) + 0 * 64 + 32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('threadIdx.x < 8')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfa_shared[0][threadIdx.y - 1])[threadIdx.x], &reinterpret_cast<const int4*>(sfa)[0 * (K / 256) + 0 * 8 + threadIdx.x], sizeof(int4))')
    cb.if_end()
        cb.stmt('__pipeline_commit()')
        cb.stmt('__pipeline_memcpy_async(&a_shared[1][threadIdx.y - 1][0][threadIdx.x], &a[1 * (K / 32) + 0 * 64 + threadIdx.x], sizeof(int4))')
        cb.stmt('__pipeline_memcpy_async(&a_shared[1][threadIdx.y - 1][1][threadIdx.x], &a[1 * (K / 32) + 0 * 64 + 32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('threadIdx.x < 8')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfa_shared[1][threadIdx.y - 1])[threadIdx.x], &reinterpret_cast<const int4*>(sfa)[1 * (K / 256) + 0 * 8 + threadIdx.x], sizeof(int4))')
    cb.if_end()
        cb.stmt('__pipeline_commit()')
        cb.comment('Main loop: process columns 0-6, prefetch next column')
        cb.for_begin('int col_idx = 0', 'col_idx < 7', 'col_idx++', unroll=True)
        cb.stmt('int next_col = col_idx + 1')
        cb.comment('Prefetch row 0 for next column')
        cb.stmt('__pipeline_memcpy_async(&a_shared[(col_idx * 2 + 2) % (Q_SIZE + 1)][threadIdx.y - 1][0][threadIdx.x], &a[0 * (K / 32) + next_col * 64 + threadIdx.x], sizeof(int4))')
        cb.stmt('__pipeline_memcpy_async(&a_shared[(col_idx * 2 + 2) % (Q_SIZE + 1)][threadIdx.y - 1][1][threadIdx.x], &a[0 * (K / 32) + next_col * 64 + 32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('threadIdx.x < 8')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfa_shared[(col_idx * 2 + 2) % (Q_SIZE + 1)][threadIdx.y - 1])[threadIdx.x], &reinterpret_cast<const int4*>(sfa)[0 * (K / 256) + next_col * 8 + threadIdx.x], sizeof(int4))')
    cb.if_end()
        cb.stmt('__pipeline_commit()')
        cb.comment('Wait for b/sfb data, load once for this column')
        cb.while_begin('!ptx::mbarrier_try_wait_parity(&bar[col_idx], 0)')
    cb.while_end()
        cb.stmt('int4 b_packed_0 = b_shared[col_idx][0][threadIdx.x]')
        cb.stmt('int4 b_packed_1 = b_shared[col_idx][1][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfb_fp8x2_0 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfb_shared[col_idx])[threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfb_fp8x2_1 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfb_shared[col_idx])[threadIdx.x + 32]')
        cb.comment('Load and compute row 0')
        cb.stmt('__pipeline_wait_prior(Q_SIZE)')
        cb.stmt('int4 a_packed_r0_0 = a_shared[(col_idx * 2) % (Q_SIZE + 1)][threadIdx.y - 1][0][threadIdx.x]')
        cb.stmt('int4 a_packed_r0_1 = a_shared[(col_idx * 2) % (Q_SIZE + 1)][threadIdx.y - 1][1][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r0_0 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(col_idx * 2) % (Q_SIZE + 1)][threadIdx.y - 1])[threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r0_1 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(col_idx * 2) % (Q_SIZE + 1)][threadIdx.y - 1])[threadIdx.x + 32]')
        cb.stmt('multiply_and_accumulate(a_packed_r0_0, b_packed_0, sfa_fp8x2_r0_0, sfb_fp8x2_0, &result[0][0], &result[0][1], &result[0][2], &result[0][3])')
        cb.stmt('multiply_and_accumulate(a_packed_r0_1, b_packed_1, sfa_fp8x2_r0_1, sfb_fp8x2_1, &result[0][0], &result[0][1], &result[0][2], &result[0][3])')
        cb.comment('Prefetch row 1 for next column')
        cb.stmt('__pipeline_memcpy_async(&a_shared[(col_idx * 2 + 3) % (Q_SIZE + 1)][threadIdx.y - 1][0][threadIdx.x], &a[1 * (K / 32) + next_col * 64 + threadIdx.x], sizeof(int4))')
        cb.stmt('__pipeline_memcpy_async(&a_shared[(col_idx * 2 + 3) % (Q_SIZE + 1)][threadIdx.y - 1][1][threadIdx.x], &a[1 * (K / 32) + next_col * 64 + 32 + threadIdx.x], sizeof(int4))')
        cb.if_begin('threadIdx.x < 8')
        cb.stmt('__pipeline_memcpy_async(&reinterpret_cast<int4*>(sfa_shared[(col_idx * 2 + 3) % (Q_SIZE + 1)][threadIdx.y - 1])[threadIdx.x], &reinterpret_cast<const int4*>(sfa)[1 * (K / 256) + next_col * 8 + threadIdx.x], sizeof(int4))')
    cb.if_end()
        cb.stmt('__pipeline_commit()')
        cb.comment('Load and compute row 1')
        cb.stmt('__pipeline_wait_prior(Q_SIZE)')
        cb.stmt('int4 a_packed_r1_0 = a_shared[(col_idx * 2 + 1) % (Q_SIZE + 1)][threadIdx.y - 1][0][threadIdx.x]')
        cb.stmt('int4 a_packed_r1_1 = a_shared[(col_idx * 2 + 1) % (Q_SIZE + 1)][threadIdx.y - 1][1][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r1_0 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(col_idx * 2 + 1) % (Q_SIZE + 1)][threadIdx.y - 1])[threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r1_1 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(col_idx * 2 + 1) % (Q_SIZE + 1)][threadIdx.y - 1])[threadIdx.x + 32]')
        cb.stmt('multiply_and_accumulate(a_packed_r1_0, b_packed_0, sfa_fp8x2_r1_0, sfb_fp8x2_0, &result[1][0], &result[1][1], &result[1][2], &result[1][3])')
        cb.stmt('multiply_and_accumulate(a_packed_r1_1, b_packed_1, sfa_fp8x2_r1_1, sfb_fp8x2_1, &result[1][0], &result[1][1], &result[1][2], &result[1][3])')
    cb.for_end()
        cb.comment('Epilogue: process last column (col_idx = 7)')
        cb.stmt('const int col_idx = 7')
        cb.comment('Wait for b/sfb data, load once')
        cb.while_begin('!ptx::mbarrier_try_wait_parity(&bar[col_idx], 0)')
    cb.while_end()
        cb.stmt('int4 b_packed_0 = b_shared[col_idx][0][threadIdx.x]')
        cb.stmt('int4 b_packed_1 = b_shared[col_idx][1][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfb_fp8x2_0 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfb_shared[col_idx])[threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfb_fp8x2_1 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfb_shared[col_idx])[threadIdx.x + 32]')
        cb.comment('Load and compute row 0')
        cb.stmt('__pipeline_wait_prior(1)')
        cb.stmt('int4 a_packed_r0_0 = a_shared[(col_idx * 2) % (Q_SIZE + 1)][threadIdx.y - 1][0][threadIdx.x]')
        cb.stmt('int4 a_packed_r0_1 = a_shared[(col_idx * 2) % (Q_SIZE + 1)][threadIdx.y - 1][1][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r0_0 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(col_idx * 2) % (Q_SIZE + 1)][threadIdx.y - 1])[threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r0_1 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(col_idx * 2) % (Q_SIZE + 1)][threadIdx.y - 1])[threadIdx.x + 32]')
        cb.stmt('multiply_and_accumulate(a_packed_r0_0, b_packed_0, sfa_fp8x2_r0_0, sfb_fp8x2_0, &result[0][0], &result[0][1], &result[0][2], &result[0][3])')
        cb.stmt('multiply_and_accumulate(a_packed_r0_1, b_packed_1, sfa_fp8x2_r0_1, sfb_fp8x2_1, &result[0][0], &result[0][1], &result[0][2], &result[0][3])')
        cb.comment('Load and compute row 1')
        cb.stmt('__pipeline_wait_prior(0)')
        cb.stmt('int4 a_packed_r1_0 = a_shared[(col_idx * 2 + 1) % (Q_SIZE + 1)][threadIdx.y - 1][0][threadIdx.x]')
        cb.stmt('int4 a_packed_r1_1 = a_shared[(col_idx * 2 + 1) % (Q_SIZE + 1)][threadIdx.y - 1][1][threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r1_0 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(col_idx * 2 + 1) % (Q_SIZE + 1)][threadIdx.y - 1])[threadIdx.x]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2_r1_1 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfa_shared[(col_idx * 2 + 1) % (Q_SIZE + 1)][threadIdx.y - 1])[threadIdx.x + 32]')
        cb.stmt('multiply_and_accumulate(a_packed_r1_0, b_packed_0, sfa_fp8x2_r1_0, sfb_fp8x2_0, &result[1][0], &result[1][1], &result[1][2], &result[1][3])')
        cb.stmt('multiply_and_accumulate(a_packed_r1_1, b_packed_1, sfa_fp8x2_r1_1, sfb_fp8x2_1, &result[1][0], &result[1][1], &result[1][2], &result[1][3])')
        cb.comment('Reduction and store')
        cb.stmt('float final_result_f[2]')
        cb.for_begin('int i = 0', 'i < 2', 'i++', unroll=True)
        cb.stmt('__half2 reduction_result_0 = __hadd2(reinterpret_cast<const __half2&>(result[i][0]),\n                    reinterpret_cast<const __half2&>(result[i][1]))')
        cb.stmt('__half2 reduction_result_1 = __hadd2(reinterpret_cast<const __half2&>(result[i][2]),\n                    reinterpret_cast<const __half2&>(result[i][3]))')
        cb.stmt('reduction_result_0 = __hadd2(reduction_result_0, reduction_result_1)')
        cb.stmt('final_result_f[i] = __half22float2(reduction_result_0).x + __half22float2(reduction_result_0).y')
        cb.for_begin('int offset = 16', 'offset > 0', 'offset /= 2')
        cb.stmt('final_result_f[i] += __shfl_down_sync(FULL_MASK, final_result_f[i], offset)')
    cb.for_end()
    cb.for_end()
        cb.if_begin('threadIdx.x == 0')
        cb.stmt('__half final_result[2]')
        cb.stmt('final_result[0] = __float2half_rn(final_result_f[0])')
        cb.stmt('final_result[1] = __float2half_rn(final_result_f[1])')
        cb.stmt('int c_offset = (blockIdx.x * 24 + min((int)blockIdx.x, 32) + threadIdx.y - 1)')
        cb.stmt('reinterpret_cast<int*>(c)[c_offset] = reinterpret_cast<int&>(final_result)')
    cb.if_end()
    cb.if_end()
    cb.if_end()
cb.func_end()
cb.func_begin("gemv_kernel", TypeRef("void"), params=[Param("a", TypeRef("const __nv_fp4x2_storage_t* __restrict__")), Param("b", TypeRef("const __nv_fp4x2_storage_t* __restrict__")), Param("sfa", TypeRef("const __nv_fp8_e4m3* __restrict__")), Param("sfb", TypeRef("const __nv_fp8_e4m3* __restrict__")), Param("c", TypeRef("__half* __restrict__")), Param("M", TypeRef("int")), Param("K", TypeRef("int"))], qualifier="__global__")
    cb.stmt('extern __shared__ unsigned char shared_storage[]')
    cb.stmt('auto* b_shared = reinterpret_cast<__nv_fp4x2_storage_t*>(shared_storage)')
    cb.stmt('auto* sfb_shared = reinterpret_cast<__nv_fp8_e4m3*>(b_shared + (K / 2))')
    cb.stmt('__shared__ __half c_shared[32]')
    cb.stmt('b += blockIdx.y * (K / 2) * 128')
    cb.stmt('sfb += blockIdx.y * (K / 16) * 128')
    cb.for_begin('int i = threadIdx.y * 32 + threadIdx.x', 'i < K / 32', 'i += blockDim.y * blockDim.x')
        cb.stmt('reinterpret_cast<int4*>(b_shared)[i] = reinterpret_cast<const int4*>(b)[i]')
    cb.for_end()
    cb.for_begin('int i = threadIdx.y * 32 + threadIdx.x', 'i < K / 256', 'i += blockDim.y * blockDim.x')
        cb.stmt('reinterpret_cast<int4*>(sfb_shared)[i] = reinterpret_cast<const int4*>(sfb)[i]')
    cb.for_end()
    cb.stmt('__syncthreads()')
    cb.comment('Each warp computes one result and saves it to shared memory')
    cb.stmt('int result_0 = 0')
    cb.stmt('int result_1 = 0')
    cb.stmt('int result_2 = 0')
    cb.stmt('int result_3 = 0')
    cb.stmt('int offset = blockIdx.y * (K * M / 2) + (blockIdx.x * 32 + threadIdx.y) * (K / 2)')
    cb.stmt('a += offset')
    cb.stmt('sfa += offset / 8')
    cb.for_begin('int i = threadIdx.x', 'i < K / 32', 'i += 32')
        cb.stmt('int4 a_packed = reinterpret_cast<const int4*>(a)[i]')
        cb.stmt('int4 b_packed = reinterpret_cast<int4*>(b_shared)[i]')
        cb.stmt('__nv_fp8x2_storage_t sfa_fp8x2 = reinterpret_cast<const __nv_fp8x2_storage_t*>(sfa)[i]')
        cb.stmt('__nv_fp8x2_storage_t sfb_fp8x2 = reinterpret_cast<__nv_fp8x2_storage_t*>(sfb_shared)[i]')
        cb.stmt('multiply_and_accumulate(a_packed, b_packed, sfa_fp8x2, sfb_fp8x2, &result_0, &result_1, &result_2, &result_3)')
    cb.for_end()
    cb.comment('Reduce the result and store it in shared memory')
    cb.stmt('__half2 reduction_result_0 = __hadd2(reinterpret_cast<const __half2&>(result_0),\n            reinterpret_cast<const __half2&>(result_1))')
    cb.stmt('__half2 reduction_result_1 = __hadd2(reinterpret_cast<const __half2&>(result_2),\n            reinterpret_cast<const __half2&>(result_3))')
    cb.stmt('reduction_result_0 = __hadd2(reduction_result_0, reduction_result_1)')
    cb.stmt('float final_result_f = __half22float2(reduction_result_0).x + __half22float2(reduction_result_0).y')
    cb.for_begin('int offset = 16', 'offset > 0', 'offset /= 2')
        cb.stmt('final_result_f += __shfl_down_sync(FULL_MASK, final_result_f, offset)')
    cb.for_end()
    cb.if_begin('threadIdx.x == 0')
        cb.stmt('c_shared[threadIdx.y] = __float2half_rn(final_result_f)')
    cb.if_end()
    cb.stmt('__syncthreads()')
    cb.comment('Write the result to global memory')
    cb.if_begin('threadIdx.y == 0')
        cb.stmt('int c_offset = blockIdx.y * M + blockIdx.x * 32 + threadIdx.x')
        cb.stmt('c[c_offset] = c_shared[threadIdx.x]')
    cb.if_end()
cb.func_end()
cb.func_begin("gemv_cuda", TypeRef("torch::Tensor"), params=[Param("a", TypeRef("torch::Tensor")), Param("b", TypeRef("torch::Tensor")), Param("sfa", TypeRef("torch::Tensor")), Param("sfb", TypeRef("torch::Tensor")), Param("c", TypeRef("torch::Tensor"))])
    cb.stmt('const int64_t M = a.size(0)')
    cb.stmt('const int64_t K = a.size(1) * 2')
    cb.stmt('const int64_t L = a.size(2)')
    cb.stmt('dim3 block_dim(32, 32, 1)')
    cb.stmt('dim3 grid_dim(M / 32, L, 1)')
    cb.stmt('const auto* a_ptr = reinterpret_cast<const __nv_fp4x2_storage_t*>(a.data_ptr())')
    cb.stmt('const auto* b_ptr = reinterpret_cast<const __nv_fp4x2_storage_t*>(b.data_ptr())')
    cb.stmt('const auto* sfa_ptr = reinterpret_cast<const __nv_fp8_e4m3*>(sfa.data_ptr())')
    cb.stmt('const auto* sfb_ptr = reinterpret_cast<const __nv_fp8_e4m3*>(sfb.data_ptr())')
    cb.stmt('auto* c_ptr = reinterpret_cast<__half*>(c.data_ptr<c10::Half>())')
    cb.stmt('size_t shared_mem_bytes =\n        (static_cast<size_t>(K) / 2) * sizeof(__nv_fp4x2_storage_t) +\n        (static_cast<size_t>(K) / 16) * sizeof(__nv_fp8_e4m3)')
    cb.if_begin('M == 4096 && K == 7168 && L == 8')
        cb.stmt('grid_dim = dim3(18, 8, 1)')
        cb.stmt('block_dim = dim3(32, 32, 1)')
        cb.stmt('gemv_kernel_4096_7168_L8<<<grid_dim, block_dim>>>(\n            reinterpret_cast<const int4*>(a.data_ptr()),\n            reinterpret_cast<const int4*>(b.data_ptr()),\n            reinterpret_cast<const int*>(sfa.data_ptr()),\n            reinterpret_cast<const int*>(sfb.data_ptr()),\n            c_ptr\n        )')
    cb.else_begin()
        cb.if_begin('M == 4096 && K == 7168')
        cb.stmt('gemv_kernel_4096_7168<<<grid_dim, block_dim, shared_mem_bytes>>>(\n            a_ptr,\n            b_ptr,\n            sfa_ptr,\n            sfb_ptr,\n            c_ptr\n        )')
    cb.else_begin()
        cb.if_begin('M == 7168 && K == 2048 && L == 4')
        cb.stmt('grid_dim = dim3(37, 4, 1)')
        cb.stmt('block_dim = dim3(32, 32, 1)')
        cb.stmt('gemv_kernel_7168_2048_L4<<<grid_dim, block_dim>>>(\n            reinterpret_cast<const int4*>(a.data_ptr()),\n            reinterpret_cast<const int4*>(b.data_ptr()),\n            reinterpret_cast<const int*>(sfa.data_ptr()),\n            reinterpret_cast<const int*>(sfb.data_ptr()),\n            c_ptr\n        )')
    cb.else_begin()
        cb.if_begin('M == 7168 && K == 16384')
        cb.stmt('grid_dim = dim3(148, 1, 1)')
        cb.stmt('block_dim = dim3(32, 26, 1)')
        cb.stmt('gemv_kernel_7168_16384<<<grid_dim, block_dim>>>(\n            reinterpret_cast<const int4*>(a.data_ptr()),\n            reinterpret_cast<const int4*>(b.data_ptr()),\n            reinterpret_cast<const int*>(sfa.data_ptr()),\n            reinterpret_cast<const int*>(sfb.data_ptr()),\n            c_ptr\n        )')
    cb.else_begin()
        cb.stmt('gemv_kernel<<<grid_dim, block_dim, shared_mem_bytes>>>(\n            a_ptr,\n            b_ptr,\n            sfa_ptr,\n            sfb_ptr,\n            c_ptr,\n            static_cast<int>(M),\n            static_cast<int>(K)\n        )')
    cb.if_end()
    cb.if_end()
    cb.if_end()
    cb.if_end()
    cb.ret('c')
cb.func_end()

cuda_source = cb.build()
