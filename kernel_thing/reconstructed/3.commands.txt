# Builder commands for python source
# Automatically generated - execute to recreate AST/CUDA
# Mode: structured

# === Full CUDA Source ===
from cuda_ast import *
from builder import CudaBuilder, ASTBuilder, reg, imm, vec, mem, sym

cb = CudaBuilder()
cb.include("cuda.h", system=True)
cb.include("cuda_runtime.h", system=True)
cb.include("torch/extension.h", system=True)
cb.include("cuda_fp4.h", system=True)
cb.include("cuda_fp8.h", system=True)
cb.include("cuda_fp16.h", system=True)
cb.constexpr("kWarpGroupSize", TypeRef("int"), '32', storage="static")
cb.func_begin("decode_fp4_pack_to_half2x4", TypeRef("void"), params=[Param("*dst_halves", TypeRef("int")), Param("in", TypeRef("int"))], qualifier="__device__", forceinline=True)
    cb.asm_raw(['{', '.reg .b8 tmp0, tmp1, tmp2, tmp3;', 'mov.b32 {tmp0, tmp1, tmp2, tmp3}, %4;', 'cvt.rn.f16x2.e2m1x2 %0, tmp0;', 'cvt.rn.f16x2.e2m1x2 %1, tmp1;', 'cvt.rn.f16x2.e2m1x2 %2, tmp2;', 'cvt.rn.f16x2.e2m1x2 %3, tmp3;', '}'], outputs=[('=r', 'dst_halves[0]'), ('=r', 'dst_halves[1]'), ('=r', 'dst_halves[2]'), ('=r', 'dst_halves[3]')], inputs=[('r', 'in')], clobbers=[])
cb.func_end()
cb.func_begin("decode_fp8_pair_to_half2", TypeRef("void"), params=[Param("*dst_pair", TypeRef("int")), Param("in", TypeRef("int16_t"))], qualifier="__device__", forceinline=True)
    cb.asm_raw(['cvt.rn.f16x2.e4m3x2 %0, %1;'], outputs=[('=r', 'dst_pair[0]')], inputs=[('h', 'in')], clobbers=[])
cb.func_end()
cb.func_begin("warp_group_reduce_sum", TypeRef("float"), params=[Param("lane_accum", TypeRef("float"))], qualifier="__device__", forceinline=True, template=['int WIDTH'])
    cb.if_begin('WIDTH > 1', constexpr=True)
        cb.for_begin('int shuffle_delta = WIDTH / 2', 'shuffle_delta > 0', 'shuffle_delta /= 2', unroll=True)
        cb.stmt('lane_accum += __shfl_down_sync(0xffffffffu, lane_accum, shuffle_delta, WIDTH)')
    cb.for_end()
    cb.if_end()
    cb.ret('lane_accum')
cb.func_end()
cb.func_begin("load_fp4_noalloc_u64x2", TypeRef("void"), params=[Param("(&dst)[2]", TypeRef("uint64_t")), Param("addr", TypeRef("uint64_t"))], qualifier="__device__", forceinline=True)
    cb.asm_raw(['ld.global.nc.L1::no_allocate.v2.u64 {%0, %1}, [%2];'], outputs=[('=l', 'dst[0]'), ('=l', 'dst[1]')], inputs=[('l', 'addr')], clobbers=[])
cb.func_end()
cb.func_begin("load_fp4_cache_u64x2", TypeRef("void"), params=[Param("(&dst)[2]", TypeRef("uint64_t")), Param("addr", TypeRef("uint64_t"))], qualifier="__device__", forceinline=True)
    cb.asm_raw(['ld.global.nc.L1::evict_last.v2.u64 {%0, %1}, [%2];'], outputs=[('=l', 'dst[0]'), ('=l', 'dst[1]')], inputs=[('l', 'addr')], clobbers=[])
cb.func_end()
cb.func_begin("load_scale_noalloc_u16", TypeRef("void"), params=[Param("&dst", TypeRef("uint16_t")), Param("addr", TypeRef("uint64_t"))], qualifier="__device__", forceinline=True)
    cb.asm_raw(['ld.global.nc.L1::no_allocate.u16 %0, [%1];'], outputs=[('=h', 'dst')], inputs=[('l', 'addr')], clobbers=[])
cb.func_end()
cb.func_begin("load_scale_cache_u16", TypeRef("void"), params=[Param("&dst", TypeRef("uint16_t")), Param("addr", TypeRef("uint64_t"))], qualifier="__device__", forceinline=True)
    cb.asm_raw(['ld.global.nc.L1::evict_last.u16 %0, [%1];'], outputs=[('=h', 'dst')], inputs=[('l', 'addr')], clobbers=[])
cb.func_end()
cb.func_begin("fetch_fp4_operand_group", TypeRef("void"), params=[Param("ptr_A_row", TypeRef("const __nv_fp4x2_e2m1*")), Param("ptr_B_panel", TypeRef("const __nv_fp4x2_e2m1*")), Param("block_tile_offset", TypeRef("int")), Param("(&matrix_frag)[2]", TypeRef("uint64_t")), Param("(&vector_frag)[2]", TypeRef("uint64_t"))], qualifier="__device__", forceinline=True)
    cb.stmt('int tile_elem_base = block_tile_offset * 16')
    cb.stmt('uint64_t ptr_A_row_addr = reinterpret_cast<uint64_t>(ptr_A_row + tile_elem_base)')
    cb.stmt('uint64_t ptr_B_panel_addr = reinterpret_cast<uint64_t>(ptr_B_panel + tile_elem_base)')
    cb.stmt('load_fp4_noalloc_u64x2(matrix_frag, ptr_A_row_addr)')
    cb.stmt('load_fp4_cache_u64x2(vector_frag, ptr_B_panel_addr)')
cb.func_end()
cb.func_begin("fetch_blockscale_pair", TypeRef("void"), params=[Param("ptr_SFA_u16", TypeRef("const uint16_t*")), Param("ptr_SFB_u16", TypeRef("const uint16_t*")), Param("block_tile_offset", TypeRef("int")), Param("&sfa_scale_regs", TypeRef("uint16_t")), Param("&sfb_scale_regs", TypeRef("uint16_t"))], qualifier="__device__", forceinline=True)
    cb.stmt('uint64_t sfa_addr = reinterpret_cast<uint64_t>(ptr_SFA_u16 + block_tile_offset)')
    cb.stmt('uint64_t sfb_addr = reinterpret_cast<uint64_t>(ptr_SFB_u16 + block_tile_offset)')
    cb.stmt('load_scale_noalloc_u16(sfa_scale_regs, sfa_addr)')
    cb.stmt('load_scale_cache_u16(sfb_scale_regs, sfb_addr)')
cb.func_end()
cb.func_begin("accumulate_blockscaled_fp4", TypeRef("float"), params=[Param("(&matrix_frag)[2]", TypeRef("const uint64_t")), Param("(&vector_frag)[2]", TypeRef("const uint64_t")), Param("sfa_scale_regs", TypeRef("uint16_t")), Param("sfb_scale_regs", TypeRef("uint16_t"))], qualifier="__device__", forceinline=True)
    cb.stmt('const int* matrix_frag_packed = reinterpret_cast<const int*>(&matrix_frag[0])')
    cb.stmt('const int* vector_frag_packed = reinterpret_cast<const int*>(&vector_frag[0])')
    cb.stmt('half2 row_scale_fp16x2')
    cb.stmt('half2 vec_scale_fp16x2')
    cb.stmt('decode_fp8_pair_to_half2(reinterpret_cast<int*>(&row_scale_fp16x2), static_cast<int16_t>(sfa_scale_regs))')
    cb.stmt('decode_fp8_pair_to_half2(reinterpret_cast<int*>(&vec_scale_fp16x2), static_cast<int16_t>(sfb_scale_regs))')
    cb.stmt('half2 fused_scales = __hmul2(row_scale_fp16x2, vec_scale_fp16x2)')
    cb.stmt('half2 acc_group0 = __float2half2_rn(0.f)')
    cb.stmt('half2 acc_group1 = __float2half2_rn(0.f)')
    cb.for_begin('int pack = 0', 'pack < 4', '++pack', unroll=True)
        cb.stmt('half2 a_chunk[4]')
        cb.stmt('half2 b_chunk[4]')
        cb.stmt('decode_fp4_pack_to_half2x4(reinterpret_cast<int*>(a_chunk), matrix_frag_packed[pack])')
        cb.stmt('decode_fp4_pack_to_half2x4(reinterpret_cast<int*>(b_chunk), vector_frag_packed[pack])')
        cb.for_begin('int j = 0', 'j < 4', '++j', unroll=True)
        cb.stmt('int tile_iter_idx = pack * 4 + j')
        cb.if_begin('tile_iter_idx < 8')
        cb.stmt('acc_group0 = __hfma2(a_chunk[j], b_chunk[j], acc_group0)')
    cb.else_begin()
        cb.stmt('acc_group1 = __hfma2(a_chunk[j], b_chunk[j], acc_group1)')
    cb.if_end()
    cb.for_end()
    cb.for_end()
    cb.stmt('__half group0 = __hadd(__low2half(acc_group0), __high2half(acc_group0))')
    cb.stmt('__half group1 = __hadd(__low2half(acc_group1), __high2half(acc_group1))')
    cb.stmt('__half scale_half0 = __low2half(fused_scales)')
    cb.stmt('__half scale_half1 = __high2half(fused_scales)')
    cb.stmt('float scaled_dot = __half2float(__hmul(group0, scale_half0)) +\n                       __half2float(__hmul(group1, scale_half1))')
    cb.ret('scaled_dot')
cb.func_end()
cb.struct_begin("BlockscaledParams")
    cb.using("stride_index_t", "uint64_t")
    cb.field(Field("k_extent", TypeRef("int")))
    cb.field(Field("runtime_tile_iters", TypeRef("int")))
    cb.field(Field("ptr_A", TypeRef("void *__restrict__")))
    cb.field(Field("ptr_B", TypeRef("void *__restrict__")))
    cb.field(Field("ptr_SFA", TypeRef("void *__restrict__")))
    cb.field(Field("ptr_SFB", TypeRef("void *__restrict__")))
    cb.field(Field("ptr_C", TypeRef("void *__restrict__")))
    cb.field(Field("batch_stride_A", TypeRef("stride_index_t")))
    cb.field(Field("batch_stride_B", TypeRef("stride_index_t")))
    cb.field(Field("batch_stride_SFA", TypeRef("stride_index_t")))
    cb.field(Field("batch_stride_SFB", TypeRef("stride_index_t")))
    cb.field(Field("batch_stride_C", TypeRef("stride_index_t")))
    cb.field(Field("stride_A", TypeRef("stride_index_t")))
    cb.field(Field("stride_SFA", TypeRef("stride_index_t")))
cb.struct_end()
cb.struct_begin("RowPointers")
    cb.field(Field("ptr_A_row", TypeRef("const __nv_fp4x2_e2m1*")))
    cb.field(Field("ptr_B_panel", TypeRef("const __nv_fp4x2_e2m1*")))
    cb.field(Field("ptr_SFA_u16", TypeRef("const uint16_t*")))
    cb.field(Field("ptr_SFB_u16", TypeRef("const uint16_t*")))
    cb.field(Field("batch_offset_C", TypeRef("size_t")))
cb.struct_end()
cb.func_begin("compose_row_pointers", TypeRef("RowPointers"), params=[Param("kernel_params", TypeRef("const BlockscaledParams&")), Param("batch_idx", TypeRef("int")), Param("row_idx", TypeRef("int"))], qualifier="__device__", forceinline=True)
    cb.stmt('const size_t batch_offset_A   = static_cast<size_t>(batch_idx) * kernel_params.batch_stride_A')
    cb.stmt('const size_t batch_offset_SFA = static_cast<size_t>(batch_idx) * kernel_params.batch_stride_SFA')
    cb.stmt('const size_t batch_offset_B   = static_cast<size_t>(batch_idx) * kernel_params.batch_stride_B')
    cb.stmt('const size_t batch_offset_SFB = static_cast<size_t>(batch_idx) * kernel_params.batch_stride_SFB')
    cb.stmt('const size_t batch_offset_C   = static_cast<size_t>(batch_idx) * kernel_params.batch_stride_C')
    cb.stmt('RowPointers row_ptrs{}')
    cb.stmt('row_ptrs.ptr_A_row = static_cast<const __nv_fp4x2_e2m1*>(kernel_params.ptr_A) + batch_offset_A + row_idx * kernel_params.stride_A')
    cb.stmt('const __nv_fp8_e4m3* sfa_vec = static_cast<const __nv_fp8_e4m3*>(kernel_params.ptr_SFA) + batch_offset_SFA + row_idx * kernel_params.stride_SFA')
    cb.stmt('row_ptrs.ptr_B_panel = static_cast<const __nv_fp4x2_e2m1*>(kernel_params.ptr_B) + batch_offset_B')
    cb.stmt('const __nv_fp8_e4m3* sfb_vec = static_cast<const __nv_fp8_e4m3*>(kernel_params.ptr_SFB) + batch_offset_SFB')
    cb.stmt('row_ptrs.ptr_SFA_u16 = reinterpret_cast<const uint16_t*>(sfa_vec)')
    cb.stmt('row_ptrs.ptr_SFB_u16 = reinterpret_cast<const uint16_t*>(sfb_vec)')
    cb.stmt('row_ptrs.batch_offset_C = batch_offset_C')
    cb.ret('row_ptrs')
cb.func_end()
cb.func_begin("BlockscaledCooperativeKernel", TypeRef("void"), params=[Param("kernel_params", TypeRef("const __grid_constant__ BlockscaledParams"))], qualifier="__global__", template=['int CTA_ROWS', 'int LANES_PER_STRIPE', 'int STATIC_TILE_ITERS'], launch_bounds=LaunchBounds('CTA_ROWS * LANES_PER_STRIPE', 8))
    cb.stmt('const int thread_linear = threadIdx.x')
    cb.stmt('const int row_in_block  = thread_linear / LANES_PER_STRIPE')
    cb.stmt('const int lane_idx      = thread_linear % LANES_PER_STRIPE')
    cb.stmt('const int batch_idx     = blockIdx.z')
    cb.stmt('const int row_idx       = blockIdx.x * CTA_ROWS + row_in_block')
    cb.stmt('const RowPointers row_ptrs = compose_row_pointers(kernel_params, batch_idx, row_idx)')
    cb.stmt('float thread_accum = 0.f')
    cb.for_begin('int tile_iter_idx = 0', 'tile_iter_idx < STATIC_TILE_ITERS', '++tile_iter_idx', unroll=True)
        cb.stmt('int block_tile_offset = tile_iter_idx * LANES_PER_STRIPE + lane_idx')
        cb.stmt('uint64_t matrix_frag[2], vector_frag[2]')
        cb.stmt('uint16_t sfa_scale_regs, sfb_scale_regs')
        cb.stmt('fetch_fp4_operand_group(\n            row_ptrs.ptr_A_row, row_ptrs.ptr_B_panel,\n            block_tile_offset,\n            matrix_frag, vector_frag)')
        cb.stmt('fetch_blockscale_pair(\n            row_ptrs.ptr_SFA_u16, row_ptrs.ptr_SFB_u16,\n            block_tile_offset,\n            sfa_scale_regs, sfb_scale_regs)')
        cb.stmt('thread_accum += accumulate_blockscaled_fp4(matrix_frag, vector_frag, sfa_scale_regs, sfb_scale_regs)')
    cb.for_end()
    cb.if_begin('LANES_PER_STRIPE <= kWarpGroupSize', constexpr=True)
        cb.stmt('thread_accum = warp_group_reduce_sum<LANES_PER_STRIPE>(thread_accum)')
        cb.if_begin('lane_idx == 0')
        cb.stmt('__half* row_output = (__half*)kernel_params.ptr_C + row_ptrs.batch_offset_C + row_idx')
        cb.stmt('row_output[0] = __float2half(thread_accum)')
    cb.if_end()
    cb.else_begin()
        cb.stmt('static_assert((LANES_PER_STRIPE % kWarpGroupSize) == 0,\n                      "LANES_PER_STRIPE must be a multiple of kWarpGroupSize for BlockscaledCooperativeKernel")')
        cb.stmt('__shared__ float tile_partial_storage[CTA_ROWS * LANES_PER_STRIPE]')
        cb.stmt('float* row_slice = tile_partial_storage + row_in_block * LANES_PER_STRIPE')
        cb.stmt('row_slice[lane_idx] = thread_accum')
        cb.stmt('__syncthreads()')
        cb.if_begin('lane_idx < kWarpGroupSize')
        cb.stmt('float lane_accum = row_slice[lane_idx]')
        cb.for_begin('int shuffle_delta = kWarpGroupSize', 'shuffle_delta < LANES_PER_STRIPE', 'shuffle_delta += kWarpGroupSize', unroll=True)
        cb.stmt('lane_accum += row_slice[lane_idx + shuffle_delta]')
    cb.for_end()
        cb.stmt('lane_accum = warp_group_reduce_sum<kWarpGroupSize>(lane_accum)')
        cb.if_begin('lane_idx == 0')
        cb.stmt('__half* row_output = (__half*)kernel_params.ptr_C + row_ptrs.batch_offset_C + row_idx')
        cb.stmt('row_output[0] = __float2half(lane_accum)')
    cb.if_end()
    cb.if_end()
    cb.if_end()
cb.func_end()
cb.func_begin("BlockscaledWarpKernel", TypeRef("void"), params=[Param("kernel_params", TypeRef("const __grid_constant__ BlockscaledParams"))], qualifier="__global__", template=['int CTA_ROWS', 'int LANES_PER_STRIPE', 'int STATIC_TILE_ITERS'], launch_bounds=LaunchBounds('CTA_ROWS * LANES_PER_STRIPE', 8))
    cb.stmt('static_assert(LANES_PER_STRIPE <= kWarpGroupSize,\n                  "BlockscaledWarpKernel expects LANES_PER_STRIPE to fit within a single warp")')
    cb.stmt('const int thread_linear = threadIdx.x')
    cb.stmt('const int row_in_block  = thread_linear / LANES_PER_STRIPE')
    cb.stmt('const int lane_idx      = thread_linear % LANES_PER_STRIPE')
    cb.stmt('const int batch_idx     = blockIdx.z')
    cb.stmt('const int row_idx       = blockIdx.x * CTA_ROWS + row_in_block')
    cb.stmt('const RowPointers row_ptrs = compose_row_pointers(kernel_params, batch_idx, row_idx)')
    cb.stmt('float thread_accum = 0.f')
    cb.stmt('auto tile_body = [&](int tile_iter_idx) {\n        int block_tile_offset = tile_iter_idx * LANES_PER_STRIPE + lane_idx;\n\n        uint64_t matrix_frag[2], vector_frag[2];\n        uint16_t sfa_scale_regs, sfb_scale_regs;\n\n        fetch_fp4_operand_group(\n            row_ptrs.ptr_A_row, row_ptrs.ptr_B_panel,\n            block_tile_offset,\n            matrix_frag, vector_frag);\n        fetch_blockscale_pair(\n            row_ptrs.ptr_SFA_u16, row_ptrs.ptr_SFB_u16,\n            block_tile_offset,\n            sfa_scale_regs, sfb_scale_regs);\n        thread_accum += accumulate_blockscaled_fp4(matrix_frag, vector_frag, sfa_scale_regs, sfb_scale_regs);\n    }')
    cb.if_begin('STATIC_TILE_ITERS > 0', constexpr=True)
        cb.for_begin('int tile_iter_idx = 0', 'tile_iter_idx < STATIC_TILE_ITERS', '++tile_iter_idx', unroll=True)
        cb.stmt('tile_body(tile_iter_idx)')
    cb.for_end()
    cb.else_begin()
        cb.stmt('int loop_iters = kernel_params.runtime_tile_iters')
        cb.if_begin('loop_iters == 0')
        cb.stmt('loop_iters = kernel_params.k_extent / (LANES_PER_STRIPE * 16)')
    cb.if_end()
        cb.for_begin('int tile_iter_idx = 0', 'tile_iter_idx < loop_iters', '++tile_iter_idx')
        cb.stmt('tile_body(tile_iter_idx)')
    cb.for_end()
    cb.if_end()
    cb.stmt('thread_accum = warp_group_reduce_sum<LANES_PER_STRIPE>(thread_accum)')
    cb.if_begin('lane_idx == 0')
        cb.stmt('__half* row_output = (__half*)kernel_params.ptr_C + row_ptrs.batch_offset_C + row_idx')
        cb.stmt('row_output[0] = __float2half(thread_accum)')
    cb.if_end()
cb.func_end()
cb.func_begin("launch_blockscaled_warp", TypeRef("void"), params=[Param("m_extent", TypeRef("int")), Param("batch_count", TypeRef("int")), Param("kernel_params", TypeRef("const BlockscaledParams&"))], inline=True, template=['int CTA_ROWS', 'int LANES_PER_STRIPE', 'int STATIC_TILE_ITERS'])
    cb.stmt('dim3 cta_shape(CTA_ROWS * LANES_PER_STRIPE, 1, 1)')
    cb.stmt('dim3 grid_shape(m_extent / CTA_ROWS, 1, batch_count)')
    cb.stmt('BlockscaledWarpKernel<CTA_ROWS, LANES_PER_STRIPE, STATIC_TILE_ITERS><<<grid_shape, cta_shape, 0, 0>>>(kernel_params)')
cb.func_end()
cb.func_begin("launch_blockscaled_cooperative", TypeRef("void"), params=[Param("m_extent", TypeRef("int")), Param("batch_count", TypeRef("int")), Param("kernel_params", TypeRef("const BlockscaledParams&"))], inline=True, template=['int CTA_ROWS', 'int LANES_PER_STRIPE', 'int STATIC_TILE_ITERS'])
    cb.stmt('dim3 cta_shape(CTA_ROWS * LANES_PER_STRIPE, 1, 1)')
    cb.stmt('dim3 grid_shape(m_extent / CTA_ROWS, 1, batch_count)')
    cb.stmt('BlockscaledCooperativeKernel<CTA_ROWS, LANES_PER_STRIPE, STATIC_TILE_ITERS><<<grid_shape, cta_shape, 0, 0>>>(kernel_params)')
cb.func_end()
cb.func_begin("select_and_launch_tile", TypeRef("void"), params=[Param("m_extent", TypeRef("int")), Param("batch_count", TypeRef("int")), Param("kernel_params", TypeRef("BlockscaledParams&"))], inline=True, template=['int CTA_ROWS', 'int LANES_PER_STRIPE', 'int STATIC_TILE_ITERS', 'bool USE_COOPERATIVE'])
    cb.if_begin('STATIC_TILE_ITERS == 0', constexpr=True)
        cb.stmt('kernel_params.runtime_tile_iters = kernel_params.k_extent / (LANES_PER_STRIPE * 16)')
    cb.else_begin()
        cb.stmt('kernel_params.runtime_tile_iters = 0')
    cb.if_end()
    cb.if_begin('USE_COOPERATIVE', constexpr=True)
        cb.stmt('launch_blockscaled_cooperative<CTA_ROWS, LANES_PER_STRIPE, STATIC_TILE_ITERS>(m_extent, batch_count, kernel_params)')
    cb.else_begin()
        cb.stmt('launch_blockscaled_warp<CTA_ROWS, LANES_PER_STRIPE, STATIC_TILE_ITERS>(m_extent, batch_count, kernel_params)')
    cb.if_end()
cb.func_end()
cb.func_begin("launch_tile", TypeRef("void"), params=[Param("m_extent", TypeRef("int")), Param("batch_count", TypeRef("int")), Param("kernel_params", TypeRef("BlockscaledParams&"))], inline=True, template=['int CTA_ROWS', 'int LANES_PER_STRIPE', 'int STATIC_TILE_ITERS', 'bool USE_COOPERATIVE'])
    cb.stmt('select_and_launch_tile<CTA_ROWS, LANES_PER_STRIPE, STATIC_TILE_ITERS, USE_COOPERATIVE>(\n        m_extent, batch_count, kernel_params)')
cb.func_end()
cb.using("TileLauncherFn", "void (*)(int, int, BlockscaledParams&)")
cb.struct_begin("TileDispatchEntry")
    cb.field(Field("k_extent", TypeRef("int")))
    cb.field(Field("required_m_multiple", TypeRef("int")))
    cb.raw('// 0 -> no requirement')
    cb.field(Field("launch", TypeRef("TileLauncherFn")))
cb.struct_end()
cb.func_begin("make_tile_case", TypeRef("TileDispatchEntry"), params=[Param("k_extent", TypeRef("int")), Param("required_m_multiple", TypeRef("int"), default='CTA_ROWS')], template=['int CTA_ROWS', 'int LANES_PER_STRIPE', 'int STATIC_TILE_ITERS', 'bool USE_COOPERATIVE'])
    cb.ret('TileDispatchEntry{\n        k_extent,\n        required_m_multiple,\n        launch_tile<CTA_ROWS, LANES_PER_STRIPE, STATIC_TILE_ITERS, USE_COOPERATIVE>}')
cb.func_end()
cb.constexpr("kGenericTileLauncher", TypeRef("TileLauncherFn"), 'launch_tile<8, 16, 0, false>')
cb.constexpr("kSpecialTileCases", TypeRef("TileDispatchEntry[]"), '{\n    make_tile_case<4, 32, 7, false>(3584, 0),\n    make_tile_case<1, 128, 8192 / (128 * 16), true>(8192, 0),\n    make_tile_case<1, 32, 4, false>(2048),\n    make_tile_case<64, 16, 28, false>(7168),\n    make_tile_case<2, 16, 64, false>(16384),\n    make_tile_case<8, 16, 4, false>(1024, 0),\n}')
cb.func_begin("gemv", TypeRef("torch::Tensor"), params=[Param("A", TypeRef("torch::Tensor")), Param("B", TypeRef("torch::Tensor")), Param("SFA", TypeRef("torch::Tensor")), Param("SFB", TypeRef("torch::Tensor")), Param("C", TypeRef("torch::Tensor"))])
    cb.stmt('const auto problem_size = A.sizes()')
    cb.stmt('const int m_extent = problem_size[0]')
    cb.stmt('const int k_extent = problem_size[1]')
    cb.stmt('const int batch_count = problem_size[2]')
    cb.stmt('BlockscaledParams kernel_params{}')
    cb.stmt('kernel_params.k_extent = k_extent')
    cb.stmt('kernel_params.runtime_tile_iters = 0')
    cb.stmt('kernel_params.ptr_A = A.data_ptr()')
    cb.stmt('kernel_params.ptr_B = B.data_ptr()')
    cb.stmt('kernel_params.ptr_SFA = SFA.data_ptr()')
    cb.stmt('kernel_params.ptr_SFB = SFB.data_ptr()')
    cb.stmt('kernel_params.ptr_C = C.data_ptr()')
    cb.stmt('kernel_params.batch_stride_A = A.stride(2)')
    cb.stmt('kernel_params.batch_stride_B = B.stride(2)')
    cb.stmt('kernel_params.batch_stride_SFA = SFA.stride(2)')
    cb.stmt('kernel_params.batch_stride_SFB = SFB.stride(2)')
    cb.stmt('kernel_params.batch_stride_C = C.stride(2)')
    cb.stmt('kernel_params.stride_A = A.stride(0)')
    cb.stmt('kernel_params.stride_SFA = SFA.stride(0)')
    cb.if_begin('kernel_params.k_extent <= 256')
        cb.stmt('launch_tile<16, 8, 0, false>(m_extent, batch_count, kernel_params)')
        cb.ret('C')
    cb.if_end()
    cb.for_begin('const auto& tile_case : kSpecialTileCases', '', '')
        cb.if_begin('kernel_params.k_extent == tile_case.k_extent')
        cb.stmt('const bool m_match = (tile_case.required_m_multiple == 0) ||\n                                 ((m_extent % tile_case.required_m_multiple) == 0)')
        cb.if_begin('m_match')
        cb.stmt('tile_case.launch(m_extent, batch_count, kernel_params)')
        cb.ret('C')
    cb.if_end()
    cb.if_end()
    cb.for_end()
    cb.stmt('kGenericTileLauncher(m_extent, batch_count, kernel_params)')
    cb.ret('C')
cb.func_end()

cuda_source = cb.build()
