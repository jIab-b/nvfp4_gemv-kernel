 9.7.16.1. Tensor Memory

The 5th generation TensorCore has dedicated on-chip memory that is specialized for use by TensorCore operations. This Tensor Memory is organized as a two-dimensional matrix where the horizontal rows are called lanes and the vertical columns are called columns.

On architecture sm_100a/sm_100f, the 5th generation TensorCore’s Tensor Memory has a two-dimensional structure of 512 columns and 128 rows per CTA, with each cell being 32-bits in size.

Restrictions on threads accessing the Tensor Memory via the load and store operations are specified in Access restrictions.
9.7.16.1.1. Tensor Memory Addressing

Tensor Memory addresses are 32-bit wide and specify two components.

    Lane index

    Column index

The layout is as follows:

    31 16
    	

    15 0

    Lane index
    	

    Column index

Figure 182 shows the view of the Tensor Memory Layout within CTA.
_images/tensor-memory-layout.png

Figure 182 Tensor Memory Layout and Addressing
9.7.16.1.2. Tensor Memory Allocation

The Tensor Memory is dynamically allocated. The Tensor Memory must be allocated by a single warp in a CTA using the Tensor Memory Allocation and Management Instructions.

The allocation and deallocation of Tensor Memory is performed in terms of columns. The unit of allocation is 32 columns and the number of columns being allocated must be a power of 2. When a column is allocated, all 128 lanes of the column are allocated.

All of the Tensor Memory that was allocated in a kernel, must be explicitly deallocated before the kernel exits.
9.7.16.2. Matrix and Data Movement Shape

There are two kinds of shapes involved.

    Shapes in the data movement operations

    Shapes in the MMA operations

9.7.16.2.1. Matrix Shape

The matrix multiply and accumulate operations support a limited set of shapes for the operand matrices A, B and D. The shapes of all three matrix operands are collectively described by the tuple MxNxK where A is MxK matrix, B is a KxN matrix, and D is a MxN matrix.

Table 39 shows matrix shapes that are supported for the specified types for the tcgen05.mma operation.
Table 39 Various combinations of .kind and shapes

Various Combinations
	

Shapes Supported

.kind::*
	

Has .ws
	

CTA Group
	

Sparsity
	

dtype
	

atype/btype

kind::f16
	

No .ws
	

1
	

Dense
	

.f16
	

.f16
	

64xNxK

128xNxK
	

N = {8, 16, 24, … 256} steps of 8
	

K = 16

.f32
	

.f16, .bf16

Sparse
	

.f16
	

.f16
	

K = 32

.f32
	

.f16, .bf16

2
	

Dense
	

.f16
	

.f16
	

128xNxK

256xNxK
	

N = {16, 32, … 256} steps of 16
	

K = 16

.f32
	

.f16, .bf16

Sparse
	

.f16
	

.f16
	

K = 32

.f32
	

.f16, .bf16

.ws
	

1
	

Dense
	

.f16
	

.f16
	

32xNxK

64xNxK

128xNxK
	

N = {64, 128, 256}
	

K = 16

.f32
	

.f16, .bf16

Sparse
	

.f16
	

.f16
	

N = {64, 128}
	

K = 32

.f32
	

.f16, .bf16

2
	

Either
	

.f16
	

.f16
	

Invalid

.f32
	

.f16, .bf16

.kind::tf32
	

No .ws
	

1
	

Dense
	

.f32
	

.tf32
	

64xNxK

128xNxK
	

N = {8, 16, 24, … 256} steps of 8
	

K = 8

Sparse
	

K = 16

2
	

Dense
	

128xNxK

256xNxK
	

N = {16, 32, … 256} steps of 16
	

K = 8

Sparse
	

K = 16

.ws
	

1
	

Dense
	

32xNxK 64xNxK 128xNxK
	

N = {64, 128, 256}
	

K = 8

Sparse
	

N = {64, 128}
	

K = 16

2
	

Dense
	

Invalid

Sparse

.kind::f8f6f4
	

No .ws
	

1
	

Dense
	

.f32

.f16
	

.e4m3,

.e5m2,

.e2m3,

.e3m2,

.e2m1
	

64xNxK

128xNxK
	

N = {8, 16, … 256} steps of 8
	

K = 32

Sparse
	

K = 64

2
	

Dense
	

128xNxK

256xNxK
	

N = {16, 32, … 256} steps of 16
	

K = 32

Sparse
	

K = 64

.ws
	

1
	

Dense
	

32xNxK 64xNxK 128xNxK
	

N = {64, 128, 256}
	

K = 32

Sparse
	

N = {64, 128}
	

K = 64

2
	

Dense
	

Invalid

Sparse

.kind::mxf8f6f4
	

No .ws
	

1
	

Dense
	

.f32
	

.e4m3,

.e5m2,

.e2m3,

.e3m2,

.e2m1

X

(Scale)

.ue8m0
	

128xNxK
	

N = {8, 16, … 256} steps of 8
	

K = 32

Sparse
	

K = 64

2
	

Dense
	

128xNxK

256xNxK
	

N = {16, 32, … 256} steps of 16
	

K = 32

Sparse
	

256xNxK
	

K = 64

.ws
	

1
	

Dense
	

Invalid

Sparse

2
	

Dense

Sparse

.kind::i8
	

No .ws
	

1
	

Dense
	

.s32
	

.s8, .u8
	

64xNxK

128xNxK
	

N = {8, 16, 24, 32, 48, … 256}

steps of 16 after N > 32
	

K = 32

Sparse
	

K = 64

2
	

Dense
	

128xNxK

256xNxK
	

N = {32, 64, … 256} steps of 32
	

K = 32

Sparse
	

K = 64

.ws
	

1
	

Dense
	

32xNxK 64xNxK 128xNxK
	

N = {64, 128, 256}
	

K = 32

Sparse
	

N = {64, 128}
	

K = 64

2
	

Dense
	

Invalid

Sparse

.kind::mxf4
	

No .ws
	

1
	

Dense
	

.f32
	

.e2m1

X

(Scale)

.ue8m0
	

128xNxK
	

N = {8, 16, … 256} steps of 8
	

K = 64

Sparse
	

K = 128

2
	

Dense
	

128xNxK 256xNxK 256xNxK1
	

N = {16, 32, … 256} steps of 16
	

K = 64

K1 = 96

Sparse
	

256xNxK
	

K = 128

.ws
	

1 / 2
	

Either
	

Invalid

.kind::mxf4nvf4
	

No .ws
	

1
	

Dense
	

.f32
	

.e2m1

X

(Scale)

.ue8m0,

.ue4m3
	

128xNxK
	

N = {8, 16, … 256} steps of 8
	

K = 64

Sparse
	

K = 128

2
	

Dense
	

128xNxK 256xNxK 256xNxK1
	

N = {16, 32, … 256} steps of 16
	

K = 64

K1 = 96

Sparse
	

256xNxK
	

K = 128

.ws
	

1 / 2
	

Either
	

Invalid
9.7.16.2.1.1. Target ISA Note

    K = 96 is only supported for target architecture sm_103a.

9.7.16.2.2. Specifying Matrix Shape

M and N can be specified in the Instruction descriptor.

K cannot be explicitly specified but is implicitly determined by the MMA-kind and the sparsity, as shown in the Table 39.
9.7.16.2.3. Data Movement Shape

The data movement shape indicates the dimension of the data to be moved to or from the Tensor Memory. These shapes are described as a tuple lane x size where:

    lane indicates the number of rows in the Tensor Memory; and

    size indicates the amount of data, in units of bits (b), across the columns in the Tensor Memory.

The following shapes are supported by various tcgen05 operations:

Shape
	

tcgen05.<op>

.16x64b, .16x128b, .16x256b, .16x32bx2, .32x32b
	

.ld / .st

.4x256b, .32x128b, .64x128b, .128x256b, .128x128b
	

.cp

.31x256b (implicit)
	

.shift
9.7.16.2.3.1. Memory Layout

The following shows the layout of the matrix fragments across threads of the warp.
9.7.16.2.3.1.1. Matrix fragments for shape .32x32b

A tcgen05{.ld,.st}.32x32b instruction has the following data vector register.

Fragment
	

Elements (low to high)

A vector expression containing .num number of .b32 registers as mentioned in the Table 47.
	

r0, r1, …

A warp executing tcgen05{.ld,.st}.32x32b will access 32 lanes of the Tensor Memory. It loads from or stores to each of the lane (32 * .num)-bits of data as shown in Figure 183.
_images/tcgen05-mma-fragment-3232b.png

Figure 183 Matrix Fragment for shape .32x32b
9.7.16.2.3.1.2. Matrix fragments for shape .16x64b

A tcgen05{.ld,.st}.16x64b instruction has the following data vector register.

Fragment
	

Elements (low to high)

A vector expression containing .num number of .b32 registers as mentioned in the Table 47.
	

r0, r1, …

A warp executing tcgen05{.ld,.st}.16x64b will access 16 lanes of the Tensor Memory. It loads from or stores to each of the lane (64 * .num)-bits of data as shown in Figure 184.
_images/tcgen05-mma-fragment-1664b.png

Figure 184 Matrix Fragment for shape .16x64b
9.7.16.2.3.1.3. Matrix fragments for shape .16x128b

A tcgen05{.ld,.st}.16x128b instruction has the following data vector register.

Fragment
	

Elements (low to high)

A vector expression containing .num number of .b32 registers as mentioned in the Table 47.
	

r0, r1, …

A warp executing tcgen05{.ld,.st}.16x128b will access 16 lanes of the Tensor Memory. It loads from or stores to each of the lane (128 * .num)-bits of data as shown in Figure 185.
_images/tcgen05-mma-fragment-16128b.png

Figure 185 Matrix Fragment for shape .16x128b
9.7.16.2.3.1.4. Matrix fragments for shape .16x256b

A tcgen05{.ld,.st}.16x256b instruction has the following data vector register.

Fragment
	

Elements (low to high)

A vector expression containing .num number of .b32 registers as mentioned in the Table 47.
	

r0, r1, r2, r3, …

A warp executing tcgen05{.ld,.st}.16x256b will access 16 lanes of the Tensor Memory. It loads from or stores to each of the lane (256 * .num)-bits of data as shown in Figure 186.
_images/tcgen05-mma-fragment-16256b.png

Figure 186 Matrix Fragment for shape .16x256b
9.7.16.2.3.1.5. Matrix fragments for shape .16x32bx2

A tcgen05{.ld,.st}.16x32bx2 instruction has the following data vector register.

Fragment
	

Elements (low to high)

A vector expression containing .num number of .b32 registers as mentioned in the Table 47.
	

r0, r1, …

A warp executing tcgen05{.ld,.st}.16x32bx2 will access 16 lanes of the Tensor Memory. It loads from or stores to each of the lane (32 * .num)-bits of data as shown in Figure 187.
_images/tcgen05-mma-fragment-1632b2.png

Figure 187 Matrix Fragment for shape .16x32bx2
9.7.16.3. Major-ness supported by Strides

There are two strides involved while accessing a matrix from shared memory:

    Leading dimension stride (byte offset or absolute address)

    Stride dimension byte offset

9.7.16.3.1. Leading Dimension Stride: relative offset or absolute address

There are two modes of Leading Dimension Strides as described below. Bit #52 in the Shared memory descriptor is used to distinguish between two modes.
9.7.16.3.1.1. Relative offset mode

In this mode, the leading dimension stride is specified as a relative byte offset between the columns as explained in the below table. The leading dimension stride can either be specified as a relative offset between the columns or as an absolute byte address of next buffer. The leading dimension stride is defined differently for transposed and non-transposed matrices. The leading dimension stride is defined as follows for matrices whose element types are normalized to 128-bits:

Major-ness
	

Definition

K-Major
	

    No-Swizzling: the stride from the first column to the second column of the 8x2 tile in the 128-bit element type normalized matrix.

    Swizzled layouts: not used, assumed to be 1.

MN-Major
	

    Interleave: stride from the first 8 columns to the next 8 columns.

    Swizzled layouts: stride from the first (swizzle-byte-size/16) rows to the next (swizzle-byte-size/16) rows.

9.7.16.3.1.2. Absolute address mode for K dimension being 48B

The tcgen05.mma instruction with K-dimension of 48B would overflow the 128B shared memory boundary if the data is packed contiguously.

In this case, the absolute address mode can be used to break up the data in the shared memory into two chunks such that both these chunks are laid out within the aligned 128-byte address boundary. The leading dimension absolute address can point to the second data chunk in the shared memory.
9.7.16.3.1.2.1. Restrictions on the Leading Dimension Absolute Address Stride

Following are the restrictions on the absolute address stride mode:

    Only 128B swizzle (with 16B atomicity) is supported.

    Only K-Major mode is supported. That is, the transpose bits(bits #15 and #16) in Instruction descriptor must be 0.

    The matrix base offset must be 0.

9.7.16.3.2. Stride Dimension Byte Offset

The stride dimension byte offset is defined differently for transposed and non-transposed matrices. The stride dimension byte offset is defined as follows for matrices whose element types are normalized to 128-bits:

Major-ness
	

Definition

K-Major
	

The offset from the first 8 rows to the next 8 rows.

MN-Major
	

    Interleave: offset from the first row to the next row.

    Swizzled layout: offset from the first 8 columns to the next 8 columns

9.7.16.3.3. Canonical Layouts

In terms of CuTe layouts the canonical layout can be expressed as follows:

Major- ness
	

Swizzling mode
	

Canonical Layout without swizzling
	

Swizzling on the previous column

MN- major
	

No-swizzling or Interleaved
	

((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))
	

Swizzle<0, 4, 3>

32B Swizzling
	

((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))
	

Swizzle<1, 4, 3>

64B Swizzling
	

((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))
	

Swizzle<2, 4, 3>

128B Swizzling
	

((T,8,m),(8,k)):((1,T,LBO),(8T,SBO))
	

Swizzle<3, 4, 3>

K- major
	

No-swizzling or Interleaved
	

((8,m),(T,2k)):((1T,SBO),(1,LBO))
	

Swizzle<0, 4, 3>

32B Swizzling
	

((8,m),(T,2k)):((2T,SBO),(1,T))
	

Swizzle<1, 4, 3>

64B Swizzling
	

((8,m),(T,2k)):((4T,SBO),(1,T))
	

Swizzle<2, 4, 3>

128B Swizzling
	

((8,m),(T,2k)):((8T,SBO),(1,T))
	

Swizzle<3, 4, 3>

where

    T = 128 / sizeof-elements-in-bits T represents scale factor which normalizes matrix element types to 128-bits.

    m represents the number of repeating patterns across rows.

    k represents the number of repeating patterns across columns.

Examples

    K-Major, no-swizzling and tf32 type: Figure 188
    _images/async-warpgroup-k-no-swizzle-tf32.png

    Figure 188 K major, no-swizzling and tf32 type

    the strides and related details are as follows:

    Exact layout : Swizzle<0,4,3> o ((8,2),(4,4)):((4,32),(1,64))

    Canonical Layout :Swizzle<0,4,3> o ((8,m),(T,2k)):((1T,SBO),(1,LBO))

    Parameters
    	

    Value

    T
    	

    4

    m
    	

    2

    k
    	

    2

    LBO (relative offset)
    	

    64*sizeof(tf32)

    SBO
    	

    32*sizeof(tf32)

    Encoding of LBO in descriptor
    	

    (LBO) >> 4 = 16

    Encoding of SBO in descriptor
    	

    (SBO) >> 4 = 8

    K-Major, 32B swizzling and tf32 type: Figure 189
    _images/async-warpgroup-k-32B-swizzle-tf32.png

    Figure 189 K major, 32B swizzling and tf32 type

    the strides and related details are as follows:

    Exact layout : Swizzle<1,4,3> o ((8,2),(4,4)):((8,64),(1,4))

    Canonical Layout :Swizzle<1,4,3> o ((8,m),(T,2k)):((2T,SBO),(1,T))

    Parameters
    	

    Value

    T
    	

    4

    m
    	

    2

    k
    	

    2

    LBO (relative offset)
    	

    NA

    SBO
    	

    64*sizeof(tf32)

    Encoding of LBO in descriptor
    	

    1 (assumed)

    Encoding of SBO in descriptor
    	

    (SBO) >> 4 = 16

    MN-Major, no-swizzling and bf16 type: Figure 190
    _images/async-warpgroup-mn-no-swizzle-bf16.png

    Figure 190 MN major, no-swizzling and bf16 type

    the strides and related details are as follows:

    Exact layout : Swizzle<0,4,3> o ((8,1,2),(8,2)):((1,8,64),(8,128))

    Canonical Layout :Swizzle<0,4,3> o ((T,1,m),(8,k)):((1,T,SBO),(1T,LBO))

    Parameters
    	

    Value

    T
    	

    8

    m
    	

    2

    k
    	

    2

    LBO (relative offset)
    	

    128*sizeof(bf16)

    SBO
    	

    64*sizeof(bf16)

    Encoding of LBO in descriptor
    	

    (LBO) >> 4 = 16

    Encoding of SBO in descriptor
    	

    (SBO) >> 4 = 8

    MN-Major, 32B swizzling and bf16 type: Figure 191
    _images/async-warpgroup-mn-32B-swizzle-bf16.png

    Figure 191 MN major, 32B swizzling and bf16 type

    the strides and related details are as follows:

    Exact layout : Swizzle<1,4,3> o ((8,2,2),(8,2)):((1,8,128),(16,256))

    Canonical Layout :Swizzle<1,4,3> o ((T,2,m),(8,k)):((1,T,LBO),(2T,SBO))

    Parameters
    	

    Value

    T
    	

    8

    m
    	

    2

    k
    	

    2

    LBO (relative offset)
    	

    128*sizeof(bf16)

    SBO
    	

    256*sizeof(bf16)

    Encoding of LBO in descriptor
    	

    (LBO) >> 4 = 16

    Encoding of SBO in descriptor
    	

    (SBO) >> 4 = 32

    MN-Major, 64B swizzling and bf16 type: Figure 192
    _images/async-warpgroup-mn-64B-swizzle-bf16.png

    Figure 192 MN major, 64B swizzling and bf16 type

    the strides and related details are as follows:

    Exact layout : Swizzle<2,4,3> o ((8,4,2),(8,2)):((1,8,256),(32,512))

    Canonical Layout :Swizzle<2,4,3> o ((T,4,m),(8,k)):((1,T,LBO),(4T,SBO))

    Parameters
    	

    Value

    T
    	

    8

    m
    	

    2

    k
    	

    2

    LBO (relative offset)
    	

    256*sizeof(bf16)

    SBO
    	

    512*sizeof(bf16)

    Encoding of LBO in descriptor
    	

    (LBO) >> 4 = 32

    Encoding of SBO in descriptor
    	

    (SBO) >> 4 = 64

9.7.16.4. Matrix Descriptors

There are three kinds of matrix descriptors used by the tcgen05 family of instructions.
9.7.16.4.1. Shared memory descriptor

The shared memory descriptor describes the properties of multiplicand matrix in shared memory including its location in the shared memory of the current CTA. It is a 64-bit value contained in a register with the following layout:
Table 40 Shared memory descriptor layout

Bit-field
	

Size in bits
	

Description

0-13
	

14
	

matrix-descriptor-encode (Matrix start address)

16-29
	

14
	

matrix-descriptor-encode (Leading dimension byte offset relative)

OR

matrix-descriptor-encode (Leading dimension byte address absolute)

32-45
	

14
	

matrix-descriptor-encode (Stride dimension byte offset)

46-48
	

3
	

Fixed constant value of 0b001

49-51
	

3
	

Matrix base offset

52
	

1
	

Leading dimension stride mode: - 0: byte offset relative - 1: byte address absolute

53-60
	

8
	

Fixed constant value of 0xb00000000

61-63
	

3
	

Specifies the swizzling mode to be used: 0. No swizzling 1. 128-Byte with 32B atomic swizzling 2. 128-Byte swizzling 4. 64-Byte swizzling 6. 32-Byte swizzling

Note: Values 3, 5 and 7 are invalid

where matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 4

The value of base offset is 0 when the repeating pattern of the specified swizzling mode starts as per shown in Table 41.
Table 41 Starting address of repeating pattern for various swizzling modes

Swizzling mode
	

Starting address of the repeating pattern

128-Byte swizzle
	

1024-Byte boundary

64-Byte swizzle
	

512-Byte boundary

32-Byte swizzle
	

256-Byte boundary

Otherwise, the base offset must be a non-zero value, computed using the following formula: base offset = (pattern start addr >> 0x7) & 0x7

The following must be 16-byte aligned:

    Matrix start address

    Leading dimension byte offset

    Stride dimension byte offset

9.7.16.4.1.1. Target ISA Note

    The byte address mode for the leading dimension stride is supported on sm_103a.

9.7.16.4.2. Instruction descriptor

The instruction descriptor describes the shapes, types and other details of all the matrices and the matrix-multiplication-and-accumulation operation. It is a 32-bit value in registers and the exact layout is dependent on the MMA-Kind:
Table 42 Instruction descriptor format for .kind::tf32, .kind::f16, .kind::f8f6f4 and .kind::i8

Bits
	

Size

(bits)
	

Description
	

Values

.kind::tf32
	

.kind::f16
	

.kind::f8f6f4
	

.kind::i8

0-1
	

2
	

Sparsity selector, if Sparsity is enabled
	

0-3

2
	

1
	

Sparsity
	

Dense = 0

Sparse = 1

3
	

1
	

Saturate for integer types
	

0 (NA)
	

No Saturate = 0 Saturate = 1

4-5
	

2
	

dtype (Matrix D type)
	

F32 = 1
	

F16 = 0 F32 = 1
	

S32 = 2

6
	

1
	

Reserved
	

0

7-9
	

3
	

atype (Matrix A type)
	

TF32 = 2
	

F16 = 0

BF16 = 1
	

E4M3 = 0 E5M2 = 1 E2M3 = 3 E3M2 = 4 E2M1 = 5
	

Unsigned 8b = 0

Signed 8b = 1

10-12
	

3
	

btype (Matrix B type)

13
	

1
	

Negate A Matrix
	

No Negate = 0

Negate = 1
	

No Negate = 0

14
	

1
	

Negate B Matrix

15
	

1
	

Transpose A Matrix
	

No Transpose = 0

Transpose = 1

16
	

1
	

Transpose B Matrix

17-22
	

6
	

N, Dimension of Matrix B (3 LSBs not included)
	

N >> 3

23
	

1
	

Reserved
	

0

24-28
	

5
	

M, Dimension of Matrix A (4 LSBs not included)
	

M >> 4

29
	

1
	

Reserved
	

0

30-31
	

2
	

Maximum shift while attempting B matrix -reuse in .ws
	

no shift = 0 maximum shift of 8 = 1 maximum shift of 16 = 2 maximum shift of 32 = 3
Table 43 Instruction descriptor format for .kind::mxf8f6f4

Bits
	

Size

(bits)
	

Description
	

Values

.kind::mxf8f6f4

0-1
	

2
	

Reserved
	

0

2
	

1
	

Sparsity
	

Dense = 0

Sparse = 1

3
	

1
	

Reserved
	

0

4-5
	

2
	

Matrix B Scale Factor Data ID
	

0-3

6
	

1
	

Reserved
	

0

7-9
	

3
	

atype (Matrix A type)
	

E4M3 = 0 E5M2 = 1 E2M3 = 3 E3M2 = 4 E2M1 = 5

10-12
	

3
	

btype (Matrix B type)

13
	

1
	

Negate A Matrix
	

No Negate = 0

Negate = 1

14
	

1
	

Negate B Matrix

15
	

1
	

Transpose A Matrix
	

No Transpose = 0

Transpose = 1

16
	

1
	

Transpose B Matrix

17-22
	

6
	

N, Dimension of Matrix B (3 LSBs not included)
	

N >> 3

23
	

1
	

Scale Matrix Type, for both scale_A / scale_B
	

UE8M0 = 1

24-26
	

3
	

Reserved
	

0

27-28
	

2
	

M, Dimension of Matrix A (7 LSBs not included)
	

M >> 7

29-30
	

2
	

Matrix A Scale Factor Data ID
	

0-3

31
	

1
	

Reserved
	

0
Table 44 Instruction descriptor format for .kind::mxf4 and .kind::mxf4nvf4

Bits
	

Size

(bits)
	

Description
	

Values

.kind::mxf4
	

.kind::mxf4nvf4

0-1
	

2
	

Reserved
	

0

2
	

1
	

Sparsity
	

Dense = 0

Sparse = 1

3
	

1
	

Reserved
	

0

4-5
	

2
	

Matrix B Scale Factor Data ID
	

0 or 2

6
	

1
	

Reserved
	

0

7-9
	

3
	

atype (Matrix A type)
	

E2M1 = 1

10-11
	

2
	

btype (Matrix B type)

12
	

1
	

Reserved
	

0

13
	

1
	

Negate A Matrix
	

No Negate = 0

Negate = 1

14
	

1
	

Negate B Matrix

15
	

1
	

Transpose A Matrix
	

No Transpose = 0

16
	

1
	

Transpose B Matrix

17-22
	

6
	

N, Dimension of Matrix B (3 LSBs not included)
	

N >> 3

23
	

1
	

Scale Matrix Type, for both scale_A / scale_B
	

UE8M0 = 1
	

UE4M3 = 0

24-26
	

3
	

Reserved
	

0

27-28
	

2
	

M, Dimension of Matrix A (7 LSBs not included)
	

M >> 7

29-30
	

2
	

Matrix A Scale Factor Data ID
	

0 or 2

31
	

1
	

K Dimension
	

(Dense K=64 / Sparse K=128) = 0

(Dense K=96) = 1
9.7.16.4.3. Zero-Column Mask Descriptor

The zero-column mask descriptor is used to generate a mask that specifies which columns of B matrix will have zero value for the MMA operation regardless of the values present in the shared memory. The total size of the generated mask is N-bits.

A 0-bit in the mask specifies that values of the corresponding column in matrix B should be used for the MMA operation. A 1-bit in the mask specifies 0s must be used for the entire column for the MMA operation.

The zero-column mask descriptor is a 64-bit value in registers with the following layout:
Table 45 Zero-Column Mask descriptor layout

Bits
	

Size (bits)
	

Field Name
	

Description

0-7
	

8
	

Start Count 0 (sc0)
	

Specifies the LSBs that must be skipped

for sub-mask mask-i

8-15
	

8
	

Start Count 1 (sc1)

16-23
	

8
	

Start Count 2 (sc2)

24-31
	

8
	

Start Count 3 (sc3)

32
	

1
	

First Span 0 (fs0)
	

Specifies the starting value for

sub-mask mask-i

33
	

1
	

First Span 1 (fs1)

34
	

1
	

First Span 2 (fs2)

35
	

1
	

First Span 3 (fs3)

36-38
	

3
	

Reserved
	

39
	

1
	

Non-Zero Mask
	

Value 0 indicates generated mask will have all 0s Value 1 indicates the mask has to be generated

40-47
	

8
	

Skip Span
	

(Count of consecutive columns where B matrix is used) - 1

48-55
	

8
	

Use Span
	

(Count of consecutive columns where 0s ar used) - 1

56-61
	

6
	

Column Shift
	

Shifts column by specified amount. Thus allows MMA on non-0 starting column. Max shift amount = 16 for M=32 Max shift amount = 32 otherwise

The zero-column mask is made up of one or more sub-mask depending on M, as shown in the table:

M
	

Zero-Column Mask breakup
	

Sub-masks
	

First Span used
	

Start Column used

128
	

Single sub-mask of size N-bits
	

mask0
	

fs0
	

sc0

64
	

Two sub-masks, each with size of N/2 bits
	

mask0, mask1
	

fs0, fs1
	

sc0, sc1

32
	

Four sub-masks, each with size of N/4 bits
	

mask0, mask1 mask2, mask3
	

fs0, fs1, fs2, fs3
	

sc0, sc1, sc2, sc3

The following table shows the coverage of the sub-masks across N-dimension:

Sub-mask
	

M

128
	

64
	

32

mask0
	

Columns [0, N-1]
	

Columns [0, N/2-1]
	

Columns [0, N/4-1]

mask1
	

–
	

Columns [N/2, N-1]
	

Columns [N/4, N/2-1]

mask2
	

–
	

–
	

Columns [N/2, (N/4*3)-1]

mask3
	

–
	

–
	

Columns [(N/4*3), N-1]

The following examples shows zero-column mask descriptor and their corresponding mask generated:

    Example 1: M = 128

    Input zero-column mask descriptor:

    Start count
    	

    First span
    	

    Non-Zero Mask
    	

    Skip Span
    	

    Use Span
    	

    Shift

    {0, 0, 0, 0}
    	

    {0, 0, 0, 0}
    	

    0
    	

    4
    	

    3
    	

    0

    Output zero-column mask: 0x0.

    As Non-Zero Mask field is 0, the mask is 0x0. All the columns of the matrix B will be used for the MMA operation.

    Example 2: M = 128

    Input zero-column mask descriptor:

    Start count
    	

    First span
    	

    Non-Zero Mask
    	

    Skip Span
    	

    Use Span
    	

    Shift

    {-, -, -, 0}
    	

    {-, -, -, 0}
    	

    1
    	

    2
    	

    3
    	

    0

    Output mask0: 0b … 111 0000 111 0000 (size = N)

    Example 3: M = 64

    Input zero-column mask descriptor:

    Start count {.., sc1, sc0}
    	

    First span {.., fs1, fs0}
    	

    Non-Zero Mask
    	

    Skip Span
    	

    Use Span
    	

    Shift

    {-, -, 0, 0}
    	

    {-, -, 0, 1}
    	

    1
    	

    2
    	

    3
    	

    0

    Output mask0: 0b … 111 0000 111 0000 111

    Output masl1: 0b … 0000 111 0000 111 0000

    Example 4: M = 32

    Input zero-column mask descriptor:

    Start count {sc3, sc2, sc1, sc0}
    	

    First span {fs3, fs2, fs1, fs0}
    	

    Non-Zero Mask
    	

    Skip Span
    	

    Use Span
    	

    Shift

    {1, 2, 1, 0}
    	

    {0, 0, 1, 1}
    	

    1
    	

    2
    	

    3
    	

    2

    Output mask0: 0b … 0000 111 0000 111

    Output mask1: 0b … 0000 111 0000 11

    Output mask2: 0b … 111 0000 111 00

    Output mask3: 0b … 111 0000 111 000

    If N = 128 then B Matrix with columns from 2 to 129 will be used for the MMA operation, due to the shift of 2.

9.7.16.5. Issue Granularity

Each of the tcgen05 operation has different requirements for the number of threads/warps that needs to issue them.

The following table lists the execution granularity requirements of each of the tcgen05 operation:
Table 46 Execution granularity requirements for tcgen05 operations

tcgen05 operation
	

.cta_group
	

Issue Granularity

.mma,
.cp,
.shift,
.commit

::1
	

An issue from a single thread in the current CTA would initiate the base operation.

::2
	

Issue from a single thread from the CTA-Pair would initiate the base operation. When the current CTA issues the operation, the peer CTA should be active and should not have exited.

.alloc,
.dealloc,
.relinquish_alloc_permit

::1
	

Issue from a single warp in the current CTA would initiate the allocation management instruction.

::2
	

Issue from two warps, one in each of the current CTA and its Peer CTA, collectively needs to perform the operation. When the current CTA issues the operation, the peer CTA should be active and should not have exited.

.ld,
.st,
.wait::{ld, st}

N/A
	

Issue from a warp in the current CTA can access only 1/4 of the Tensor Memory of the current CTA. So, a warpgroup is needed to access the entire Tensor Memory of the current CTA.

.fence::*

N/A
	

A thread needs to fence all its accesses to the tensor memory that it wants to order with other accesses to the tensor memory from other threads.
9.7.16.5.1. CTA Pair

Any 2 CTAs within the cluster whose %cluster_ctarank differs by the last bit only is said to form a CTA pair.

Within a CTA pair, the CTA whose last bit in the %cluster_ctarank is:

    0 is termed the even numbered CTA within the CTA pair.

    1 is termed as the odd numbered CTA within the CTA pair.

Most of the tcgen05 operations can either execute at a single CTA level granularity OR at a CTA pair level granularity. When a tcgen05 operation is performed at CTA pair granularity, the Tensor Memory of both the CTAs within the CTA pair are accessed. The set of threads that need to issue the tcgen05 operation is listed in the Issue Granularity.
9.7.16.5.2. Peer CTA

The peer CTA of the odd CTA within the CTA pair is the even CTA in the same pair. Similarly, the peer CTA of the even CTA within the CTA pair is the odd CTA in the same pair.
9.7.16.6. Memory Consistency Model for 5th generation of TensorCore operations

Ordering of tcgen05 instructions is described in terms of two key concepts:

    Pipelined tcgen05 instructions

    Specialized tcgen05-specific inter-thread synchronization mechanisms.

These concepts combine to form four canonical synchronization patterns, as described further below.
9.7.16.6.1. Asynchronous Operations

The tcgen05 family of instructions are divided into 2 categories:

    Asynchronous instructions:

    These tcgen05 operations are not inherently ordered with respect to other tcgen05 operations in the same thread (unless pipelined as mentioned below).

    Synchronous instructions:

    These tcgen05 operations are inherently ordered with respect to other tcgen05 operations in the same order.

    The Tensor Memory allocation related instructions that access shared memory maintain same-address ordering with respect to non-tcgen05 instructions.

The following table lists the category of each of the tcgen05 instruction:

tcgen05.* operation
	

Category

.alloc
	

Synchronous

instructions

.dealloc

.relinquish_alloc_permit

.fence::*

.wait::*

.commit

.mma
	

Asynchronous

instructions

.cp

.shift

.ld

.st
9.7.16.6.2. Pipelined tcgen05 Instructions

The asynchronous tcgen05 operations may execute and complete in a different order than they were issued. However, some specific pairs of the asynchronous tcgen05 instructions form tcgen05 pipelines, where in the two asynchronous operations are guaranteed to execute in the same order as the instructions that issued them. The specific pairings are as follows:

    tcgen05.mma.cta_group::N -> tcgen05.mma.cta_group::N (same N and accumulator and shape)

    tcgen05.copy.cta_group::N -> tcgen05.mma.cta_group::N (same N)

    tcgen05.shift.cta_group::N -> tcgen05.mma.cta_group::N (same N)

    tcgen05.shift.cta_group::N -> tcgen05.cp.4x256b.cta_group::N (same N)

    tcgen05.mma.cta_group::N -> tcgen05.shift.cta_group::N (same N)

9.7.16.6.2.1. Implicitly pipelined tcgen05 Instructions

Instructions tcgen05.commit and tcgen05.wait are implicitly pipelined with respect to previously issued tcgen05.{mma,cp,shift} and tcgen05.{ld,st} instructions respectively that they track from the same thread.
9.7.16.6.2.1.1. mbarrier based completion mechanism

Completion of the following instruction’s asynchronous operations is observed through the mbarrier based waiting mechanism:

    tcgen05.mma

    tcgen05.cp

    tcgen05.shift

tcgen05.commit is used to track the completion of the above asynchronous instructions.

Following are the implicitly pipelined tcgen05 instruction pairing that uses mbarrier based completion mechanism:

    tcgen05.mma.cta_group::N -> tcgen05.commit.cta_group::N (same N)

    tcgen05.cp.cta_group::N -> tcgen05.commit.cta_group::N (same N)

    tcgen05.shift.cta_group::N -> tcgen05.commit.cta_group::N (same N)

9.7.16.6.2.1.2. tcgen05.wait instruction based completion mechanism

Completion of the following instruction’s asynchronous operations is observed through tcgen05.wait based waiting mechanism:

    tcgen05.ld

    tcgen05.st

tcgen05.wait::ld and tcgen05.wait::st is used to track the completion of the tcgen05.ld and tcgen05.st asynchronous instructions.

Following are the implicitly pipelined tcgen05 instruction pairing that uses tcgen05.wait based completion mechanism:

    tcgen05.ld -> tcgen05.wait::ld

    tcgen05.st -> tcgen05.wait::st

9.7.16.6.3. Specialized Inter-thread Synchronization for tcgen05 instructions

The tcgen05 instructions support a specialized inter-thread synchronization which are optimized for tcgen05 family of instructions. The standard memory consistency model synchronization mechanisms also apply to the tcgen05 family of instructions.

The TensorCore 5th Generation Specialized Synchronization Operations section contains the specialized inter-thread synchronization for tcgen05 instructions.

The tcgen05.fence::before_thread_sync and tcgen05.fence::after_thread_sync composes with execution ordering instructions, like morally strong ld/st/atom instructions, mbarrier instruction, barrier instructions and so on, to establish an ordering between the tcgen05 operations across threads. The asynchronous tcgen05 instructions that are ordered across threads also form a tcgen05 pipeline.

An asynchronous tcgen05 operation prior to a tcgen05.fence::before_thread_sync is ordered before all subsequent tcgen05 and the execution ordering operations.

An asynchronous tcgen05 operation subsequent to a tcgen05.fence::after_thread_sync is ordered after all the prior tcgen05 and the execution ordering operations.
9.7.16.6.4. Canonical synchronization patterns

Using the above rules, the following are the five canonical synchronization patterns:
9.7.16.6.4.1. Pipelined instructions, same thread

In this pattern, no explicit ordering mechanism is needed and the ordering guarantee is provided by the pipelined instruction pairing.

Example:

tcgen05.mma
tcgen05.mma (same shape and accumulator)

The two instructions will be executed in program order.
9.7.16.6.4.2. Non-pipelined instructions, same thread

In this pattern, explicit waiting mechanisms are used to wait for the completion of the asynchronous tcgen05 operations.

Example 1:

tcgen05.st
tcgen05.wait::st
tcgen05.ld

tcgen05.wait::st is used to wait for the completion of the prior asynchronous instruction tcgen05.st.

Example 2:

tcgen05.mma [d], ...
tcgen05.commit.mbarrier::arrive::one
mbarrier.try_wait.relaxed.cluster (loop until successful)
tcgen05.fence::after_thread_sync
tcgen05.ld [d], ...

For the completion of the asynchronous tcgen05.mma, tcgen05.commit is used.

As tcgen05.ld is an asynchronous operation, the instruction tcgen05.fence::after_thread_sync is needed.

No explicit tcgen05.fence::before_thread_sync is needed as this is implicitly performed by tcgen05.commit. The combination of tcgen05.mma and tcgen05.commit forms a conceptual asynchronous pipeline and establishes execution ordering.

tcgen05.mma [d], ...
tcgen05.fence::before_thread_sync
mbarrier::arrive

9.7.16.6.4.3. Pipelined instructions, different thread

In this pattern, no explicit waiting mechanism is needed but proper synchronization between threads is needed.

Example:

Thread 0
	

Thread 1

tcgen05.cp
tcgen05.fence::before_thread_sync
mbarrier.arrive.relaxed.cluster

	

mbarrier.try_wait.relaxed.cluster // loop till success
tcgen05.fence::after_thread_sync
tcgen05.mma

9.7.16.6.4.4. Non-pipelined instructions, different thread

In this pattern, the producer threads that issue the asynchronous tcgen05 instructions must explicitly wait for the instructions’ completion before synchronizing with the consumer threads.

Example 1:

Thread 0
	

Thread 1

tcgen05.ld
tcgen05.wait::ld
tcgen05.fence::before_thread_sync
mbarrier.arrive.relaxed.cluster

	

mbarrier.try_wait.relaxed.cluster // loop till success
tcgen05.fence::after_thread_sync
tcgen05.mma

Example 1:

Thread 0
	

Thread 1

tcgen05.mma
tcgen05.commit.mbarrier::arrive::one [mbar]

	

mbarrier.try_wait.relaxed.cluster [mbar] // loop till success
tcgen05.fence::after_thread_sync
tcgen05.ld

The synchronization mechanisms can also be composed with each other. For example:

Thread 0
	

Thread 1

tcgen05.mma
tcgen05.commit.mbarrier::arrive::one [bar1]
mbarrier.try_wait.relaxed.cluster [bar1] // loop
...
tcgen05.fence::after_thread_sync
...// completion is guaranteed
tcgen05.fence::before_thread_sync
mbarrier.arrive.relaxed.cluster   [bar2] // loop
...

	

mbarrier.try_wait.relaxed.cluster [bar2] // loop
...
tcgen05.fence::after_thread_sync
tcgen05.ld

9.7.16.6.4.5. Register dependencies, same thread

For tcgen05.ld, an intra-thread ordering through true register dependency will be respected regardless of the presence or absence of other forms of synchronization. This form of register dependency does not imply any other form of ordering. For example, a register dependency does not imply that a dependee instruction’s memory accesses will be performed before a dependent instruction’s memory accesses. To enforce such memory orderings and avoiding anti-dependency hazards around tcgen05.ld, tcgen05.wait::ld must be used.

Example:

tcgen05.ld %r1, ...;
tcgen05.mma ..., %r1, ...;

9.7.16.6.5. Shared Memory Accesses

The shared memory accesses by tcgen05.mma and tcgen05.cp operations are performed in the asynchronous proxy (async proxy).

Accessing the same memory location across miltiple proxies needs a cross-proxy fence. For the async proxy, fence.proxy.async should be used to synchronize memory between generic proxy and the async proxy.
9.7.16.7. Tensor Memory Allocation and Management Instructions
9.7.16.7.1. Tensorcore 5th Generation Instructions: tcgen05.alloc, tcgen05.dealloc, tcgen05.relinquish_alloc_permit

tcgen05.alloc, tcgen05.dealloc, tcgen05.relinquish_alloc_permit

Dynamic Tensor Memory allocation management instructions

Syntax

tcgen05.alloc.cta_group.sync.aligned{.shared::cta}.b32  [dst], nCols;

tcgen05.dealloc.cta_group.sync.aligned.b32              taddr, nCols;

tcgen05.relinquish_alloc_permit.cta_group.sync.aligned;

.cta_group = { .cta_group::1, .cta_group::2 }

Description

tcgen05.alloc is a potentially blocking instruction which dynamically allocates the specified number of columns in the Tensor Memory and writes the address of the allocated Tensor Memory into shared memory at the location specified by address operand dst. The tcgen05.alloc blocks if the requested amount of Tensor Memory is not available and unblocks as soon as the requested amount of Tensor Memory becomes available for allocation.

Instruction tcgen05.dealloc deallocates the Tensor Memory specified by the Tensor Memory address taddr. The operand taddr must point to a previous Tensor Memory allocation.

All of the Tensor Memory that was allocated using tcgen05.alloc instruction in a kernel, must be explicitly deallocated using tcgen05.dealloc before the kernel exits.

The unsigned 32-bit operand nCols specify the number of columns to be allocated or de-allocated. The unit of allocation and de-allocation is 32 columns and all of lanes per column. The number of columns must be a power of 2. The operand nCols must be within the range [32, 512]. The number of columns allocated should not increase between any two allocations in the execution order within the CTA. Operand nCols must be power of 2.

Instruction tcgen05.relinquish_alloc_permit specifies that the CTA of the executing thread is relinquishing the right to allocate Tensor Memory. So, it is illegal for a CTA to perform tcgen05.alloc after any of its constituent threads execute tcgen05.relinquish_alloc_permit.

If no state space is specified then Generic Addressing is used. If the address specified by dst does not fall within the address window of .shared::cta state space then the behavior is undefined.

Qualifier .cta_group specifies the number of CTAs involved in the allocation and de-allocation operation. When .cta_group::1 is specified, one warp from the CTA must perform the allocation and de-allocation. When .cta_group::2 is specified, one warp from each of the peer CTAs must collectively perform the allocation and de-allocation. Refer to the Issue Granularity section. When .cta_group::2 is specified, the issuing warp must make sure that peer CTA is launched and is still active.

All tcgen05 instructions within a kernel must specify the same value for the .cta_group qualifier.

The mandatory .sync qualifier indicates that the instruction causes the executing thread to wait until all threads in the warp execute the same instruction before resuming execution.

The mandatory .aligned qualifier indicates that all threads in the warp must execute the same instruction. In conditionally executed code, the instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined.

The behavior of the instruction is undefined if all the threads in the warp do not use the same values of nCols, or if any thread in the warp has exited.

The store operation in tcgen05.alloc is treated as a weak memory operation in the Memory Consistency Model.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

    sm_110f or higher in the same family

Examples

// Example 1:

tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sMemAddr1], 32;
ld.shared.b32 taddr, [sMemAddr1];
// use taddr ...
// more allocations and its usages ...
tcgen05.dealloc.cta_group::1.sync.aligned.b32  taddr, 32;
// more deallocations ...
tcgen05.relinquish_alloc_permit.cta_group::1.sync.aligned;

// Example 2:

// Following instructions are performed by current warp and the warp in the peer-CTA:
tcgen05.alloc.cta_group::2.sync.aligned.shared::cta.b32 [sMemAddr2], 32;
ld.shared.b32 taddr, [sMemAddr2];
// use taddr ...
// more allocations and its usages ...
tcgen05.dealloc.cta_group::2.sync.aligned.b32  taddr, 32;
// more deallocations ...
tcgen05.relinquish_alloc_permit.cta_group::2.sync.aligned;

9.7.16.8. Tensor Memory and Register Load/Store Instructions

The threads of the CTA can perform the loads and stores to the Tensor Memory of the CTA and move data between registers and Tensor Memory. The loads and stores of data can be performed in certain shapes as specified in the Matrix and Data Movement Shape section.
9.7.16.8.1. Access restrictions

Not all threads of the CTA can access the entire Tensor Memory via the tcgen05.ld and tcgen05.st operations.

The Tensor Memory of a CTA is divided into 4 equal chunks such that each warp of a warpgroup in the CTA can access a chunk of the Tensor Memory. All the columns of the Tensor Memory can be accessed by all the four warps of a warpgroup. A lane of the Tensor Memory can be accessed by a single warp in the warpgroup. The following table describes the access restriction.

ID of the warp within the warpgroup
	

Accessible Lanes

0
	

0-31

1
	

32-63

2
	

64-95

3
	

96-127
9.7.16.8.2. Packing and Unpacking

Optionally, the following pack and unpack operations can be performed during the load and store:

    Packing: two 16-bit chunks can be packed into a single 32-bit chunk in the register in tcgen05.ld

    Unpacking: a single 32-bit chunk in the register can be unpacked into two 16-bit chunks in tcgen05.st

as shown in the Figure 193.
_images/tcgen05-ld-st-pack-unpack.png

Figure 193 Pack/Unpack operations for tcgen05 ld/st
9.7.16.8.3. Tensorcore 5th Generation Instructions: tcgen05.ld

tcgen05.ld

Asynchronous collective load from tensor memory into registers.

Syntax

// Base load instruction:

tcgen05.ld.sync.aligned.shape1.num{.pack}.b32    r, [taddr];

tcgen05.ld.sync.aligned.shape2.num{.pack}.b32    r, [taddr], immHalfSplitoff;

.shape1 = { .16x64b, .16x128b, .16x256b, .32x32b }
.shape2 = { .16x32bx2 }
.num    = { .x1, .x2, .x4, .x8, .x16, .x32, .x64, .x128 }
.pack   = { .pack::16b }

// Floating point type load along with reduction :

tcgen05.ld.red.sync.aligned.shape3.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr];

tcgen05.ld.red.sync.aligned.shape4.num.redOp{.abs}{.NaN}.f32 r, redval, [taddr], immHalfSplitoff;

// Integer type load along with reduction :

tcgen05.ld.red.sync.aligned.shape3.num.redOp.type r, redval, [taddr];

tcgen05.ld.red.sync.aligned.shape4.num.redOp.type r, redval, [taddr], immHalfSplitoff;

.shape3 = { .32x32b   }
.shape4 = { .16x32bx2 }
.redOp  = { .min, .max }
.type   = { .u32, .s32 }

Description

Instruction tcgen05.ld asynchronously loads data from the Tensor Memory at the location specified by the 32-bit address operand taddr into the destination register r, collectively across all threads of the warps.

All the threads in the warp must specify the same value of taddr, which must be the base address of the collective load operation. Otherwise, the behavior is undefined.

The .shape qualifier and the .num qualifier together determines the total dimension of the data which is loaded from the Tensor Memory. The .shape qualifier indicates the base dimension of data to be accessed as described in the Data Movement Shape. The .num qualifier indicates the repeat factor on the base dimension resulting in the total dimension of the data that is accessed.

The shape .16x32bx2 performs two accesses into Tensor Memory of the shape .16x32b. The base address of the first access is specified by taddr and the base address of the second access is specified by taddr+immHalfSplitoff, where immHalfSplitoff is an immediate argument.

The destination operand r is a brace-enclosed vector expression consisting of one or more 32-bit registers as per the value of .shape and .num. The size of the vector for various combinations of .num and .shape is shown in Table 47.
Table 47 Various-combinations of .num and .shape

.num
	

.shape

.16x32bx2 / .16x64b / .32x32b
	

.16x128b
	

.16x256b

.x1
	

1
	

2
	

4

.x2
	

2
	

4
	

8

.x4
	

4
	

8
	

16

.x8
	

8
	

16
	

32

.x16
	

16
	

32
	

64

.x32
	

32
	

64
	

128

.x64
	

64
	

128
	

NA

.x128
	

128
	

NA
	

NA

The qualifier .red specifies that the reduction operation specified by .redOp is performed on the data that is loaded across columns in each lane. The result of the reduction operation is written into the corresponding thread’s 32-bit destination register operand redVal. When .red qualifier is specified, .num modifier must be at least .x2.

The optional qualifier .pack::16b can be used to pack two 16-bit elements from adjacent columns into a single 32-bit element during the load as shown in the section Packing and Unpacking.

The mandatory .sync qualifier indicates that tcgen05.ld causes the executing thread to wait until all threads in the warp execute the same tcgen05.ld instruction before resuming execution.

The mandatory .aligned qualifier indicates that all threads in the warp must execute the same tcgen05.ld instruction. In conditionally executed code, a tcgen05.ld instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined.

The behavior of tcgen05.ld is undefined if all threads do not use the same values of taddr, or if any thread in the warp has exited.

The instruction tcgen05.ld is performed asynchronously and more details are specified in the section Memory Consistency Model for 5th generation of TensorCore operations.
PTX ISA Notes

Introduced in PTX ISA version 8.6.

tcgen05.ld.red is introduced in PTX ISA version 8.8.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

    sm_110f or higher in the same family

tcgen05.ld.red is supported on following architectures:

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8:

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

        sm_103f or higher in the same family

    sm_110f or higher in the same family

Examples

tcgen05.ld.sync.aligned.32x32b.x2.b32     {r0, r1}, [taddr1];

tcgen05.ld.sync.aligned.16x128b.x4.b32    {r0, r1, r2, r3, r4, r5, r6, r7}, [taddr2];

tcgen05.ld.red.sync.aligned.16x32bx2.x8.u32.max {r0, r1, r2, r3, r4, r5, r6, r7},
                                                 redVal, [taddr3], 16;

9.7.16.8.4. Tensorcore 5th Generation Instructions: tcgen05.st

tcgen05.st

Asynchronous collective store to tensor memory from registers.

Syntax

tcgen05.st.sync.aligned.shape1.num{.unpack}.b32    [taddr], r;

tcgen05.st.sync.aligned.shape2.num{.unpack}.b32    [taddr], immHalfSplitoff, r;

.shape1 = { .16x64b, .16x128b, .16x256b, .32x32b }
.shape2 = { .16x32bx2 }
.num    = { .x1, .x2, .x4, .x8, .x16, .x32, .x64, .x128 }
.unpack = { .unpack::16b }

Description

Instruction tcgen05.st asynchronously stores data from the source register r into the Tensor Memory at the location specified by the 32-bit address operand taddr, collectively across all threads of the warps.

All the threads in the warp must specify the same value of taddr, which must be the base address of the collective store operation. Otherwise, the behavior is undefined.

The .shape qualifier and the .num qualifier together determines the total dimension of the data which is stored to the Tensor Memory. The .shape qualifier indicates the base dimension of data to be accessed as described in the Data Movement Shape. The .num qualifier indicates the repeat factor on the base dimension resulting in the total dimension of the data that is accessed.

The shape .16x32bx2 performs two accesses into Tensor Memory of the shape .16x32b. The base address of the first access is specified by taddr and the base address of the second access is specified by taddr+immHalfSplitoff, where immHalfSplitoff is an immediate argument.

The source operand r is a brace-enclosed vector expression consisting of one or more 32-bit registers as per the value of .shape and .num. The size of the vector for various combinations of .num and .shape is shown in Table 48.
Table 48 Various-combinations of .num and .shape

.num
	

.shape

.16x32bx2 / .16x64b / .32x32b
	

.16x128b
	

.16x256b

.x1
	

1
	

2
	

4

.x2
	

2
	

4
	

8

.x4
	

4
	

8
	

16

.x8
	

8
	

16
	

32

.x16
	

16
	

32
	

64

.x32
	

32
	

64
	

128

.x64
	

64
	

128
	

NA

.x128
	

128
	

NA
	

NA

The optional qualifier .unpack::16b can be used to unpack a 32-bit element in the register into two 16-bit elements and store them in adjacent columns as shown in the section Packing and Unpacking.

The mandatory .sync qualifier indicates that tcgen05.st causes the executing thread to wait until all threads in the warp execute the same tcgen05.st instruction before resuming execution.

The mandatory .aligned qualifier indicates that all threads in the warp must execute the same tcgen05.st instruction. In conditionally executed code, a tcgen05.st instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined.

The behavior of tcgen05.st is undefined if all threads do not use the same values of taddr, or if any thread in the warp has exited.

The instruction tcgen05.st is performed asynchronously and more details are specified in the section Memory Consistency Model for 5th generation of TensorCore operations.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

    sm_110f or higher in the same family

Examples

tcgen05.st.sync.aligned.16x64b.x4.b32               [taddr0], {r0,  r1,  r2,  r3};

tcgen05.st.sync.aligned.16x128b.x1.unpack::16b.b32  [taddr1], {r0,  r1};

9.7.16.8.5. Tensorcore 5th Generation Instructions: tcgen05.wait

tcgen05.wait

Waits for the completion of all prior asynchronous tcgen05.ld / tcgen05.st instructions.

Syntax

tcgen05.wait_operation.sync.aligned;

.wait_operation = { .wait::ld, .wait::st }

Description

Instruction tcgen05.wait::st causes the executing thread to block until all prior tcgen05.st operations issued by the executing thread have completed.

Instruction tcgen05.wait::ld causes the executing thread to block until all prior tcgen05.ld operations issued by the executing thread have completed.

The mandatory .sync qualifier indicates that tcgen05.wait_operation causes the executing thread to wait until all threads in the warp execute the same tcgen05.wait_operation instruction before resuming execution.

The mandatory .aligned qualifier indicates that all threads in the warp must execute the same tcgen05.wait_operation instruction.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

    sm_110f or higher in the same family

Examples

Example 1:

tcgen05.ld.sync.aligned.32x32b.x2.b32     {r0, r1}, [taddr0];

// Prevents subsequent tcgen05.mma from racing ahead of the tcgen05.ld

tcgen05.wait::ld.sync.aligned;

tcgen05.mma.cta_group::1.kind::f16   [taddr0],  a-desc,  b-desc, idesc, p;

Example 2:

tcgen05.st.sync.aligned.32x32b.x2.b32     [taddr0], {r0, r1};

// Prevents the write to taddr0 in tcgen05.mma from racing ahead of the tcgen05.st

tcgen05.wait::st.sync.aligned;

tcgen05.mma.cta_group::1.kind::f16   [taddr0],  a-desc,  b-desc, idesc, p;

9.7.16.9. Tensor Memory Data Movement Instructions

Data from the shared memory can be copied asynchronously to the Tensor Memory using the Tensorcore 5th Generation Instructions: tcgen05.cp operation.
9.7.16.9.1. Optional Decompression

Optionally, during the copy, a vector of 4-bit and 6-bit custom floating point types can be decompressed into 8-bit types.
9.7.16.9.1.1. Decompression of 4-bit floating point to 8-bit type

A contiguous set of 16 elements of 4-bits each followed by 8 bytes of padding can be converted into 16 elements of 8-bits each as shown in Figure 194.
_images/tcgen05-decompression-4b8b.png

Figure 194 Decompression from 4-bit to 8-bit

The individual 4-bit to 8-bit decompression would look like as shown in Figure 195.
_images/tcgen05-decompression-4b8b-individual.png

Figure 195 Individual decompression from 4-bit to 8-bit
9.7.16.9.1.2. Decompression of 6-bit floating point to 8-bit type

A contiguous set of 16 elements of 6-bits each followed by 4 bytes of padding is decompressed into 16 elements of 8-bits each as shown in Figure 196.
_images/tcgen05-decompression-6b8b.png

Figure 196 Decompression from 6-bit to 8-bit

The individual 6-bit to 8-bit decompression for types E3M2 and E2M3 is shown in Figure 197 and Figure 198 respectively.
_images/tcgen05-decompression-6b8b-individual1.png

Figure 197 Individual decompression from 6-bit to 8-bit for E3M2 type
_images/tcgen05-decompression-6b8b-individual2.png

Figure 198 Individual decompression from 6-bit to 8-bit for E2M3 type
9.7.16.9.2. Tensorcore 5th Generation Instructions: tcgen05.cp

tcgen05.cp

Initiates an asynchronous copy operation from shared memory to the Tensor Memory.

Syntax

tcgen05.cp.cta_group.shape{.multicast}{.dst_fmt.src_fmt} [taddr], s-desc;

.cta_group = { .cta_group::1, .cta_group::2 }
.src_fmt   = { .b6x16_p32 , .b4x16_p64 }
.dst_fmt   = { .b8x16 }
.shape     = { .128x256b, .4x256b, .128x128b, .64x128b**, .32x128b*** }
.multicast = { .warpx2::02_13** , .warpx2::01_23**, .warpx4*** }

Description

Instruction tcgen05.cp initiates an asynchronous copy operation from shared memory to the location specified by the address operand taddr in the Tensor Memory.

The 64-bit register operand s-desc is the matrix descriptor which represents the source matrix in the shared memory that needs to be copied. The format of the matrix descriptor is described in Matrix Descriptors.

The .shape qualifier indicates the dimension of data to be copied as described in the Data Movement Shape.

Qualifier .cta_group specifies the number of CTAs whose Tensor Memory is accessed when a single thread of a single CTA executes the tcgen05.cp instruction. When .cta_group::1 is specified, the data is copied into the Tensor Memory of the current CTA. When .cta_group::2 is specified, the data is copied into the Tensor Memory of both the current and the peer CTAs.

All tcgen05 instructions within a kernel must specify the same value for the .cta_group qualifier.

When the qualifiers .dst_fmt and .src_fmt are specified, the data is decompressed from the source format .src_fmt in the shared memory to the destination format .dst_fmt in Tensor Memory by the copy operation. The details of source and the destination formats as specified in the section Optional Decompression.

Some of the .shape qualifiers require certain .multicast qualifiers.

    .64x128b requires .warpx2::02_13 or .warpx2::01_23

    .32x128b requires .warpx4

When the .multicast qualifier is specified as either .warpx2::02_13 or .warpx2::01_23 then the data being copied is multicasted into warp pairs and each warp in the warp pair receive half of the data. Warp pairs are formed as follows:

    .warpx2::02_13 : warps 0 and 2 form a pair; warps 1 and 3 form a pair.

    .warpx2::01_23 : warps 0 and 1 form a pair; warps 2 and 3 form a pair.

When the .multicast modifier is specified as .warpx4 then the data being copied is multicasted into all 4 warps.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

    sm_110f or higher in the same family

Examples

tcgen05.cp.cta_group::1.128x256b                 [taddr0], sdesc0;
tcgen05.cp.cta_group::2.128x128b.b8x16.b6x16_p32 [taddr1], sdesc1;
tcgen05.cp.cta_group::1.64x128b.warpx2::02_13    [taddr2], sdesc2;

9.7.16.9.3. Tensorcore 5th Generation Instructions: tcgen05.shift

tcgen05.shift

Asynchronously shift down the rows of the matrix in the Tensor Memory for a warp.

Syntax

tcgen05.shift.cta_group.down  [taddr];

.cta_group = { .cta_group::1, .cta_group::2 }

Description

Instruction tcgen05.shift is an asynchronous instruction which initiates the shifting of 32-byte elements downwards across all the rows, except the last, by one row. The address operand taddr specifies the base address of the matrix in the Tensor Memory whose rows must be down shifted.

The lane of the address operand taddr must be aligned to 32.

Qualifier .cta_group specifies the number of CTAs whose Tensor Memory is touched when a single thread of a single CTA executes the tcgen05.shift instruction. When .cta_group::1 is specified, the shift operation is performed in the Tensor Memory of the current CTA. When .cta_group::2 is specified, the shift operation is performed in the Tensor Memory of both the current and the peer CTAs.

All tcgen05 instructions within a kernel must specify the same value for the .cta_group qualifier.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    sm_103a

    sm_110a

Examples

tcgen05.shift.down.cta_group::1 [taddr0];
tcgen05.shift.down.cta_group::2 [taddr1];

9.7.16.10. TensorCore 5th Generation Matrix Multiply and accumulate Operations

The 5th generation of TensorCore operations of shape MxNxK perform matrix multiplication and accumulation of the form:

D = A*B+D

where:

    the A matrix has shape MxK, in either Tensor Memory or Shared Memory

    the B matrix has shape KxN, in Shared Memory of the current CTA and optionally in peer CTA

    the D matrix is of the shape MxN, in Tensor Memory

Optionally an input predicate can be used to disable the input from the accumulator matrix and the following operation can be performed as

D = A*B

The matrix multiplication and accumulation operations are categorized into various kinds based on input types and the throughput of the multiplication operation. The following shows the different kinds of MMA operations that are supported:

    f16 : supports f16 and bf16 input types.

    tf32 : supports tf32 input types.

    f8f6f4 : supports all input combinations of f8, f6 and f4 types.

    i8 : supports signed and unsigned 8-bit integer input types.

    mxf8f6f4/mxf4 : supports mx-floating points input types.

    mxf4nvf4 : supports mxf4 type and a custom NVIDIA floating-point type for inputs where the type of the vector elements is 4 bits and requires a common scaling factor to form the complete floating-point type, similar to other mx-types.

Optionally, the 5th generation of TensorCore MMAs support dense and sparse matrix A. Sparse Matrices describes the details of the sparse matrices.

Some of the MMA-kinds requires scaling of input matrices from memory to form the matrix A and matrix B before performing the MMA operation. Block Scaling describes the details of the scaling of matrices.

The following table show the various matrices involved in the MMA operations and the memory in which they can reside:

Matrix Type
	

Memory

A
	

Tensor Memory OR Shared Memory

B
	

Shared Memory

D
	

Tensor Memory

Sparse Meta Data

A-Scale / B-Scale

A sequence of MMA instructions may reuse the same A matrix with a sequence of B matrices or may reuse the same B matrix with a sequence of A matrices. In these patterns the TensorCore may be able to laod the unchanged matrix once and reuse it through the sequence without multiple reloads. The A or B matrices are loaded into a TensorCore collector buffer (i.e., special cache).

An MMA instruction has an optional collector qualifier to specify when an A or B matrix is new to the sequence and should be loaded, unchanged within the sequence and should be reused, or the last use in the sequence and should be discarded. The collector qualifier is used to give the TensorCore permission to reuse a previously loaded A or B matrix; however reuse is opportunistic in that the TensorCore may reload a matrix even when it has permission to reuse that matrix. Thus, the source memory of an A or B matrix must not be modified while the MMA instruction using those matrices has not completed - regardless of collector qualifier permissions.

The 5th generation of TensorCore MMAs can be used for general matrix multiplication OR for convolution operations. In case of convolutions, the activations can be stored in either matrix A or matrix B while the weights will be stored in the other matrix.

Activation Matrix
	

Weights Matrix
	

Name of the op
	

Instruction Name
	

Collector Buffer Applicability

A
	

B
	

Activation Stationary
	

(default tcgen05.mma)
	

Collector buffer is applicable on matrix A

B
	

A
	

Weights Stationary
	

.ws
	

Collector buffer is applicable on matrix B
9.7.16.10.1. Transpose and Negate operations

The matrices A and B can be transposed by specifying the Tranpose A Matrix and Transpose B Matrix bits in the instruction descriptor respectively.

The elements of the matrices A and B can be negated by specifying the Negate A Matrix and Negate B Matrix bits in the instruction descriptor respectively.

The support for Transpose and Negate operation for various MMA-Kind are shown in Table 49.
Table 49 Transpose and Negate operation for various MMA-Kind

MMA-Kind
	

Is Transpose A/B supported
	

Is Negate A/B supported

.kind::tf32
	

Yes
	

Yes

.kind::f16
	

Yes
	

Yes

.kind::f8f6f4
	

Yes
	

Yes

.kind::mxf8f6f4
	

Yes
	

Yes

.kind::i8
	

Yes
	

No

.kind::mxf4
	

No
	

Yes

.kind::mxf4nvf4
	

No
	

Yes

For .kind::tf32, the transpose operations on matrices A and B are supported only with 128B swizzling mode with 32B swizzle-atomicity.

For all other MMA-Kinds, the transpose operations on matrices A and B are not supported on 128B swizzling mode with 32B swizzle-atomicity.

Table 50 shows the valid combinations of N shape with .cta_group qualifier for 8bit transpose B.
Table 50 Various combinations of N shape with .cta_group qualifier for 8bit transpose B

.cta_group
	

N shape

1
	

16 <= N <= 256, step 16

2
	

32 <= N <= 256, step 32
9.7.16.10.2. Matrix Layout Organization

Table 51 describes the major-ness used for different matrices.
Table 51 Major-ness for different matrices

Matrix
	

Residing in Memory
	

Default Major-ness

D
	

Tensor Memory
	

Row-Major

A
	

Tensor Memory

Shared Memory
	

Depends on swizzling mode. Refer Shared Memory Layout and Swizzling

B
	

Shared Memory
9.7.16.10.3. Valid Combinations of Type-Size, Major-ness and Swizzling
Table 52 Valid Combinations of Type-Size, Major-ness and Swizzling

Type-Size
	

Major-ness
	

Matrix
	

Supported Swizzle

4-bit, 6-bit, 8-bit, 16-bit, 32-bit
	

Row
	

A
	

All swizzling modes

Column
	

B

8-bit

16-bit
	

Column (transpose)
	

A
	

All except 128B swizzling with 32B atomicity

Row (transpose)
	

B

32-bit
	

Column (transpose)
	

A
	

Only 128B swizzling with 32B atomicity

Row (transpose)
	

B
9.7.16.10.4. Packing formats of elements in Tensor and Shared memory
9.7.16.10.4.1. Packing format for matrix D in Tensor Memory

The sub-word elements of matrix D are expected not to be packed within a 32-bit Tensor Memory word. For example, if the type of elements of the matrix D is 16 bits then a Tensor Memory word would contain a single 16-bit element in its lower 16 bits.
9.7.16.10.4.2. Packing format for matrix A and B

The 6-bit and 4-bit floating point types have different packing format requirements for different MMA kinds in both Tensor memory and Shared memory. The requirements are as follows.
9.7.16.10.4.3. Packing format used for matrix A by .kind::mxf8f6f4 in Tensor Memory

The individual 4-bit and the 6-bit floating point type elements must be packed in an 8-bit container in Tensor memory as shown below. The 8-bit containers must be contiguously packed in a 32-bit Tensor Memory word. For example, if the type of elements of the matrix A is 6 bits then 4 consecutive A elements should be packed in one 32-bit Tensor Memory word.

    4-bit packing format as shown in Figure 199
    _images/tcgen05-packing-formats-mxf8f6f4-tmem-dig1.png

    Figure 199 4-bit packing format with type E2M1

    6-bit packing format

        Type E3M2 as shown in Figure 200
        _images/tcgen05-packing-formats-mxf8f6f4-tmem-dig2.png

        Figure 200 6-bit packing format with type E3M2

        Type E2M3 as shown in Figure 201
        _images/tcgen05-packing-formats-mxf8f6f4-tmem-dig3.png

        Figure 201 6-bit packing format with type E2M3

9.7.16.10.4.4. Packing format used for matrix A and B by .kind::mxf8f6f4 in Shared Memory

The 4-bit and 6-bit floating point elements in shared memory must be contiguously packed along with padding as follows.

    4-bit packing format as shown in Figure 202
    _images/tcgen05-packing-formats-mxf8f6f4-smem-dig1.png

    Figure 202 4-bit packing format

    6-bit packing format as shown in Figure 203

    _images/tcgen05-packing-formats-mxf8f6f4-smem-dig2.png

    Figure 203 6-bit packing format

9.7.16.10.4.5. Packing format used for matrix A by .kind::mxf4 and .kind::mxf4nvf4 in Tensor Memory

Two 4-bit floating point type elements must be packed in an 8-bit container in Tensor memory as shown in Figure 204 for mxf4.
_images/tcgen05-packing-formats-mxf4-tmem-dig1.png

Figure 204 4-bit packing format with type E2M1
9.7.16.10.4.6. Packing format used for matrix A and B by .kind::mxf4 and .kind::mxf4nvf4 in Shared Memory

The packing format for 4-bit floating point elements in shared memory is to pack two 4-bit elements in a 8-bit container, with no padding.
9.7.16.10.5. Data Path Layout Organization

Different MMA variants access the tensor memory with different layout organization. The following table lists the various layouts:

M
	

cta_group
	

A-Sparsity
	

Is .ws mode
	

Datapath organization
	

Layout ID
	

Tensor Memory Datapath Lane Alignment

32
	

::1
	

Either
	

Yes
	

1x4
	

Layout G
	

0

64
	

::1
	

Either
	

Yes
	

2x3
	

Layout E
	

0

64
	

::1
	

Either
	

No
	

4x1 (1/2 datapath utilized)
	

Layout F
	

0 or 16

128
	

::1
	

Either
	

Either
	

4x1
	

Layout D
	

0

128
	

::2
	

Dense
	

N/A
	

2x2
	

Layout B
	

0

128
	

::2
	

Sparse
	

N/A
	

4x1 (1/2 datapath utilized)
	

Layout C
	

0 or 16

256
	

::2
	

Either
	

N/A
	

4x1
	

Layout A
	

0

The layouts which utilize only half the datapath lanes, i.e., Layout F and Layout C, must use the same Tensor Memory lane alignment across matrices A, D and the sparsity metadata matrix.

The following shows the warps that can access the Tensor Memory regions via tcgen05.ld / tcgen05.st along with the addresses for various Tensor Memory Layouts.
9.7.16.10.5.1. Layout A (M = 256)

Layout organization for M = 256 is shown in Figure 205.
_images/tcgen05-data-path-layout-a1.png

Figure 205 Layout organization for M = 256

Addresses for the above region to be used in tcgen05.ld / tcgen05.st is shown in Figure 206
_images/tcgen05-data-path-layout-a2.png

Figure 206 Addresses to use in tcgen05.ld / tcgen05.st
9.7.16.10.5.2. Layout B (M = 128 + cta-group::2 + Dense A matrix)

Layout organization for M = 128 + .cta_group::2 + Dense A matrix is shown in Figure 207.
_images/tcgen05-data-path-layout-b1.png

Figure 207 Layout organization for M = 128 + .cta_group::2 + Dense A matrix

Addresses for the above region to be used in tcgen05.ld / tcgen05.st is shown in Figure 208
_images/tcgen05-data-path-layout-b2.png

Figure 208 Addresses to use in tcgen05.ld / tcgen05.st
9.7.16.10.5.3. Layout C (M = 128 + cta-group::2 + Sparse A matrix)

Layout organization for M = 128 + .cta_group::2 + Sparse A matrix is shown in Figure 209.
_images/tcgen05-data-path-layout-c1.png

Figure 209 Layout organization for M = 128 + .cta_group::2 + Sparse A matrix

Addresses for the above region to be used in tcgen05.ld / tcgen05.st is shown in Figure 210
_images/tcgen05-data-path-layout-c2.png

Figure 210 Addresses to use in tcgen05.ld / tcgen05.st
9.7.16.10.5.4. Layout D (M = 128 + cta-group::1)

Layout organization for M = 128 + .cta_group::1 is shown in Figure 211.
_images/tcgen05-data-path-layout-d1.png

Figure 211 Layout organization for M = 128 + .cta_group::1

Addresses for the above region to be used in tcgen05.ld / tcgen05.st is shown in Figure 212
_images/tcgen05-data-path-layout-d2.png

Figure 212 Addresses to use in tcgen05.ld / tcgen05.st
9.7.16.10.5.5. Layout E (M = 64 + .ws mode)

Layout organization for M = 64 + .ws mode is shown in Figure 213.
_images/tcgen05-data-path-layout-e1.png

Figure 213 Layout organization for M = 64 + .ws mode

Addresses for the above region to be used in tcgen05.ld / tcgen05.st is shown in Figure 214
_images/tcgen05-data-path-layout-e2.png

Figure 214 Addresses to use in tcgen05.ld / tcgen05.st
9.7.16.10.5.6. Layout F (M = 64 + non .ws mode)

Layout organization for M = 64 + non .ws mode is shown in Figure 215.
_images/tcgen05-data-path-layout-f1.png

Figure 215 Layout organization for M = 64 + non .ws mode

Addresses for the above region to be used in tcgen05.ld / tcgen05.st is shown in Figure 216
_images/tcgen05-data-path-layout-f2.png

Figure 216 Addresses to use in tcgen05.ld / tcgen05.st
9.7.16.10.5.7. Layout G (M = 32)

Layout organization for M = 32 is shown in Figure 217.
_images/tcgen05-data-path-layout-g1.png

Figure 217 Layout organization for M = 32

Addresses for the above region to be used in tcgen05.ld / tcgen05.st is shown in Figure 218
_images/tcgen05-data-path-layout-g2.png

Figure 218 Addresses to use in tcgen05.ld / tcgen05.st
9.7.16.10.6. Shared Memory Layout and Swizzling

If the bit Transpose A Matrix / Transpose B Matrix in the Instruction descriptor is 0, then K-major is used for matrix A / B respectively. If the bit Transpose A Matrix in the Instruction descriptor is 1 then M-major is used for matrix A. If the bit Transpose B Matrix in the Instruction descriptor is 1, then N-major is used for matrix B.

In a column-major default BLAS library such as cuBLAS, the matrices A and B with and without transpose can be classified as either K-Major or M-or-N-Major as shown in the following table:
	

Non-Transposed
	

Transposed

A
	

K-major
	

M-major

B
	

K-major
	

N-major

To avoid confusion with A, B, row-major, col-major, transpose, and non-transpose, we will use MN-Major and K-Major throughout this section.

The matrices in the shared memory are made up of one or more “swizzle layout atom”. The exact layout of these swizzle atoms depends on the swizzling mode, swizzle-atomicity, and the leading dimension. The layout of the swizzle are shown in Table 53
Table 53 Layout for swizzle atoms

Swizzling mode and Swizzle-Atomicity
	

Leading Dimension
	

Swizzle atom layout (128b element)

128B Swizzling with 32B atomicity
	

M/N
	

8x4

–
	

–

128B Swizzling with 16B atomicity
	

M/N
	

8x8

K
	

8x8

64B Swizzling Mode
	

M/N
	

4x8

K
	

8x4

32B Swizzling Mode
	

M/N
	

2x8

K
	

8x2

None
	

M/N
	

1x8

K
	

8x1

The above shapes are for elements of size 128 bits. For smaller element sizes, the same shapes would get multiplied along the leading dimension by a factor of 128 / sizeof_bits(Element). For example, 128B MN major swizzle atom would have a shape of (8*(128/32))x8 = 32x8 for tf32 tensor core inputs.

Some example Layouts of MxK or KxN matrices with various swizzling modes, and are in units of 128b elements as shown by each colored cell as shown in Figure 219, Figure 220, Figure 221, Figure 222, Figure 223, Figure 224, Figure 225, Figure 226, Figure 227.
_images/tcgen05-smem-layout-128B-32B-atom-mn.png

Figure 219 MN major 128B swizzling with 32B atomicity
_images/tcgen05-smem-layout-128B-mn.png

Figure 220 MN major 128B swizzling
_images/tcgen05-smem-layout-128B-k.png

Figure 221 K major 128B swizzling
_images/tcgen05-smem-layout-64B-mn.png

Figure 222 MN major 64B swizzling
_images/tcgen05-smem-layout-64B-k.png

Figure 223 K major 64B swizzling
_images/tcgen05-smem-layout-32B-mn.png

Figure 224 MN major 32B swizzling
_images/tcgen05-smem-layout-32B-k.png

Figure 225 K major 32B swizzling
_images/tcgen05-smem-layout-no-swizzle-mn.png

Figure 226 MN major no-swizzling mode
_images/tcgen05-smem-layout-no-swizzle-k.png

Figure 227 K major no-swizzling mode

Following are some of the examples of the 128B swizzling layout for tf32 element type.

    K-Major: Figure 228

        _images/tcgen05-smem-layout-k.png

        Figure 228 K major

    MN-Major: Figure 229

        _images/tcgen05-smem-layout-mn.png

        Figure 229 MN major

9.7.16.10.7. Block Scaling

The tcgen05.mma instructions with the following .kind qualifier:

    .kind::mxf8f6f4

    .kind::mxf4

    .kind::mxf4nvf4

perform matrix multiplication with block scaling. This operation has the following form:

(A * scale_A)  * (B * scale_B) + D

where scale_A and scale_B are matrices residing in Tensor Memory.

For a scale_A matrix of shape M x SFA_N, each row of matrix A is divided into SFA_N number of chunks and each chunk of a row is multiplied with the corresponding element in the SF_A of the same row.

Similarly, for a scale_B matrix of shape SFB_M x N, each column of matrix B is divided into the SFB_M number of chunks and each chunk of a column is multiplied with the corresponding element in the SF_B of the same column.

Scale factors for A and B matrices need to be duplicated to all 32 lane partitions of tensor memory.

Figure 230 shows an example of tcgen05.mma with block scaling of scale_vec::2X.
_images/tcgen05-mma-block-scaling.png

Figure 230 tcgen05.mma with block scaling of scale_vec::2X
9.7.16.10.7.1. Valid combinations of scale_vectorsize with types and MMA-Kind

The shape of scale_A and scale_B matrices depend on the .scale_vectorsize as shown in Table 54.
Table 54 Valid combinations of scale_vectorsize and shapes

.scale_vectorsize
	

.kind::*
	

K
	

Shape of scale_A
	

Shape of scale_B

.scale_vec::1X
	

.kind::mxf8f6f4
	

All supported values of K
	

M x 1
	

1 x N

.scale_vec::2X
	

.kind::mxf4, .kind::mxf4nvf4
	

All supported values of K
	

M x 2
	

2 x N

.scale_vec::4X
	

.kind::mxf4nvf4
	

All supported values of K
	

M x 4
	

4 x N

.block16
	

.kind::mxf4nvf4
	

K = 96
	

M x 6
	

6 x N

All supported values of K except 96
	

M x 4
	

4 x N

.block32
	

.kind::mxf4, .kind::mxf4nvf4
	

K = 96
	

M x 3
	

3 x N

All supported values of K except 96
	

M x 2
	

2 x N

.kind::mxf8f6f4
	

All supported values of K
	

M x 1
	

1 x N

The valid combination of the exact element types and the .scale_vectorsize are listed in Table 55.
Table 55 Valid combinations of scale_vectorsize with types and MMA-Kind

.kind::*
	

Element Data Type
	

Scale Data Type
	

.scale_vectorsize

.kind::mxf8f6f4
	

E4M3, E5M2, E2M3 E3M2, E2M1
	

UE8M0
	

.scale_vec::1X / .block32

.kind::mxf4
	

E2M1
	

UE8M0
	

.scale_vec::2X / .block32

.kind::mxf4nvf4
	

E2M1
	

UE8M0
	

.scale_vec::2X / .block32, .scale_vec::4X / .block16

E2M1
	

UE4M3
	

.scale_vec::4X / .block16

New .blockN qualifiers are aliases for .scale_vec::NX qualifiers as:

    .block32 is alias for .scale_vec::1X or .scale_vec::2X based on .kind and K dimension

    .block16 is alias for .scale_vec::4X

9.7.16.10.7.2. Scale Factor A ID

The value of the scale factor A ID selects the sub-columns in the Tensor Memory to form the scale factor A matrix, which is used to scale the matrix A.

The following shows the scale factor matrix layout for various scale vector sizes:
9.7.16.10.7.2.1. Layout of the Scale Factor A Matrix for scale_vec::1X/block32 with K=32/K=64

There is one scale factor per row of the A matrix with block size as 32 and the scale factor must be provided in 1-byte aligned sub-column of the Tensor Memory. SFA_ID specifies the byte offset in the Tensor Memory word that must be used for the scale factor matrix. Figure 231 shows which sub-columns get selected for different values of SFA_ID.
_images/tcgen05-mma-scale-factor-a-1x-dig.png

Figure 231 Layout of scale factor A matrix with scale_vec::1X/block32 with K=32/K=64

For example, if SFA_ID is 0, then all the green columns are selected to form the scale factor matrix. Similarly, SFA_ID values of 1, 2 and 3 would select the blue, yellow, and red columns, respectively.
9.7.16.10.7.2.2. Layout of the Scale Factor A Matrix for scale_vec::2X/block32 with K=64/K=128

There are two scale factors per row of the A matrix with block size as 32 and the scale factor must be provided in 2-byte aligned sub-column of the Tensor Memory. SFA_ID specifies the half word offset in the Tensor Memory word that must be used for the scale factor matrix. Figure 232 shows which sub-columns gets selected for different values of SFA_ID.
_images/tcgen05-mma-scale-factor-a-2x-dig.png

Figure 232 Layout of scale factor A matrix with scale_vec::2X/block32 with K=64/K=128

For example, if SFA_ID is 0, then all the green columns are selected to form the scale factor matrix. Similarly, if SFA_ID is 2, then all of the blue columns are selected to form the scale factor matrix.
9.7.16.10.7.2.3. Layout of the Scale Factor A Matrix for scale_vec::4X/block16 with K=64/K=128

There are four scale factors per row of the A matrix with block size as 16 and the scale factor must be provided in 4-byte aligned sub-column of the Tensor Memory. The SFA_ID value must be 0 and this specifies that all of the columns (in green) will be used for the scale factor matrix. Figure 233 shows which sub-columns gets selected for different values of SFA_ID.
_images/tcgen05-mma-scale-factor-a-4x-dig.png

Figure 233 Layout of scale factor A matrix with scale_vec::4X/block16 with K=64/K=128
9.7.16.10.7.2.4. Layout of the Scale Factor A Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)

There are three scale factors per row of the A matrix with block size as 32 and the scale factor must be provided in 4-byte aligned sub-column of the Tensor Memory. SFA_ID specifies the byte offset in the Tensor Memory word that must be used for the scale factor matrix. Figure 234, Figure 235, Figure 236 and Figure 237 show which sub-columns get selected for different values of SFA_ID.
_images/tcgen05-mma-scale-factor-a-block32-k96-dig1.png

Figure 234 Layout of scale factor A matrix with block32 with K=96 with SFA_ID=00
_images/tcgen05-mma-scale-factor-a-block32-k96-dig2.png

Figure 235 Layout of scale factor A matrix with block32 with K=96 with SFA_ID=01
_images/tcgen05-mma-scale-factor-a-block32-k96-dig3.png

Figure 236 Layout of scale factor A matrix with block32 with K=96 with SFA_ID=10
_images/tcgen05-mma-scale-factor-a-block32-k96-dig4.png

Figure 237 Layout of scale factor A matrix with block32 with K=96 with SFA_ID=11

For example, if SFA_ID is 0, then all the green columns are selected to form the scale factor matrix. Similarly, SFA_ID values of 1, 2 and 3 would select the blue, yellow, and red columns, respectively.
9.7.16.10.7.2.5. Layout of the Scale Factor A Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)

There are six scale factors per row of the A matrix with block size as 16 and the scale factor must be provided in 4-byte aligned sub-column of the Tensor Memory. SFA_ID specifies the byte offset in the Tensor Memory word that must be used for the scale factor matrix. Figure 238 and Figure 239 show which sub-columns get selected for different values of SFA_ID.
_images/tcgen05-mma-scale-factor-a-block16-k96-dig1.png

Figure 238 Layout of scale factor A matrix with block16 with K=96 with SFA_ID=00
_images/tcgen05-mma-scale-factor-a-block16-k96-dig2.png

Figure 239 Layout of scale factor A matrix with block16 with K=96 with SFA_ID=10

For example, if SFA_ID is 0, then all the green columns are selected to form the scale factor matrix. Similarly, if SFA_ID is 2, then all of the blue columns are selected to form the scale factor matrix.
9.7.16.10.7.3. Scale Factor B ID

The value of the scale factor B ID selects the sub-columns in the Tensor Memory to form the scale factor B matrix, which is used to scale the matrix B.

The following shows the scale factor matrix layout for various scale vector sizes:
9.7.16.10.7.3.1. Layout of the Scale Factor B Matrix for scale_vec::1X/block32 with K=32/K=64

There is one scale factor per row of the B matrix with block size as 32 and the scale factor must be provided in 1-byte aligned sub-column of the Tensor Memory. SFB_ID specifies the byte offset in the Tensor Memory word that must be used for the scale factor matrix. Figure 240 shows which sub-columns get selected for different values of SFB_ID.
_images/tcgen05-mma-scale-factor-b-1x-dig.png

Figure 240 Layout of scale factor B matrix with scale_vec::1X/block32 with K=32/K=64

For example, if SFB_ID is 0, then all the green columns are selected to form the scale factor matrix. Similarly, SFB_ID values of 1, 2 and 3 would select the blue, yellow, and red columns, respectively.
9.7.16.10.7.3.2. Layout of the Scale Factor B Matrix for scale_vec::2X/block32 with K=64/K=128

There are two scale factors per row of the B matrix with block size as 32 and the scale factor must be provided in 2-byte aligned sub-column of the Tensor Memory. SFB_ID specifies the half word offset in the Tensor Memory word that must be used for the scale factor matrix. Figure 241 shows which sub-columns get selected for different values of SFB_ID.
_images/tcgen05-mma-scale-factor-b-2x-dig.png

Figure 241 Layout of scale factor B matrix with scale_vec::2X/block32 with K=64/K=128

For example, if SFB_ID is 0, then all the green columns are selected to form the scale factor matrix. Similarly, if SFB_ID is 2, then all of the blue columns are selected to form the scale factor matrix.
9.7.16.10.7.3.3. Layout of the Scale Factor B Matrix for scale_vec::4X/block16 with K=64/K=128

There are four scale factors per row of the B matrix with block size as 16 and the scale factor must be provided in 4-byte aligned sub-column of the Tensor Memory. The SFB_ID value must be 0 and this specifies that all of the columns (in green) will be used for the scale factor matrix. Figure 242 shows which sub-columns get selected for different values of SFB_ID.
_images/tcgen05-mma-scale-factor-b-4x-dig.png

Figure 242 Layout of scale factor B matrix with scale_vec::4X/block16 with K=64/K=128
9.7.16.10.7.3.4. Layout of the Scale Factor B Matrix for block32 with K=96 (Semantically equivalent to scale_vec::3X)

There are three scale factors per row of the B matrix with block size as 32 and the scale factor must be provided in 4-byte aligned sub-column of the Tensor Memory. SFB_ID specifies the byte offset in the Tensor Memory word that must be used for the scale factor matrix.

For N<=128, Figure 243, Figure 244, Figure 245 and Figure 246 show which sub-columns get selected for different values of SFB_ID.
_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig1.png

Figure 243 Layout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=00
_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig2.png

Figure 244 Layout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=01
_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig3.png

Figure 245 Layout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=10
_images/tcgen05-mma-scale-factor-b-block32-k96-nlt128-dig4.png

Figure 246 Layout of scale factor B matrix with block32 with K=96 and N<=128 with SFA_ID=11

For N>128, Figure 247, Figure 248, Figure 249, Figure 250, Figure 251 and Figure 252 show which sub-columns get selected for different values of SFB_ID.
_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig1.png

Figure 247 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=00
_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig2.png

Figure 248 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=01
_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig3.png

Figure 249 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=10
_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig4.png

Figure 250 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=10
_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig5.png

Figure 251 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=11
_images/tcgen05-mma-scale-factor-b-block32-k96-ngt128-dig6.png

Figure 252 Layout of scale factor B matrix with block32 with K=96 and N>128 with SFA_ID=11

For example, if SFB_ID is 0, then all the green columns are selected to form the scale factor matrix. Similarly, SFB_ID values of 1, 2 and 3 would select the blue, yellow, and red columns, respectively.
9.7.16.10.7.3.5. Layout of the Scale Factor B Matrix for block16 with K=96 (Semantically equivalent to scale_vec::6X)

There are six scale factors per row of the B matrix with block size as 16 and the scale factor must be provided in 4-byte aligned sub-column of the Tensor Memory. SFB_ID specifies the byte offset in the Tensor Memory word that must be used for the scale factor matrix.

For N<=128, Figure 253 and Figure 254 show which sub-columns get selected for different values of SFB_ID.
_images/tcgen05-mma-scale-factor-b-block16-k96-nlt128-dig1.png

Figure 253 Layout of scale factor B matrix with block16 with K=96 and N<=128 with SFA_ID=00
_images/tcgen05-mma-scale-factor-b-block16-k96-nlt128-dig2.png

Figure 254 Layout of scale factor B matrix with block16 with K=96 and N<=128 with SFA_ID=10

For N>128, Figure 255, Figure 256, Figure 257 and Figure 258 show which sub-columns get selected for different values of SFB_ID.
_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig1.png

Figure 255 Layout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=00
_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig2.png

Figure 256 Layout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=00
_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig3.png

Figure 257 Layout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=10
_images/tcgen05-mma-scale-factor-b-block16-k96-ngt128-dig4.png

Figure 258 Layout of scale factor B matrix with block16 with K=96 and N>128 with SFA_ID=10

For example, if SFB_ID is 0, then all the green columns are selected to form the scale factor matrix. Similarly, if SFB_ID is 2, then all of the blue columns are selected to form the scale factor matrix.
9.7.16.10.8. Sparse Matrices

This instruction tcgen05.mma.sp can be used when the matrix A is a structured sparse matrix with 50% zeros in each row distributed as per its sparse granularity.

In a MxNxK sparse tcgen05.mma.sp operation, the matrix A of shape MxK is stored in a packed form as Mx(K/2) in memory. For each K-wide row of matrix A, 50% of elements are zeros and the remaining K/2 non-zero elements are stored in memory. The metadata specifies the mapping of the K/2 non-zero elements to the K elements before performing the MMA operation.

Granularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a sub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the sub-chunk is shape-specific. The following table lists the granularity of different tcgen05.mma.sp variants:

.kind of tcgen05.mma
	

Sparse Granularity

.kind::tf32
	

1:2

.kind::f16
	

2:4

.kind::f8f6f4

.kind::mxf8f6f4

.kind::i8

.kind::mxf4
	

4:8 (in pairs)
9.7.16.10.8.1. Sparse tcgen05.mma.sp with .kind::tf32

For .kind::tf32, matrix A is structured sparse at a granularity of 1:2. In other words, each chunk of two adjacent elements in a row of matrix A has one zero and one non-zero element. Only the non-zero element is stored in memory and the 4-bit index in the metadata indicates the position of the non-zero element in the two-wide chunk. The only meaningful values of the index are:

    0b1110

    0b0100

Rest of the values result in undefined behavior.
_images/tcgen05-sparse-mma-metadata-tf32.png

Figure 259 Sparse tcgen05.mma metadata example for tf32 kind
9.7.16.10.8.2. Sparse tcgen05.mma.sp with .kind::f16, .kind::f8f6f4, .kind::mxf8f6f4, .kind::i8

For the following .kind variants of tcgen05.mma:

    .kind::f16

    .kind::f8f8f4

    .kind::mxf8f6f4

    .kind::i8

matrix A is structured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A has two zero and two non-zero elements. Only the non-zero elements are stored in memory and the two 2-bit indices in the metadata indicates the position of the two non-zero elements in the four-wide chunk. The only meaningful values of the index are:

    0b0100

    0b1000

    0b1100

    0b1001

    0b1101

    0b0110

    0b1110

_images/tcgen05-sparse-mma-metadata-f16-f8f6f4-mxf8f6f4.png

Figure 260 Sparse tcgen05.mma metadata example for f16/f8f6f4/mxf8f6f4 kind
9.7.16.10.8.3. Sparse tcgen05.mma.sp with .kind::mxf4 and .kind::mxf4nvf4

For .kind::mxf4 and .kind::mxf4nvf4, matrix A is pair-wise structured sparse at a granularity of 4:8. In other words, each chunk of eight adjacent elements in a row of matrix A has four zero and four non-zero elements. The zero and non-zero elements are clustered in sub-chunks of two elements each within the eight-wide chunk, so each two-wide sub-chunk within the eight-wide chunk must be all zeros or all non-zeros. Only the four non-zero elements are stored in memory and the two 2-bit indices in the metadata indicates the position of the two two-wide sub-chunks with non-zero values in the eight-wide chunk of a row of matrix A. The only meaningful values of the index are:

    0b0100

    0b1000

    0b1100

    0b1001

    0b1101

    0b0110

    0b1110

Rest of the values result in undefined behavior.
_images/tcgen05-sparse-mma-metadata-mxf4.png

Figure 261 Sparse tcgen05.mma metadata example for mxf4 kind
9.7.16.10.8.4. Sparsity selector

The value of the sparsity selector selects the sub-columns in the Tensor Memory to form the sparsity metadata matrix, which is used with matrix A to form the multiplicand matrix.

The following shows the sparse metadata matrix layout in Tensor Memory for various MMA variants:
9.7.16.10.8.4.1. Layout of the Sparsity Metadata Matrix for M = 64 for .kind::f16

Figure 262 shows which sub-columns gets selected for different values of Sparsity Selector.
_images/tcgen05-sparse-matrices-sparsity-selector-kind-f16-m64.png

Figure 262 Sparsity Metadata Layout for M = 64 for .kind::f16
9.7.16.10.8.4.2. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for .kind::f16

Figure 263 shows which sub-columns gets selected for different values of Sparsity Selector.
_images/tcgen05-sparse-matrices-sparsity-selector-kind-f16-m128-256.png

Figure 263 Sparsity Metadata Layout for M = 128 / M = 256 for .kind::f16
9.7.16.10.8.4.3. Layout of the Sparsity Metadata Matrix for M = 64 for .kind::tf32

Figure 264 shows which sub-columns gets selected for different values of Sparsity Selector.
_images/tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m64.png

Figure 264 Sparsity Metadata Layout for M = 64 for .kind::tf32
9.7.16.10.8.4.4. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for .kind::tf32

Figure 265 shows which sub-columns gets selected for different values of Sparsity Selector.
_images/tcgen05-sparse-matrices-sparsity-selector-kind-tf32-m128-256.png

Figure 265 Sparsity Metadata Layout for M = 128 / M = 256 for .kind::tf32
9.7.16.10.8.4.5. Layout of the Sparsity Metadata Matrix for M = 64 for .kind::f8f6f4, .kind::mxf8f6f4, .kind::i8, .kind::mxf4, .kind::mxf4nvf4

The value of the sparsity selector:

    must be 0 for .kind::i8 and .kind::f8f6f4

    is assumed to be 0 for .kind::mxf8f6f4, .kind::mxf4 and .kind::mxf4nvf4

and all of the columns are selected as shown in Figure 266
_images/tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m64.png

Figure 266 Sparsity Metadata Layout for M = 64 for .kind::f8f6f4, .kind::mxf8f6f4, .kind::i8, .kind::mxf4, .kind::mxf4nvf4
9.7.16.10.8.4.6. Layout of the Sparsity Metadata Matrix for M = 128 / M = 256 for .kind::f8f6f4, .kind::mxf8f6f4, .kind::i8, .kind::mxf4, .kind::mxf4nvf4

The value of the sparsity selector:

    must be 0 for .kind::i8 and .kind::f8f6f4

    is assumed to be 0 for .kind::mxf8f6f4, .kind::mxf4 and .kind::mxf4nvf4

and all of the columns are selected as shown in Figure 267
_images/tcgen05-sparse-matrices-sparsity-selector-kind-f8f6f4-mxf8f6f4-m128-256.png

Figure 267 Sparsity Metadata Layout for M = 128 / M = 256 for .kind::f8f6f4, .kind::mxf8f6f4, .kind::i8, .kind::mxf4, .kind::mxf4nvf4
9.7.16.10.8.5. Alignment restriction

The layouts which utilize only half the datapath lanes as specified in Data Path Layout Organization, i.e. Layout F and Layout C, must use the same alignment across matrices A, D and the sparsity metadata matrix.
9.7.16.10.9. TensorCore 5th Generation of MMA Instructions
9.7.16.10.9.1. TensorCore 5th Generation Instructions: tcgen05.mma

tcgen05.mma

Perform the 5th generation of matrix multiply and accumulate operation.

Syntax

// 1. Floating-point type without block scaling:

tcgen05.mma.cta_group.kind   [d-tmem],  a-desc,  b-desc, idesc,
                             { disable-output-lane }, enable-input-d {, scale-input-d};

tcgen05.mma.cta_group.kind   [d-tmem], [a-tmem], b-desc, idesc,
                             { disable-output-lane }, enable-input-d {, scale-input-d};

.kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4 }
.cta_group = { .cta_group::1, .cta_group::2 }

----------------------------------------------------------------------------------

// 2. Floating-point type with block scaling:

tcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}
                                        [d-tmem],  a-desc,  b-desc, idesc,
                                        [scale-A-tmem], [scale-B-tmem], enable-input-d;

tcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}
                                        [d-tmem], [a-tmem], b-desc, idesc,
                                        [scale-A-tmem], [scale-B-tmem], enable-input-d;

.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }
.cta_group      = { .cta_group::1,   .cta_group::2 }
.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }

----------------------------------------------------------------------------------

// 3. Convolution MMA for floating-point type without block scaling:

tcgen05.mma.cta_group.kind.collector_usage [d-tmem],  a-desc,  b-desc, idesc,
                                           { disable-output-lane }, enable-input-d {, scale-input-d};

tcgen05.mma.cta_group.kind{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc, idesc,
                                                    { disable-output-lane }, enable-input-d {, scale-input-d};

tcgen05.mma.cta_group.kind.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,
                                                    { disable-output-lane }, enable-input-d {, scale-input-d};

.kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4 }
.cta_group = { .cta_group::1,   .cta_group::2 }
.collector_usage = { .collector::buffer::op }
::buffer         = { ::a }
::op             = { ::fill, ::use, ::lastuse, ::discard* }

----------------------------------------------------------------------------------

// 4. Activation Stationary MMA for floating-point type with block scaling:

tcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage
                                            [d-tmem],  a-desc,  b-desc, idesc,
                                            [scale-A-tmem], [scale-B-tmem], enable-input-d;

tcgen05.mma.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage
                                            [d-tmem], [a-tmem], b-desc, idesc,
                                            [scale-A-tmem], [scale-B-tmem], enable-input-d;

.cta_group       = { .cta_group::1,   .cta_group::2 }
.scale_vectorsize  = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }
.kind            = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }
.collector_usage = { .collector::buffer::op }
::buffer         = { ::a }
::op             = { ::fill, ::use, ::lastuse, ::discard* }

----------------------------------------------------------------------------------

// 5. Integer type:

tcgen05.mma.cta_group.kind::i8  [d-tmem],  a-desc,  b-desc, idesc,
                                { disable-output-lane }, enable-input-d;

tcgen05.mma.cta_group.kind::i8  [d-tmem], [a-tmem], b-desc, idesc,
                                { disable-output-lane }, enable-input-d;

.cta_group = { .cta_group::1,   .cta_group::2  }

----------------------------------------------------------------------------------

// 6. Convolution MMA for integer type:

tcgen05.mma.cta_group.kind::i8.collector_usage          [d-tmem],  a-desc,  b-desc, idesc,
                                                        { disable-output-lane }, enable-input-d;

tcgen05.mma.cta_group.kind::i8.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,
                                                        { disable-output-lane }, enable-input-d;

tcgen05.mma.cta_group.kind::i8{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc, idesc,
                                                        { disable-output-lane }, enable-input-d;

.cta_group       = { .cta_group::1,   .cta_group::2  }
.collector_usage = { .collector::buffer::op }
::buffer         = { ::a }
::op             = { ::fill, ::use, ::lastuse, ::discard* }

Description

Instruction tcgen05.mma is an asynchronous instruction which initiates an MxNxK matrix multiply and accumulate operation, D = A*B+D where the A matrix is MxK, the B matrix is KxN, and the D matrix is MxN.

The operation of the form D = A*B is issued when the input predicate argument enable-input-d is false.

The optional immediate argument scale-input-d can be specified to scale the input matrix D as follows: D = A*B+D * (2 ^ - scale-input-d)

The valid range of values for argument scale-input-d is [0, 15]. The argument scale-input-d is only valid for .kind::tf32 and .kind::f16.

The 32-bit register operand idesc is the instruction descriptor as described in Instruction descriptor, specifies the shapes, exact types, sparsity and other details of the input matrices, output matrix and the matrix multiply and accumulate operation.

The qualifier .cta_group::1 specifies that the matrix multiply and accumulate operation is performed on the Tensor Memory of the executing thread’s CTA only. The qualifier .cta_group::2 specifies that the matrix multiply and accumulate operation is performed on the Tensor Memory of the executing thread’s CTA and its peer CTA.

All tcgen05 instructions within a kernel must specify the same value for the .cta_group qualifier.

The instruction tcgen05.mma has single thread semantics, unlike the collective instructions mma.sync or wgmma.mma_async. So, a single thread issuing the tcgen05.mma will result in the initiation of the whole matrix multiply and accumulate operation. Refer to the section Issue Granularity.

The qualifier .kind specifies the general kind of the element types of the multiplicand matrices. The exact types of the elements of the input and output matrices for each MMA-kind are specified in the Instruction descriptor.

The address operand d-tmem specifies the address of the destination and the accumulation matrix D in the Tensor Memory. The address operand a-tmem specifies the address of the matrix A in the Tensor Memory. The 64-bit register operand a-desc and b-desc are the matrix descriptors which represent the matrices A and B in shared memory respectively. The format of the matrix descriptor is described in Matrix Descriptors.

The vector operand disable-output-lane specifies the lane(s) in the Tensor Memory that should be not be updated with the resultant matrix D. Elements of the vector operand disable-output-lane forms a mask where each bit corresponds to a lane of the Tensor Memory, with least significant bit of the first element of the vector (leftmost in syntax) corresponding to the lane 0 of the Tensor Memory. If a bit in the mask is 1, then the corresponding lane in the Tensor Memory for the resultant matrix D will not be updated. The size of the vector is as follows:

.cta_group
	

Size of the vector disable-output-lane

::1
	

4

::2
	

8

Qualifier .block_scale specifies that the matrices A and B are scaled with scale_A and scale_B matrices respectively before performing the matrix multiply and accumulate operation as specified in the section Block Scaling. The address operand scale-A-tmem and scale-B-tmem specify the base address the matrices scale_A and scale_B respectively in the Tensor Memory.

For qualifier .scale_vectorsize,

    If .scale_vec::NX is specified: N specifies the number of columns in scale_A matrix and number of rows in scale_B matrix.

    If .blockN is specified: N specifies the block size for which single scale factor will be applied. In this form, value of N is same as the K-dimension / (N of .scale_vec::NX).

Aliased .scale_vectorsize variants:

    .block16 is aliased with:

        .scale_vec::4X when .kind = .kind::mxf4nvf4 and K = 64 or 128

    .block32 is aliased with:

        .scale_vec::1X when .kind = .kind::mxf8f6f4 for all supported values of K

        .scale_vec::2X when .kind = .kind::mxf4 or .kind::mxf4nvf4 and K = 64 or 128

The valid combinations of MMA-kind and .scale_vectorsize are described in Table 54. For .kind::mxf4 when the qualifier .scale_vectorsize is not specified, then it defaults to .block32. For .kind::mxf4nvf4, the qualifier .scale_vectorsize must be explicitly specified.

The qualifier .ashift shifts the rows of the A matrix down by one row, except for the last row in the Tensor Memory. Qualifier .ashift is only allowed with M = 128 or M = 256.

The qualifier .collector_usage specifies the usage of collector buffer for matrix A. Following collector buffer operations can be specified:

.collector_usage
	

Semantics

.collector::a::fill
	

Specifies that the A matrix read from the memory should be filled in collector buffer.

.collector::a::use
	

Specifies that the A matrix can be read from the collector buffer. This requires a previous fill to the collector buffer to be still valid.

.collector::a::lastuse
	

Specifies that the A matrix can be read from the collector buffer and the contents of the collector buffer can be discarded. This requires a previous fill to the collector buffer to be valid till the collector buffer is read.

.collector::a::discard
	

Specifies that the contents of the collector buffer for A can be discarded.

If no .collector_usage qualifier is specified, then it defaults to .collector::a::discard. It is illegal to specify either of .collector::a::use or .collector::a::fill along with .ashift.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Qualifier .kind::mxf4nvf4 introduced in PTX ISA version 8.7.

Qualifiers .block16 and .block32 introduced in PTX ISA version 8.8.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8 except .kind::i8:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

    sm_110f or higher in the same family

Qualifier .kind::i8 is supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    sm_110a

Argument scale-input-d requires sm_100a and is supported on sm_100f or higher in the same family from PTX ISA version 8.8.

For .scale_vectorsize,

    .scale_vec::1X, .scale_vec::2X, .scale_vec::4X requires sm_100a.

    .block16, .block32 requires sm_100f or sm_110f.

For Target ISA details on matrix shape, check Target ISA Note.

For Target ISA details on shared memory descriptor, check Target ISA Note.

Examples

tcgen05.mma.cta_group::1.kind::tf32      [taddr0],  adesc,  bdesc, idesc, {m0, m1, m2, m3}, p;
tcgen05.mma.cta_group::1.kind::mxf8f6f4  [taddr2],  [taddr1],  bdesc, idesc,
                                         [tmem_scaleA], [tmem_scaleB], p;

tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];

loop:
mbarrier.try_wait.parity.b64 p, [mbarObj0], 0;
@!p bra loop;

9.7.16.10.9.2. TensorCore 5th Generation Instructions: tcgen05.mma.sp

tcgen05.mma.sp

Perform the 5th generation of matrix multiply and accumulate operation with sparse A matrix.

Syntax

// 1. Floating-point type without block scaling:

tcgen05.mma.sp.cta_group.kind  [d-tmem],  a-desc,  b-desc, [sp-meta-tmem] ,  idesc,
                               { disable-output-lane }, enable-input-d{, scale-input-d};

tcgen05.mma.sp.cta_group.kind  [d-tmem], [a-tmem], b-desc, [sp-meta-tmem] , idesc,
                               { disable-output-lane }, enable-input-d{, scale-input-d};

.kind       = { .kind::f16, , .kind::tf32, .kind::f8f6f4 }
.cta_group  = { .cta_group::1,  .cta_group::2 }

----------------------------------------------------------------------------------

// 2. Floating-point type with block scaling:

tcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}
                                         [d-tmem],  a-desc,  b-desc , [sp-meta-tmem] , idesc,
                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;

tcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}
                                         [d-tmem], [a-tmem], b-desc , [sp-meta-tmem] , idesc,
                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;

.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }
.cta_group      = { .cta_group::1,  .cta_group::2 }
.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }

----------------------------------------------------------------------------------

// 3. Convolution MMA with floating-point type without block scaling:

tcgen05.mma.sp.cta_group.kind.collector_usage           [d-tmem],  a-desc,  b-desc,
                                                        [sp-meta-tmem] ,  idesc,
                                                        { disable-output-lane }, enable-input-d
                                                        {, scale-input-d};

tcgen05.mma.sp.cta_group.kind.ashift{.collector_usage}  [d-tmem], [a-tmem], b-desc,
                                                        [sp-meta-tmem] , idesc,
                                                        { disable-output-lane }, enable-input-d
                                                        {, scale-input-d};

tcgen05.mma.sp.cta_group.kind{.ashift}.collector_usage  [d-tmem], [a-tmem], b-desc,
                                                        [sp-meta-tmem] , idesc,
                                                        { disable-output-lane }, enable-input-d
                                                        {, scale-input-d};

.kind            = { .kind::f16, .kind::tf32, .kind::f8f6f4 }
.collector_usage = { .collector::buffer::op }
::buffer         = { ::a }
::op             = { ::fill, ::use, ::lastuse, ::discard* }

----------------------------------------------------------------------------------

// 4. Activation Stationary MMA with floating-point type with block scaling:

tcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage
                                         [d-tmem],  a-desc,  b-desc , [sp-meta-tmem] , idesc,
                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;

tcgen05.mma.sp.cta_group.kind.block_scale{.scale_vectorsize}.collector_usage
                                         [d-tmem], [a-tmem], b-desc , [sp-meta-tmem] , idesc,
                                         [scale-A-tmem], [scale-B-tmem], enable-input-d;

.kind = { .kind::mxf8f6f4, .kind::mxf4, .kind::mxf4nvf4 }
.scale_vectorsize = { .scale_vec::1X, .scale_vec::2X, .scale_vec::4X, .block16, .block32 }
.collector_usage = { .collector::buffer::op }
::buffer         = { ::a }
::op             = { ::fill, ::use, ::lastuse, ::discard* }

----------------------------------------------------------------------------------

// 5. Integer type:

tcgen05.mma.sp.cta_group.kind::i8 [d-tmem],  a-desc,  b-desc, [sp-meta-tmem] , idesc,
                                  { disable-output-lane }, enable-input-d;

tcgen05.mma.sp.cta_group.kind::i8 [d-tmem], [a-tmem], b-desc, [sp-meta-tmem] , idesc,
                                  { disable-output-lane }, enable-input-d;

.cta_group      = { .cta_group::1,  .cta_group::2 }

----------------------------------------------------------------------------------

// 6. Convolution MMA with Integer type:

tcgen05.mma.sp.cta_group.kind::i8.collector_usage          [d-tmem],  a-desc,  b-desc,
                                                           [sp-meta-tmem] , idesc,
                                                           { disable-output-lane }, enable-input-d;

tcgen05.mma.sp.cta_group.kind::i8.ashift{.collector_usage} [d-tmem], [a-tmem], b-desc,
                                                           [sp-meta-tmem], idesc ,
                                                           { disable-output-lane }, enable-input-d;

tcgen05.mma.sp.cta_group.kind::i8{.ashift}.collector_usage [d-tmem], [a-tmem], b-desc,
                                                           [sp-meta-tmem], idesc ,
                                                           { disable-output-lane }, enable-input-d;

.collector_usage = { .collector::buffer::op }
::buffer         = { ::a }
::op             = { ::fill, ::use, ::lastuse, ::discard* }

Description

Instruction tcgen05.mma.sp is an asynchronous instruction which initiates an MxNxK matrix multiply and accumulate operation of the form D = A*B+D where the A matrix is Mx(K/2), the B matrix is KxN, and the D matrix is MxN. Sparse Matrices describes the details of the sparsity.

The operation of the form D = A*B is issued when the input predicate argument enable-input-d is false.

The optional immediate argument scale-input-d can be specified to scale the input matrix D as follows: D = A*B+D * (2 ^ - scale-input-d)

The valid range of values for argument scale-input-d is [0, 15]. The argument scale-input-d is only valid for .kind::tf32 and .kind::f16.

The 32-bit register operand idesc is the instruction descriptor as described in Instruction descriptor, specifies the shapes, exact types, sparsity and other details of the input matrices, output matrix and the matrix multiply and accumulate operation.

The qualifier .cta_group::1 specifies that the matrix multiply and accumulate operation is performed on the Tensor Memory of the executing thread’s CTA only. The qualifier .cta_group::2 specifies that the matrix multiply and accumulate operation is performed on the Tensor Memory of the executing thread’s CTA and its peer CTA.

All tcgen05 instructions within a kernel must specify the same value for the .cta_group qualifier.

The instruction tcgen05.mma.sp has single thread semantics, unlike the collective instructions mma.sync or wgmma.mma_async. So, a single thread issuing the tcgen05.mma.sp will result in the initiation of the whole matrix multiply and accumulate operation. Refer to the section Issue Granularity.

The qualifier .kind specifies the general kind of the element types of the multiplicand matrices. The exact types of the elements of the input and output matrices for each MMA-kind are specified in the Instruction descriptor.

The address operand d-tmem specifies the address of the destination and the accumulation matrix D in the Tensor Memory. The address operand a-tmem specifies the address of the matrix A in the Tensor Memory. The 64-bit register operand a-desc and b-desc are the matrix descriptors which represent the matrices A and B in shared memory respectively. The format of the matrix descriptor is described in Matrix Descriptors.

The vector operand disable-output-lane specifies the lane(s) in the Tensor Memory that should be not be updated with the resultant matrix D. Elements of the vector operand disable-output-lane forms a mask where each bit corresponds to a lane of the Tensor Memory. with least significant bit of the first element of the vector (leftmost in syntax) corresponding to the lane 0 of the Tensor Memory. If a bit in the mask is 1, then the corresponding lane in the Tensor Memory for the resultant matrix D will not be updated. The size of the vector is as follows:

.cta_group
	

Size of the vector disable-output-lane

::1
	

4

::2
	

8

Qualifier .block_scale specifies that the matrices A and B are scaled with scale_A and scale_B matrices respectively before performing the matrix multiply and accumulate operation as specified in the section Block Scaling. The address operand scale-A-tmem and scale-B-tmem specify the base address the matrices scale_A and scale_B respectively in the Tensor Memory.

For qualifier .scale_vectorsize,

    If .scale_vec::NX is specified: N specifies the number of columns in scale_A matrix and number of rows in scale_B matrix.

    If .blockN is specified: N specifies the block size for which single scale factor will be applied. In this form, value of N is same as the K-dimension / (N of .scale_vec::NX).

Aliased .scale_vectorsize variants:

    .block16 is aliased with:

        .scale_vec::4X when .kind = .kind::mxf4nvf4 and K = 64 or 128

    .block32 is aliased with:

        .scale_vec::1X when .kind = .kind::mxf8f6f4 for all supported values of K

        .scale_vec::2X when .kind = .kind::mxf4 or .kind::mxf4nvf4 and K = 64 or 128

The valid combinations of MMA-kind and .scale_vectorsize are described in Table 54. For .kind::mxf4 when the qualifier .scale_vectorsize is not specified, then it defaults to .block32. For .kind::mxf4nvf4, the qualifier .scale_vectorsize must be explicitly specified.

The qualifier .ashift shifts the rows of the A matrix down by one row, except for the last row in the Tensor Memory. Qualifier .ashift is only allowed with M = 128 or M = 256.

The qualifier .collector_usage specifies the usage of collector buffer for matrix A. Following collector buffer operations can be specified:

.collector_usage
	

Semantics

.collector::a::fill
	

Specifies that the A matrix read from the memory should be filled in collector buffer.

.collector::a::use
	

Specifies that the A matrix can be read from the collector buffer. This requires a previous fill to the collector buffer to be still valid.

.collector::a::lastuse
	

Specifies that the A matrix can be read from the collector buffer and the contents of the collector buffer can be discarded. This requires a previous fill to the collector buffer to be valid till the collector buffer is read.

.collector::a::discard
	

Specifies that the contents of the collector buffer for A can be discarded.

If no .collector_usage qualifier is specified, then it defaults to .collector::a::discard. It is illegal to specify either of .collector::a::use or .collector::a::fill along with .ashift.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Qualifier .kind::mxf4nvf4 introduced in PTX ISA version 8.7.

Qualifiers .block16 and .block32 introduced in PTX ISA version 8.8.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8 except .kind::i8/.kind::mxf4nvf4/.kind::mxf4:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

    sm_110f or higher in the same family

Qualifier .kind::i8 is supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    sm_110a

Qualifiers .kind::mxf4nvf4 and .kind::mxf4 are supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    sm_103a

    sm_110a

Argument scale-input-d requires sm_100a and is supported on sm_100f or higher in the same family from PTX ISA version 8.8.

For .scale_vectorsize,

    .scale_vec::1X, .scale_vec::2X, .scale_vec::4X requires sm_100a.

    .block16, .block32 requires sm_100f or sm_110f.

For Target ISA details on matrix shape, check Target ISA Note.

For Target ISA details on shared memory descriptor, check Target ISA Note.

Examples

tcgen05.mma.sp.cta_group::1.kind::f16      [taddr0],  adesc,  bdesc, [tmem_spmeta0], idesc, p;

tcgen05.mma.sp.cta_group::1.kind::mxf8f6f4.collector::a:fill
                                           [taddr2],  [taddr1],  bdesc, [tmem_spmeta1], idesc,
                                           [tmem_scaleA], [tmem_scaleB], p;

tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];

loop:
mbarrier.try_wait.parity.b64 p, [mbarObj0], 0;
@!p bra loop;

9.7.16.10.9.3. TensorCore 5th Generation Instructions: tcgen05.mma.ws

tcgen05.mma.ws

Perform the 5th generation of weight stationary convolution matrix multiply and accumulate operation.

Syntax

// 1. Floating-point type without block scaling:

tcgen05.mma.ws.cta_group::1.kind{.collector_usage}    [d-tmem],  a-desc,  b-desc,  idesc,
                                                      enable-input-d {, zero-column-mask-desc };

tcgen05.mma.ws.cta_group::1.kind{.collector_usage}    [d-tmem], [a-tmem], b-desc, idesc,
                                                      enable-input-d {, zero-column-mask-desc };

.kind = { .kind::f16, .kind::tf32, .kind::f8f6f4 }

----------------------------------------------------------------------------------

// 2. Integer type:

tcgen05.mma.ws.cta_group::1.kind::i8{.collector_usage} [d-tmem],  a-desc,  b-desc, idesc,
                                                       enable-input-d {, zero-column-mask-desc};

tcgen05.mma.ws.cta_group::1.kind::i8{.collector_usage} [d-tmem], [a-tmem], b-desc, idesc,
                                                       enable-input-d {, zero-column-mask-desc};

.collector_usage = { .collector::buffer::op }
::buffer = { ::b0, ::b1, ::b2, ::b3 }
::op   = { ::fill, ::use, ::lastuse, ::discard}

Description
Instruction tcgen05.mma.ws is an asynchronous instruction which initiates an MxNxK matrix multiply and accumulate operation, D = A*B+D where the A matrix is MxK, the B matrix is KxN, and the D matrix is MxN.

The operation of the form D = A*B is issued when the input predicate argument enable-input-d is false.

The 32-bit register operand idesc is the instruction descriptor as described in Instruction descriptor, specifies the shapes, exact types, sparsity and other details of the input matrices, output matrix and the matrix multiply and accumulate operation.

The qualifier .cta_group::1 specifies that the matrix multiply and accumulate operation is performed on the Tensor Memory of the executing thread’s CTA only.

All tcgen05 instructions within a kernel must specify the same value for the .cta_group qualifier.

The instruction tcgen05.mma.ws has single thread semantics, unlike the collective instructions mma.sync or wgmma.mma_async. So, a single thread issuing the tcgen05.mma.ws will result in the initiation of the whole matrix multiply and accumulate operation. Refer to the section Issue Granularity.

The qualifier .kind specifies the general kind of the element types of the multiplicand matrices. The exact types of the elements of the input and output matrices for each MMA-kind are specified in the Instruction descriptor.

The address operand d-tmem specifies the address of the destination and the accumulation matrix D in the Tensor Memory. The address operand a-tmem specifies the address of the matrix A in the Tensor Memory. The 64-bit register operand a-desc and b-desc are the matrix descriptors which represent the matrices A and B in shared memory respectively. The format of the matrix descriptor is described in Matrix Descriptors.

The optional operand zero-column-mask-desc is a 64-bit register which specifies the Zero-Column Mask Descriptor. The zero-column mask descriptor is used to generate a mask that specifies which columns of B matrix will have zero value for the matrix multiply and accumulate operation regardless of the values present in the shared memory.

The qualifier .collector_usage specifies the usage of collector buffer for Matrix B. Following collector buffer operations can be specified:

.collector_usage
	

Semantics

.collector::bN::fill
	

Specifies that the B matrix read from the memory should be filled in collector buffer #N.

.collector::bN::use
	

Specifies that the B matrix can be read from the collector buffer #N. This requires a previous fill to the collector buffer #N to be still valid.

.collector::bN::lastuse
	

Specifies that the B matrix can be read from the collector buffer #N after which the contents of the collector buffer #N can be discarded. This requires a previous fill to the collector buffer #N to be valid till the collector buffer #N is read.

.collector::bN::discard
	

Specifies that the contents of the collector buffer #N can be discarded.

If no .collector_usage qualifier is specified, then it defaults to .collector::b0::discard.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8 except .kind::i8:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

    sm_110f or higher in the same family

Qualifier .kind::i8 is supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    sm_110a

Examples

tcgen05.mma.ws.cta_group::1.kind::i8.collector::b2:use [taddr2], [taddr1], bdesc, idesc, p;
tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];

loop:
mbarrier.try_wait.parity.b64 p, [mbarObj0], 0;
@!p bra loop;

9.7.16.10.9.4. TensorCore 5th Generation Instructions: tcgen05.mma.ws.sp

tcgen05.mma.ws.sp

Perform the 5th generation of weight stationary convolution matrix multiply and accumulate operation with sparse A matrix.

Syntax

// 1. Floating-point type without block scaling:

tcgen05.mma.ws.sp.cta_group::1.kind{.collector_usage} [d-tmem],  a-desc,  b-desc,
                                                      [sp-meta-tmem] ,  idesc,
                                                      enable-input-d {, zero-column-mask-desc};

tcgen05.mma.ws.sp.cta_group::1.kind{.collector_usage} [d-tmem], [a-tmem], b-desc,
                                                      [sp-meta-tmem] , idesc,
                                                      enable-input-d {, zero-column-mask-desc};

.kind = { .kind::f16, .kind::tf32, .kind::f8f6f4 }

----------------------------------------------------------------------------------

// 2. Integer type:

tcgen05.mma.ws.sp.cta_group::1.kind::i8{.collector_usage} [d-tmem], a-desc, b-desc,
                                                          [sp-meta-tmem] , idesc,
                                                          enable-input-d {, zero-column-mask-desc};

tcgen05.mma.ws.sp.cta_group::1.kind::i8{.collector_usage} [d-tmem], [a-tmem], b-desc,
                                                          [sp-meta-tmem] , idesc,
                                                          enable-input-d {, zero-column-mask-desc};

.collector_usage = { .collector::buffer::op }
::buffer = { ::b0, ::b1, ::b2, ::b3 }
::op   = { ::fill, ::use, ::lastuse, ::discard}

Description

Instruction tcgen05.mma.ws.sp is an asynchronous instruction which initiates an MxNxK matrix multiply and accumulate operation, D = A*B+D where the A matrix is Mx(K/2), the B matrix is KxN, and the D matrix is MxN. Sparse Matrices describes the details of the sparsity.

The operation of the form D = A*B is issued when the input predicate argument enable-input-d is false.

The 32-bit register operand idesc is the instruction descriptor as described in Instruction descriptor, specifies the shapes, exact types, sparsity and other details of the input matrices, output matrix and the matrix multiply and accumulate operation.

The qualifier .cta_group::1 specifies that the matrix multiply and accumulate operation is performed on the Tensor Memory of the executing thread’s CTA only.

All tcgen05 instructions within a kernel must specify the same value for the .cta_group qualifier.

The instruction tcgen05.mma.ws.sp has single thread semantics, unlike the collective instructions mma.sync or wgmma.mma_async. So, a single thread issuing the tcgen05.mma.ws.sp will result in the initiation of the whole matrix multiply and accumulate operation. Refer to the section Issue Granularity.

The qualifier .kind specifies the general kind of the element types of the multiplicand matrices. The exact types of the elements of the input and output matrices for each MMA-kind are specified in the Instruction descriptor.

The address operand d-tmem specifies the address of the destination and the accumulation matrix D in the Tensor Memory. The address operand a-tmem specifies the address of the matrix A in the Tensor Memory. The 64-bit register operand a-desc and b-desc are the matrix descriptors which represent the matrices A and B in shared memory respectively. The format of the matrix descriptor is described in Matrix Descriptors.

The optional operand zero-column-mask-desc is a 64-bit register which specifies the Zero-Column Mask Descriptor. The zero-column mask descriptor is used to generate a mask that specifies which columns of B matrix will have zero value for the matrix multiply and accumulate operation regardless of the values present in the shared memory.

The qualifier .collector_usage specifies the usage of collector buffer for Matrix B. Following collector buffer operations can be specified:

.collector_usage
	

Semantics

.collector::bN::fill
	

Specifies that the B matrix read from the memory should be filled in collector buffer #N.

.collector::bN::use
	

Specifies that the B matrix can be read from the collector buffer #N. This requires a previous fill to the collector buffer #N to be still valid.

.collector::bN::lastuse
	

Specifies that the B matrix can be read from the collector buffer #N after which the contents of the collector buffer #N can be discarded. This requires a previous fill to the collector buffer #N to be valid till the collector buffer #N is read.

.collector::bN::discard
	

Specifies that the contents of the collector buffer #N can be discarded.

If no .collector_usage qualifier is specified, then it defaults to .collector::b0::discard.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8 except .kind::i8:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

    sm_110f or higher in the same family

Qualifier .kind::i8 is supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    sm_110a

Examples

tcgen05.mma.ws.sp.cta_group::1.kind::tf32.collector::b1::fill  [taddr1], [taddr0], bdesc,
                                                               [tmem_spmeta0], idesc, p;

tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj0];

loop:
mbarrier.try_wait.parity.b64 p, [mbarObj0], 0;
@!p bra loop;

9.7.16.11. TensorCore 5th Generation Specialized Synchronization Operations
9.7.16.11.1. TensorCore 5th Generation Instructions: tcgen05.fence

tcgen05.fence

Specialized fence for the asynchronous tcgen05 operations.

Syntax

tcgen05.fence::before_thread_sync ;
tcgen05.fence::after_thread_sync  ;

Description

The instruction tcgen05.fence::before_thread_sync orders all the prior asynchronous tcgen05 operations with respect to the subsequent tcgen05 and the execution ordering operations.

The instruction tcgen05.fence::after_thread_sync orders all the subsequent asynchronous tcgen05 operations with respect to the prior tcgen05 and the execution ordering operations.

The tcgen05.fence::* instructions compose with execution ordering instructions across a thread scope and provide ordering between tcgen05 instructions across the same scope.

The tcgen05.fence::before_thread_sync instructions behave as code motion fence for prior tcgen05 instructions as they cannot be hoisted across. The tcgen05.fence::after_thread_sync instructions behave as code motion fence for subsequent tcgen05 instructions as they cannot be hoisted across.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

    sm_110f or higher in the same family

Examples

// Producer thread:

tcgen05.cp.cta_group::1.128x256b  [taddr0], sdesc0;

tcgen05.fence::before_thread_sync;
st.relaxed.b32 [flag], 1;

// Consumer thread:

loop:
ld.relaxed.b32 r, [flag];
setp.eq.u32 p, r, 1;
@!p bra loop;

tcgen05.fence::after_thread_sync;
tcgen05.mma.cta_group.kind   [taddr0], adesc, bdesc, idesc, p;

9.7.16.12. TensorCore 5th Generation Async Synchronization Operations
9.7.16.12.1. TensorCore 5th Generation Instructions: tcgen05.commit

tcgen05.commit

Makes the mbarrier object track the completion of all prior async-tcgen05 operations initiated by the executing thread.

Syntax

tcgen05.commit.cta_group.completion_mechanism{.shared::cluster}{.multicast}.b64
                                                            [mbar] {, ctaMask};

.completion_mechanism = { .mbarrier::arrive::one }
.cta_group            = { .cta_group::1, .cta_group::2 }
.multicast            = { .multicast::cluster }

Description

The instruction tcgen05.commit is an asynchronous instruction which makes the mbarrier object, specified by the address operand mbar, track the completion of all the prior asynchronous tcgen05 operations, as listed in mbarrier based completion mechanism, initiated by the executing thread. Upon the completion of the tracked asynchronous tcgen05 operations, the signal specified by the .completion_mechanism is triggered by the system on the mbarrier object.

The instruction tcgen05.commit.cta_group::1 tracks for the completion of all prior asynchronous tcgen05 operations with .cta_group::1 issued by the current thread. Similarly, the instruction tcgen05.commit.cta_group::2 tracks for the completion of all prior asynchronous tcgen05 operations with .cta_group::2 issued by the current thread.

All tcgen05 instructions within a kernel must specify the same value for the .cta_group qualifier.

The qualifier .mbarrier::arrive::one indicates that upon the completion of the prior asynchronous tcgen05 operation issued by the current thread, an arrive-on operation, with the count argument of 1, is signaled on the mbarrier object. The scope of the arrive-on operation is the cluster scope.

The optional qualifier .multicast::cluster allows signaling on the mbarrier objects of multiple CTAs in the cluster. Operand ctaMask specifies the CTAs in the cluster such that each bit position in the 16-bit ctaMask operand corresponds to the %cluster_ctarank of the destination CTA. The mbarrier signal is multicast to the same offset as mbar in the shared memory of each destination CTA.

If no state space is specified then Generic Addressing is used. If the address specified by mbar does not fall within the address window of .shared::cluster state space then the behavior is undefined.

PTX ISA Notes

Introduced in PTX ISA version 8.6.

Target ISA Notes

Supported on following architectures:

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    And is supported on following family-specific architectures from PTX ISA version 8.8:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

    sm_110f or higher in the same family

Examples

Example 1:
tcgen05.cp.cta_group::1.128x256b                      [taddr0], sdesc0;
tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbarObj1];

loop:
mbarrier.try_wait.parity.b64 p, [mbarObj1], 0;
@!p bra loop;

Example 2:
tcgen05.mma.cta_group::2.kind::tf32    [taddr0],  adesc,  bdesc, idesc, p;
tcgen05.commit.cta_group::2.mbarrier::arrive::one.b64 [mbarObj2];

loop:
mbarrier.try_wait.parity.b64 p, [mbarObj2], 0;
@!p bra loop;

9.7.17. Stack Manipulation Instructions

The stack manipulation instructions can be used to dynamically allocate and deallocate memory on the stack frame of the current function.

The stack manipulation instrucitons are:

    stacksave

    stackrestore

    alloca

9.7.17.1. Stack Manipulation Instructions: stacksave

stacksave

Save the value of stack pointer into a register.

Syntax

stacksave.type  d;

.type = { .u32, .u64 };

Description

Copies the current value of stack pointer into the destination register d. Pointer returned by stacksave can be used in a subsequent stackrestore instruction to restore the stack pointer. If d is modified prior to use in stackrestore instruction, it may corrupt data in the stack.

Destination operand d has the same type as the instruction type.

Semantics

d = stackptr;

PTX ISA Notes

Introduced in PTX ISA version 7.3.

Preview Feature:

    stacksave is a preview feature in PTX ISA version 7.3. All details are subject to change with no guarantees of backward compatibility on future PTX ISA versions or SM architectures.

Target ISA Notes

stacksave requires sm_52 or higher.

Examples

.reg .u32 rd;
stacksave.u32 rd;

.reg .u64 rd1;
stacksave.u64 rd1;

9.7.17.2. Stack Manipulation Instructions: stackrestore

stackrestore

Update the stack pointer with a new value.

Syntax

stackrestore.type  a;

.type = { .u32, .u64 };

Description

Sets the current stack pointer to source register a.

When stackrestore is used with operand a written by a prior stacksave instruction, it will effectively restore the state of stack as it was before stacksave was executed. Note that if stackrestore is used with an arbitrary value of a, it may cause corruption of stack pointer. This implies that the correct use of this feature requires that stackrestore.type a is used after stacksave.type a without redefining the value of a between them.

Operand a has the same type as the instruction type.

Semantics

stackptr = a;

PTX ISA Notes

Introduced in PTX ISA version 7.3.

Preview Feature:

    stackrestore is a preview feature in PTX ISA version 7.3. All details are subject to change with no guarantees of backward compatibility on future PTX ISA versions or SM architectures.

Target ISA Notes

stackrestore requires sm_52 or higher.

Examples

.reg .u32 ra;
stacksave.u32 ra;
// Code that may modify stack pointer
...
stackrestore.u32 ra;

9.7.17.3. Stack Manipulation Instructions: alloca

alloca

Dynamically allocate memory on stack.

Syntax

alloca.type  ptr, size{, immAlign};

.type = { .u32, .u64 };

Description

The alloca instruction dynamically allocates memory on the stack frame of the current function and updates the stack pointer accordingly. The returned pointer ptr points to local memory and can be used in the address operand of ld.local and st.local instructions.

If sufficient memory is unavailable for allocation on the stack, then execution of alloca may result in stack overflow. In such cases, attempting to access the allocated memory with ptr will result in undefined program behavior.

The memory allocated by alloca is deallocated in the following ways:

    It is automatically deallocated when the function exits.

    It can be explicitly deallocated using stacksave and stackrestore instructions: stacksave can be used to save the value of stack pointer before executing alloca, and stackrestore can be used after alloca to restore stack pointer to the original value which was previously saved with stacksave. Note that accessing deallocated memory after executing stackrestore results in undefined behavior.

size is an unsigned value which specifies the amount of memory in number of bytes to be allocated on stack. size = 0 may not lead to a valid memory allocation.

Both ptr and size have the same type as the instruction type.

immAlign is a 32-bit value which specifies the alignment requirement in number of bytes for the memory allocated by alloca. It is an integer constant, must be a power of 2 and must not exceed 2^23. immAlign is an optional argument with default value being 8 which is the minimum guaranteed alignment.

Semantics

alloca.type ptr, size, immAlign:

a = max(immAlign, frame_align); // frame_align is the minimum guaranteed alignment

// Allocate size bytes of stack memory with alignment a and update the stack pointer.
// Since the stack grows down, the updated stack pointer contains a lower address.
stackptr = alloc_stack_mem(size, a);

// Return the new value of stack pointer as ptr. Since ptr is the lowest address of the memory
// allocated by alloca, the memory can be accessed using ptr up to (ptr + size of allocated memory).
stacksave ptr;

PTX ISA Notes

Introduced in PTX ISA version 7.3.

Preview Feature:

    alloca is a preview feature in PTX ISA version 7.3. All details are subject to change with no guarantees of backward compatibility on future PTX ISA versions or SM architectures.

Target ISA Notes

alloca requires sm_52 or higher.

Examples

.reg .u32 ra, stackptr, ptr, size;

stacksave.u32 stackptr;     // Save the current stack pointer
alloca ptr, size, 8;        // Allocate stack memory
st.local.u32 [ptr], ra;     // Use the allocated stack memory
stackrestore.u32 stackptr;  // Deallocate memory by restoring the stack pointer

9.7.18. Video Instructions

All video instructions operate on 32-bit register operands. However, the video instructions may be classified as either scalar or SIMD based on whether their core operation applies to one or multiple values.

The video instructions are:

    vadd, vadd2, vadd4

    vsub, vsub2, vsub4

    vmad

    vavrg2, vavrg4

    vabsdiff, vabsdiff2, vabsdiff4

    vmin, vmin2, vmin4

    vmax, vmax2, vmax4

    vshl

    vshr

    vset, vset2, vset4

9.7.18.1. Scalar Video Instructions

All scalar video instructions operate on 32-bit register operands. The scalar video instructions are:

    vadd

    vsub

    vabsdiff

    vmin

    vmax

    vshl

    vshr

    vmad

    vset

The scalar video instructions execute the following stages:

    Extract and sign- or zero-extend byte, half-word, or word values from its source operands, to produce signed 33-bit input values.

    Perform a scalar arithmetic operation to produce a signed 34-bit result.

    Optionally clamp the result to the range of the destination type.

    Optionally perform one of the following:

        apply a second operation to the intermediate result and a third operand, or

        truncate the intermediate result to a byte or half-word value and merge into a specified position in the third operand to produce the final result.

The general format of scalar video instructions is as follows:

// 32-bit scalar operation, with optional secondary operation
vop.dtype.atype.btype{.sat}        d, a{.asel}, b{.bsel};
vop.dtype.atype.btype{.sat}.secop  d, a{.asel}, b{.bsel}, c;

// 32-bit scalar operation, with optional data merge
vop.dtype.atype.btype{.sat}   d.dsel, a{.asel}, b{.bsel}, c;


.dtype = .atype = .btype = { .u32, .s32 };
.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };
.secop = { .add, .min, .max };

The source and destination operands are all 32-bit registers. The type of each operand (.u32 or .s32) is specified in the instruction type; all combinations of dtype, atype, and btype are valid. Using the atype/btype and asel/bsel specifiers, the input values are extracted and sign- or zero-extended internally to .s33 values. The primary operation is then performed to produce an .s34 intermediate result. The sign of the intermediate result depends on dtype.

The intermediate result is optionally clamped to the range of the destination type (signed or unsigned), taking into account the subword destination size in the case of optional data merging.

.s33 optSaturate( .s34 tmp, Bool sat, Bool sign, Modifier dsel ) {
    if ( !sat )  return tmp;

    switch ( dsel ) {
        case .b0, .b1, .b2, .b3:
            if ( sign )  return CLAMP( tmp, S8_MAX, S8_MIN );
            else         return CLAMP( tmp, U8_MAX, U8_MIN );
        case .h0, .h1:
            if ( sign )  return CLAMP( tmp, S16_MAX, S16_MIN );
            else         return CLAMP( tmp, U16_MAX, U16_MIN );
        default:
            if ( sign )  return CLAMP( tmp, S32_MAX, S32_MIN );
            else         return CLAMP( tmp, U32_MAX, U32_MIN );
    }
}

This intermediate result is then optionally combined with the third source operand using a secondary arithmetic operation or subword data merge, as shown in the following pseudocode. The sign of the third operand is based on dtype.

.s33 optSecOp(Modifier secop, .s33 tmp, .s33 c) {
    switch ( secop ) {
        .add:     return tmp + c;
        .min:     return MIN(tmp, c);
        .max      return MAX(tmp, c);
        default:  return tmp;
    }
}

.s33 optMerge( Modifier dsel, .s33 tmp, .s33 c ) {
    switch ( dsel ) {
        case .h0:  return ((tmp & 0xffff)        | (0xffff0000 & c);
        case .h1:  return ((tmp & 0xffff) << 16) | (0x0000ffff & c);
        case .b0:  return ((tmp & 0xff)          | (0xffffff00 & c);
        case .b1:  return ((tmp & 0xff) <<  8)   | (0xffff00ff & c);
        case .b2:  return ((tmp & 0xff) << 16)   | (0xff00ffff & c);
        case .b3:  return ((tmp & 0xff) << 24)   | (0x00ffffff & c);
        default:   return tmp;
    }
}

The lower 32-bits are then written to the destination operand.
9.7.18.1.1. Scalar Video Instructions: vadd, vsub, vabsdiff, vmin, vmax

vadd, vsub

Integer byte/half-word/word addition/subtraction.

vabsdiff

Integer byte/half-word/word absolute value of difference.

vmin, vmax

Integer byte/half-word/word minimum/maximum.

Syntax

// 32-bit scalar operation, with optional secondary operation
vop.dtype.atype.btype{.sat}       d, a{.asel}, b{.bsel};
vop.dtype.atype.btype{.sat}.op2   d, a{.asel}, b{.bsel}, c;

// 32-bit scalar operation, with optional data merge
vop.dtype.atype.btype{.sat}  d.dsel, a{.asel}, b{.bsel}, c;

 vop   = { vadd, vsub, vabsdiff, vmin, vmax };
.dtype = .atype = .btype = { .u32, .s32 };
.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };
.op2   = { .add, .min, .max };

Description

Perform scalar arithmetic operation with optional saturate, and optional secondary arithmetic operation or subword data merge.

Semantics

// extract byte/half-word/word and sign- or zero-extend
// based on source operand type
ta = partSelectSignExtend( a, atype, asel );
tb = partSelectSignExtend( b, btype, bsel );

switch ( vop ) {
    case vadd:     tmp = ta + tb;
    case vsub:     tmp = ta - tb;
    case vabsdiff: tmp = | ta - tb |;
    case vmin:     tmp = MIN( ta, tb );
    case vmax:     tmp = MAX( ta, tb );
}
// saturate, taking into account destination type and merge operations
tmp = optSaturate( tmp, sat, isSigned(dtype), dsel );
d = optSecondaryOp( op2, tmp, c );  // optional secondary operation
d = optMerge( dsel, tmp, c );       // optional merge with c operand

PTX ISA Notes

Introduced in PTX ISA version 2.0.

Target ISA Notes

vadd, vsub, vabsdiff, vmin, vmax require sm_20 or higher.

Examples

vadd.s32.u32.s32.sat      r1, r2.b0, r3.h0;
vsub.s32.s32.u32.sat      r1, r2.h1, r3.h1;
vabsdiff.s32.s32.s32.sat  r1.h0, r2.b0, r3.b2, c;
vmin.s32.s32.s32.sat.add  r1, r2, r3, c;

9.7.18.1.2. Scalar Video Instructions: vshl, vshr

vshl, vshr

Integer byte/half-word/word left/right shift.

Syntax

// 32-bit scalar operation, with optional secondary operation
vop.dtype.atype.u32{.sat}.mode       d, a{.asel}, b{.bsel};
vop.dtype.atype.u32{.sat}.mode.op2   d, a{.asel}, b{.bsel}, c;

// 32-bit scalar operation, with optional data merge
vop.dtype.atype.u32{.sat}.mode  d.dsel, a{.asel}, b{.bsel}, c;

 vop   = { vshl, vshr };
.dtype = .atype = { .u32, .s32 };
.mode  = { .clamp, .wrap };
.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };
.op2   = { .add, .min, .max };

Description

vshl

    Shift a left by unsigned amount in b with optional saturate, and optional secondary arithmetic operation or subword data merge. Left shift fills with zero.
vshr

    Shift a right by unsigned amount in b with optional saturate, and optional secondary arithmetic operation or subword data merge. Signed shift fills with the sign bit, unsigned shift fills with zero.

Semantics

// extract byte/half-word/word and sign- or zero-extend
// based on source operand type
ta = partSelectSignExtend( a,atype, asel );
tb = partSelectSignExtend( b, .u32, bsel );
if ( mode == .clamp  && tb > 32 )  tb = 32;
if ( mode == .wrap )                       tb = tb & 0x1f;
switch ( vop ){
   case vshl:  tmp = ta << tb;
   case vshr:  tmp = ta >> tb;
}
// saturate, taking into account destination type and merge operations
tmp = optSaturate( tmp, sat, isSigned(dtype), dsel );
d = optSecondaryOp( op2, tmp, c );  // optional secondary operation
d = optMerge( dsel, tmp, c );       // optional merge with c operand

PTX ISA Notes

Introduced in PTX ISA version 2.0.

Target ISA Notes

vshl, vshr require sm_20 or higher.

Examples

vshl.s32.u32.u32.clamp  r1, r2, r3;
vshr.u32.u32.u32.wrap   r1, r2, r3.h1;

9.7.18.1.3. Scalar Video Instructions: vmad

vmad

Integer byte/half-word/word multiply-accumulate.

Syntax

// 32-bit scalar operation
vmad.dtype.atype.btype{.sat}{.scale}     d, {-}a{.asel}, {-}b{.bsel},
                                         {-}c;
vmad.dtype.atype.btype.po{.sat}{.scale}  d, a{.asel}, b{.bsel}, c;

.dtype = .atype = .btype = { .u32, .s32 };
.asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };
.scale = { .shr7, .shr15 };

Description

Calculate (a*b) + c, with optional operand negates, plus one mode, and scaling.

The source operands support optional negation with some restrictions. Although PTX syntax allows separate negation of the a and b operands, internally this is represented as negation of the product (a*b). That is, (a*b) is negated if and only if exactly one of a or b is negated. PTX allows negation of either (a*b) or c.

The plus one mode (.po) computes (a*b) + c + 1, which is used in computing averages. Source operands may not be negated in .po mode.

The intermediate result of (a*b) is unsigned if atype and btype are unsigned and the product (a*b) is not negated; otherwise, the intermediate result is signed. Input c has the same sign as the intermediate result.

The final result is unsigned if the intermediate result is unsigned and c is not negated.

Depending on the sign of the a and b operands, and the operand negates, the following combinations of operands are supported for VMAD:

 (u32 * u32) + u32  // intermediate unsigned; final unsigned
-(u32 * u32) + s32  // intermediate   signed; final   signed
 (u32 * u32) - u32  // intermediate unsigned; final   signed
 (u32 * s32) + s32  // intermediate   signed; final   signed
-(u32 * s32) + s32  // intermediate   signed; final   signed
 (u32 * s32) - s32  // intermediate   signed; final   signed
 (s32 * u32) + s32  // intermediate   signed; final   signed
-(s32 * u32) + s32  // intermediate   signed; final   signed
 (s32 * u32) - s32  // intermediate   signed; final   signed
 (s32 * s32) + s32  // intermediate   signed; final   signed
-(s32 * s32) + s32  // intermediate   signed; final   signed
 (s32 * s32) - s32  // intermediate   signed; final   signed

The intermediate result is optionally scaled via right-shift; this result is sign-extended if the final result is signed, and zero-extended otherwise.

The final result is optionally saturated to the appropriate 32-bit range based on the type (signed or unsigned) of the final result.

Semantics

// extract byte/half-word/word and sign- or zero-extend
// based on source operand type
ta = partSelectSignExtend( a, atype, asel );
tb = partSelectSignExtend( b, btype, bsel );
signedFinal = isSigned(atype) || isSigned(btype) ||
                                 (a.negate ^ b.negate) || c.negate;
tmp[127:0] = ta * tb;

lsb = 0;
if ( .po )                  {              lsb = 1; } else
if ( a.negate ^ b.negate )  { tmp = ~tmp;  lsb = 1; } else
if ( c.negate )             { c   = ~c;    lsb = 1; }

c128[127:0] = (signedFinal) sext32( c ) : zext ( c );
tmp = tmp + c128 + lsb;
switch( scale ) {
   case .shr7:   result = (tmp >>  7) & 0xffffffffffffffff;
   case .shr15:  result = (tmp >> 15) & 0xffffffffffffffff;
}
if ( .sat ) {
     if (signedFinal) result = CLAMP(result, S32_MAX, S32_MIN);
     else             result = CLAMP(result, U32_MAX, U32_MIN);
}

PTX ISA Notes

Introduced in PTX ISA version 2.0.

Target ISA Notes

vmad requires sm_20 or higher.

Examples

vmad.s32.s32.u32.sat    r0, r1, r2, -r3;
vmad.u32.u32.u32.shr15  r0, r1.h0, r2.h0, r3;

9.7.18.1.4. Scalar Video Instructions: vset

vset

Integer byte/half-word/word comparison.

Syntax

// 32-bit scalar operation, with optional secondary operation
vset.atype.btype.cmp       d, a{.asel}, b{.bsel};
vset.atype.btype.cmp.op2   d, a{.asel}, b{.bsel}, c;

// 32-bit scalar operation, with optional data merge
vset.atype.btype.cmp  d.dsel, a{.asel}, b{.bsel}, c;

.atype = .btype = { .u32, .s32 };
.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };
.dsel  = .asel  = .bsel  = { .b0, .b1, .b2, .b3, .h0, .h1 };
.op2   = { .add, .min, .max };

Description

Compare input values using specified comparison, with optional secondary arithmetic operation or subword data merge.

The intermediate result of the comparison is always unsigned, and therefore destination d and operand c are also unsigned.

Semantics

// extract byte/half-word/word and sign- or zero-extend
// based on source operand type
ta = partSelectSignExtend( a, atype, asel );
tb = partSelectSignExtend( b, btype, bsel );
tmp = compare( ta, tb, cmp ) ? 1 : 0;
d = optSecondaryOp( op2, tmp, c );    // optional secondary operation
d = optMerge( dsel, tmp, c );         // optional merge with c operand

PTX ISA Notes

Introduced in PTX ISA version 2.0.

Target ISA Notes

vset requires sm_20 or higher.

Examples

vset.s32.u32.lt    r1, r2, r3;
vset.u32.u32.ne    r1, r2, r3.h1;

9.7.18.2. SIMD Video Instructions

The SIMD video instructions operate on pairs of 16-bit values and quads of 8-bit values.

The SIMD video instructions are:

    vadd2, vadd4

    vsub2, vsub4

    vavrg2, vavrg4

    vabsdiff2, vabsdiff4

    vmin2, vmin4

    vmax2, vmax4

    vset2, vset4

PTX includes SIMD video instructions for operation on pairs of 16-bit values and quads of 8-bit values. The SIMD video instructions execute the following stages:

    Form input vectors by extracting and sign- or zero-extending byte or half-word values from the source operands, to form pairs of signed 17-bit values.

    Perform a SIMD arithmetic operation on the input pairs.

    Optionally clamp the result to the appropriate signed or unsigned range, as determinted by the destination type.

    Optionally perform one of the following:

        perform a second SIMD merge operation, or

        apply a scalar accumulate operation to reduce the intermediate SIMD results to a single scalar.

The general format of dual half-word SIMD video instructions is as follows:

// 2-way SIMD operation, with second SIMD merge or accumulate
vop2.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;

.dtype = .atype = .btype = { .u32, .s32 };
.mask  = { .h0, .h1, .h10 };
.asel  = .bsel = { .hxy, where x,y are from { 0, 1, 2, 3 } };

The general format of quad byte SIMD video instructions is as follows:

// 4-way SIMD operation, with second SIMD merge or accumulate
vop4.dtype.atype.btype{.sat}{.add}  d{.mask}, a{.asel}, b{.bsel}, c;

.dtype = .atype = .btype = { .u32, .s32 };
.mask  = { .b0,
           .b1, .b10
           .b2, .b20, .b21, .b210,
           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };
.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };

The source and destination operands are all 32-bit registers. The type of each operand (.u32 or .s32) is specified in the instruction type; all combinations of dtype, atype, and btype are valid. Using the atype/btype and asel/bsel specifiers, the input values are extracted and sign- or zero-extended internally to .s33 values. The primary operation is then performed to produce an .s34 intermediate result. The sign of the intermediate result depends on dtype.

The intermediate result is optionally clamped to the range of the destination type (signed or unsigned), taking into account the subword destination size in the case of optional data merging.
9.7.18.2.1. SIMD Video Instructions: vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2

vadd2, vsub2

Integer dual half-word SIMD addition/subtraction.

vavrg2

Integer dual half-word SIMD average.

vabsdiff2

Integer dual half-word SIMD absolute value of difference.

vmin2, vmax2

Integer dual half-word SIMD minimum/maximum.

Syntax

// SIMD instruction with secondary SIMD merge operation
vop2.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;

// SIMD instruction with secondary accumulate operation
vop2.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;

 vop2  = { vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 };
.dtype = .atype = .btype = { .u32, .s32 };
.mask  = { .h0, .h1, .h10 };  // defaults to .h10
.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };
   .asel defaults to .h10
   .bsel defaults to .h32

Description

Two-way SIMD parallel arithmetic operation with secondary operation.

Elements of each dual half-word source to the operation are selected from any of the four half-words in the two source operands a and b using the asel and bsel modifiers.

The selected half-words are then operated on in parallel.

The results are optionally clamped to the appropriate range determined by the destination type (signed or unsigned). Saturation cannot be used with the secondary accumulate operation.

For instructions with a secondary SIMD merge operation:

    For half-word positions indicated in mask, the selected half-word results are copied into destination d. For all other positions, the corresponding half-word from source operand c is copied to d.

For instructions with a secondary accumulate operation:

    For half-word positions indicated in mask, the selected half-word results are added to operand c, producing a result in d.

Semantics

// extract pairs of half-words and sign- or zero-extend
// based on operand type
Va = extractAndSignExt_2( a, b, .asel, .atype );
Vb = extractAndSignExt_2( a, b, .bsel, .btype );
Vc = extractAndSignExt_2( c );

for (i=0; i<2; i++) {
    switch ( vop2 ) {
       case vadd2:             t[i] = Va[i] + Vb[i];
       case vsub2:             t[i] = Va[i] - Vb[i];
       case vavrg2:            if ( ( Va[i] + Vb[i] ) >= 0 ) {
                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;
                               } else {
                                   t[i] = ( Va[i] + Vb[i] ) >> 1;
                               }
       case vabsdiff2:         t[i] = | Va[i] - Vb[i] |;
       case vmin2:             t[i] = MIN( Va[i], Vb[i] );
       case vmax2:             t[i] = MAX( Va[i], Vb[i] );
    }
    if (.sat) {
        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S16_MAX, S16_MIN );
        else                   t[i] = CLAMP( t[i], U16_MAX, U16_MIN );
    }
}
// secondary accumulate or SIMD merge
mask = extractMaskBits( .mask );
if (.add) {
    d = c;
    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }
} else {
    d = 0;
    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }
}

PTX ISA Notes

Introduced in PTX ISA version 3.0.

Target ISA Notes

vadd2, vsub2, varvg2, vabsdiff2, vmin2, vmax2 require sm_30 or higher.

Examples

vadd2.s32.s32.u32.sat  r1, r2, r3, r1;
vsub2.s32.s32.s32.sat  r1.h0, r2.h10, r3.h32, r1;
vmin2.s32.u32.u32.add  r1.h10, r2.h00, r3.h22, r1;

9.7.18.2.2. SIMD Video Instructions: vset2

vset2

Integer dual half-word SIMD comparison.

Syntax

// SIMD instruction with secondary SIMD merge operation
vset2.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;

// SIMD instruction with secondary accumulate operation
vset2.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;

.atype = .btype = { .u32, .s32 };
.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };
.mask  = { .h0, .h1, .h10 };  // defaults to .h10
.asel  = .bsel  = { .hxy, where x,y are from { 0, 1, 2, 3 } };
   .asel defaults to .h10
   .bsel defaults to .h32

Description

Two-way SIMD parallel comparison with secondary operation.

Elements of each dual half-word source to the operation are selected from any of the four half-words in the two source operands a and b using the asel and bsel modifiers.

The selected half-words are then compared in parallel.

The intermediate result of the comparison is always unsigned, and therefore the half-words of destination d and operand c are also unsigned.

For instructions with a secondary SIMD merge operation:

    For half-word positions indicated in mask, the selected half-word results are copied into destination d. For all other positions, the corresponding half-word from source operand b is copied to d.

For instructions with a secondary accumulate operation:

    For half-word positions indicated in mask, the selected half-word results are added to operand c, producing a result in d.

Semantics

// extract pairs of half-words and sign- or zero-extend
// based on operand type
Va = extractAndSignExt_2( a, b, .asel, .atype );
Vb = extractAndSignExt_2( a, b, .bsel, .btype );
Vc = extractAndSignExt_2( c );
for (i=0; i<2; i++) {
    t[i] = compare( Va[i], Vb[i], .cmp ) ? 1 : 0;
}
// secondary accumulate or SIMD merge
mask = extractMaskBits( .mask );
if (.add) {
    d = c;
    for (i=0; i<2; i++) {  d += mask[i] ? t[i] : 0;  }
} else {
    d = 0;
    for (i=0; i<2; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }
}

PTX ISA Notes

Introduced in PTX ISA version 3.0.

Target ISA Notes

vset2 requires sm_30 or higher.

Examples

vset2.s32.u32.lt      r1, r2, r3, r0;
vset2.u32.u32.ne.add  r1, r2, r3, r0;

9.7.18.2.3. SIMD Video Instructions: vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4

vadd4, vsub4

Integer quad byte SIMD addition/subtraction.

vavrg4

Integer quad byte SIMD average.

vabsdiff4

Integer quad byte SIMD absolute value of difference.

vmin4, vmax4

Integer quad byte SIMD minimum/maximum.

Syntax

// SIMD instruction with secondary SIMD merge operation
vop4.dtype.atype.btype{.sat}  d{.mask}, a{.asel}, b{.bsel}, c;

// SIMD instruction with secondary accumulate operation
vop4.dtype.atype.btype.add  d{.mask}, a{.asel}, b{.bsel}, c;
vop4  = { vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4 };

.dtype = .atype = .btype = { .u32, .s32 };
.mask  = { .b0,
           .b1, .b10
           .b2, .b20, .b21, .b210,
           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };
    defaults to .b3210
.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };
   .asel defaults to .b3210
   .bsel defaults to .b7654

Description

Four-way SIMD parallel arithmetic operation with secondary operation.

Elements of each quad byte source to the operation are selected from any of the eight bytes in the two source operands a and b using the asel and bsel modifiers.

The selected bytes are then operated on in parallel.

The results are optionally clamped to the appropriate range determined by the destination type (signed or unsigned). Saturation cannot be used with the secondary accumulate operation.

For instructions with a secondary SIMD merge operation:

    For byte positions indicated in mask, the selected byte results are copied into destination d. For all other positions, the corresponding byte from source operand c is copied to d.

For instructions with a secondary accumulate operation:

    For byte positions indicated in mask, the selected byte results are added to operand c, producing a result in d.

Semantics

// extract quads of bytes and sign- or zero-extend
// based on operand type
Va = extractAndSignExt_4( a, b, .asel, .atype );
Vb = extractAndSignExt_4( a, b, .bsel, .btype );
Vc = extractAndSignExt_4( c );
for (i=0; i<4; i++) {
    switch ( vop4 ) {
        case vadd4:            t[i] = Va[i] + Vb[i];
        case vsub4:            t[i] = Va[i] - Vb[i];
        case vavrg4:           if ( ( Va[i] + Vb[i] ) >= 0 ) {
                                   t[i] = ( Va[i] + Vb[i] + 1 ) >> 1;
                               } else {
                                   t[i] = ( Va[i] + Vb[i] ) >> 1;
                               }
        case vabsdiff4:        t[i] = | Va[i] - Vb[i] |;
        case vmin4:            t[i] = MIN( Va[i], Vb[i] );
        case vmax4:            t[i] = MAX( Va[i], Vb[i] );
    }
    if (.sat) {
        if ( .dtype == .s32 )  t[i] = CLAMP( t[i], S8_MAX, S8_MIN );
        else                   t[i] = CLAMP( t[i], U8_MAX, U8_MIN );
    }
}
// secondary accumulate or SIMD merge
mask = extractMaskBits( .mask );
if (.add) {
    d = c;
    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }
} else {
    d = 0;
    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }
}

PTX ISA Notes

Introduced in PTX ISA version 3.0.

Target ISA Notes

vadd4, vsub4, varvg4, vabsdiff4, vmin4, vmax4 require sm_30 or higher.

Examples

vadd4.s32.s32.u32.sat  r1, r2, r3, r1;
vsub4.s32.s32.s32.sat  r1.b0, r2.b3210, r3.b7654, r1;
vmin4.s32.u32.u32.add  r1.b00, r2.b0000, r3.b2222, r1;

9.7.18.2.4. SIMD Video Instructions: vset4

vset4

Integer quad byte SIMD comparison.

Syntax

// SIMD instruction with secondary SIMD merge operation
vset4.atype.btype.cmp  d{.mask}, a{.asel}, b{.bsel}, c;

// SIMD instruction with secondary accumulate operation
vset4.atype.btype.cmp.add  d{.mask}, a{.asel}, b{.bsel}, c;

.atype = .btype = { .u32, .s32 };
.cmp   = { .eq, .ne, .lt, .le, .gt, .ge };
.mask  = { .b0,
           .b1, .b10
           .b2, .b20, .b21, .b210,
           .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 };
    defaults to .b3210
.asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 };
   .asel defaults to .b3210
   .bsel defaults to .b7654

Description

Four-way SIMD parallel comparison with secondary operation.

Elements of each quad byte source to the operation are selected from any of the eight bytes in the two source operands a and b using the asel and bsel modifiers.

The selected bytes are then compared in parallel.

The intermediate result of the comparison is always unsigned, and therefore the bytes of destination d and operand c are also unsigned.

For instructions with a secondary SIMD merge operation:

    For byte positions indicated in mask, the selected byte results are copied into destination d. For all other positions, the corresponding byte from source operand b is copied to d.

For instructions with a secondary accumulate operation:

    For byte positions indicated in mask, the selected byte results are added to operand c, producing a result in d.

Semantics

// extract quads of bytes and sign- or zero-extend
// based on operand type
Va = extractAndSignExt_4( a, b, .asel, .atype );
Vb = extractAndSignExt_4( a, b, .bsel, .btype );
Vc = extractAndSignExt_4( c );
for (i=0; i<4; i++) {
    t[i] = compare( Va[i], Vb[i], cmp ) ? 1 : 0;
}
// secondary accumulate or SIMD merge
mask = extractMaskBits( .mask );
if (.add) {
    d = c;
    for (i=0; i<4; i++) {  d += mask[i] ? t[i] : 0;  }
} else {
    d = 0;
    for (i=0; i<4; i++)  {  d |= mask[i] ? t[i] : Vc[i];  }
}

PTX ISA Notes

Introduced in PTX ISA version 3.0.

Target ISA Notes

vset4 requires sm_30 or higher.

Examples

vset4.s32.u32.lt      r1, r2, r3, r0;
vset4.u32.u32.ne.max  r1, r2, r3, r0;

9.7.19. Miscellaneous Instructions

The Miscellaneous instructions are:

    brkpt

    nanosleep

    pmevent

    trap

    setmaxnreg

9.7.19.1. Miscellaneous Instructions: brkpt

brkpt

Breakpoint.

Syntax

brkpt;

Description

Suspends execution.

PTX ISA Notes

Introduced in PTX ISA version 1.0.

Target ISA Notes

brkpt requires sm_11 or higher.

Examples

    brkpt;
@p  brkpt;

9.7.19.2. Miscellaneous Instructions: nanosleep

nanosleep

Suspend the thread for an approximate delay given in nanoseconds.

Syntax

nanosleep.u32 t;

Description

Suspends the thread for a sleep duration approximately close to the delay t, specified in nanoseconds. t may be a register or an immediate value.

The sleep duration is approximated, but guaranteed to be in the interval [0, 2*t]. The maximum sleep duration is 1 millisecond. The implementation may reduce the sleep duration for individual threads within a warp such that all sleeping threads in the warp wake up together.

PTX ISA Notes

nanosleep introduced in PTX ISA 6.3.

Target ISA Notes

nanosleep requires sm_70 or higher.

Examples

.reg .b32 r;
.reg .pred p;

nanosleep.u32 r;
nanosleep.u32 42;
@p nanosleep.u32 r;

9.7.19.3. Miscellaneous Instructions: pmevent

pmevent

Trigger one or more Performance Monitor events.

Syntax

pmevent       a;    // trigger a single performance monitor event
pmevent.mask  a;    // trigger one or more performance monitor events

Description

Triggers one or more of a fixed number of performance monitor events, with event index or mask specified by immediate operand a.

pmevent (without modifier .mask) triggers a single performance monitor event indexed by immediate operand a, in the range 0..15.

pmevent.mask triggers one or more of the performance monitor events. Each bit in the 16-bit immediate operand a controls an event.

Programmatic performance moniter events may be combined with other hardware events using Boolean functions to increment one of the four performance counters. The relationship between events and counters is programmed via API calls from the host.

Notes

Currently, there are sixteen performance monitor events, numbered 0 through 15.

PTX ISA Notes

pmevent introduced in PTX ISA version 1.4.

pmevent.mask introduced in PTX ISA version 3.0.

Target ISA Notes

pmevent supported on all target architectures.

pmevent.mask requires sm_20 or higher.

Examples

    pmevent      1;
@p  pmevent      7;
@q  pmevent.mask 0xff;

9.7.19.4. Miscellaneous Instructions: trap

trap

Perform trap operation.

Syntax

trap;

Description

Abort execution and generate an interrupt to the host CPU.

PTX ISA Notes

Introduced in PTX ISA version 1.0.

Target ISA Notes

Supported on all target architectures.

Examples

    trap;
@p  trap;

9.7.19.5. Miscellaneous Instructions: setmaxnreg

setmaxnreg

Hint to change the number of registers owned by the warp.

Syntax

setmaxnreg.action.sync.aligned.u32 imm-reg-count;

.action = { .inc, .dec };

Description

setmaxnreg provides a hint to the system to update the maximum number of per-thread registers owned by the executing warp to the value specified by the imm-reg-count operand.

Qualifier .dec is used to release extra registers such that the absolute per-thread maximum register count is reduced from its current value to imm-reg-count. Qualifier .inc is used to request additional registers such that the absolute per-thread maximum register count is increased from its current value to imm-reg-count.

A pool of available registers is maintained per-CTA. Register adjustments requested by the setmaxnreg instructions are handled by supplying extra registers from this pool to the requesting warp or by releasing extra registers from the requesting warp to this pool, depending upon the value of the .action qualifier.

The setmaxnreg.inc instruction blocks the execution until enough registers are available in the CTA’s register pool. After the instruction setmaxnreg.inc obtains new registers from the CTA pool, the initial contents of the new registers are undefined. The new registers must be initialized before they are used.

The same setmaxnreg instruction must be executed by all warps in a warpgroup. After executing a setmaxnreg instruction, all warps in the warpgroup must synchronize explicitly before executing subsequent setmaxnreg instructions. If a setmaxnreg instruction is not executed by all warps in the warpgroup, then the behavior is undefined.

Operand imm-reg-count is an integer constant. The value of imm-reg-count must be in the range 24 to 256 (both inclusive) and must be a multiple of 8.

Changes to the register file of the warp always happen at the tail-end of the register file.

The setmaxnreg instruction requires that the kernel has been launched with a valid value of maximum number of per-thread registers specified via the appropriate compilation via the appropriate compile-time option or the appropriate performance tuning directive. Otherwise, the setmaxnreg instruction may have no effect.

When qualifier .dec is specified, the maximum number of per-thread registers owned by the warp prior to the execution of setmaxnreg instruction should be greater than or equal to the imm-reg-count. Otherwise, the behaviour is undefined.

When qualifier .inc is specified, the maximum number of per-thread registers owned by the warp prior to the execution of setmaxnreg instruction should be less than or equal to the imm-reg-count. Otherwise, the behaviour is undefined.

The mandatory .sync qualifier indicates that setmaxnreg instruction causes the executing thread to wait until all threads in the warp execute the same setmaxnreg instruction before resuming execution.

The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the same setmaxnreg instruction. In conditionally executed code, setmaxnreg instruction should only be used if it is known that all threads in warpgroup evaluate the condition identically, otherwise the behavior is undefined.

PTX ISA Notes

Introduced in PTX ISA version 8.0.

Target ISA Notes

Supported on following architectures:

    sm_90a

    sm_100a

    sm_101a (Renamed to sm_110a from PTX ISA version 9.0)

    sm_120a

    And is supported on following family-specific architectures from PTX ISA version 8.8:

        sm_100f or higher in the same family

        sm_101f or higher in the same family (Renamed to sm_110f from PTX ISA version 9.0)

        sm_120f or higher in the same family

    sm_110f or higher in the same family

Examples

setmaxnreg.dec.sync.aligned.u32 64;
setmaxnreg.inc.sync.aligned.u32 192;
