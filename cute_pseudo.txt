# Conceptual CuTe/Python implementation structure

from cutlass import cute
import torch

# Problem dimensions
N = 4096  # Vector length / Matrix rows
M = 8192  # Matrix columns / Output length  
L = 32    # Batch size
K = 128   # Tile size for K dimension (must match MMA instruction)

# Assuming mxf4nvf4 format with scale_vec::2X
BLOCK_M = 128  # Process 128 output elements at once
BLOCK_N = 128  # Process 128 vector/matrix elements (K dimension)
BLOCK_K = 128  # Same as BLOCK_N for GEMV

# Data types
ElementVector = cutlass.nv_float4_t  # NVFP4 for vector
ElementMatrix = cutlass.nv_float4_t  # NVFP4 for matrix
ElementAccum = cutlass.float32       # FP32 accumulation

# Create layouts
# Vector: [L, N] with scales [L, N/16]
vector_layout = cute.make_layout(
    cute.make_shape(L, N),
    cute.make_stride(N, 1)  # Batch-major
)

# Matrix: [L, M, N] with scales [L, M, N/16]  
matrix_layout = cute.make_layout(
    cute.make_shape(L, M, N),
    cute.make_stride(M * N, N, 1)  # Batch-major, then column-major
)

# Scale factor layouts (1 scale per 16 FP4 values)
vector_scale_layout = cute.make_layout(
    cute.make_shape(L, N // 16),
    cute.make_stride(N // 16, 1)
)

matrix_scale_layout = cute.make_layout(
    cute.make_shape(L, M, N // 16),
    cute.make_stride(M * (N // 16), N // 16, 1)
)



# Tile the tensors for computation
def batched_gemv_kernel(
    vector_ptr,      # Device pointer to vector data
    matrix_ptr,      # Device pointer to matrix data
    output_ptr,      # Device pointer to output
    vector_scale_ptr,  # Device pointer to vector scales
    matrix_scale_ptr,  # Device pointer to matrix scales
):
    # Get batch/block indices
    batch_idx = cute.blockIdx_z()
    m_block_idx = cute.blockIdx_x()
    
    # Tile the global tensors
    # Vector tile: [BLOCK_K] - this gets REUSED across all M
    vector_tile = cute.make_tensor(
        vector_ptr,
        cute.make_layout(cute.make_shape(BLOCK_K))
    )
    
    # Vector scale tile: [BLOCK_K // 16]
    vector_scale_tile = cute.make_tensor(
        vector_scale_ptr,
        cute.make_layout(cute.make_shape(BLOCK_K // 16))
    )
    
    # Matrix tile: [BLOCK_M, BLOCK_K] - changes for each M block
    matrix_tile = cute.make_tensor(
        matrix_ptr,
        cute.make_layout(cute.make_shape(BLOCK_M, BLOCK_K))
    )
    
    # Matrix scale tile: [BLOCK_M, BLOCK_K // 16]
    matrix_scale_tile = cute.make_tensor(
        matrix_scale_ptr,
        cute.make_layout(cute.make_shape(BLOCK_M, BLOCK_K // 16))
    )
    
    # TMEM allocation for accumulator
    tmem_alloc = cute.allocate_tmem(columns=64)  # Allocate TMEM columns
    
    # Output accumulator in TMEM: [BLOCK_M]
    accum_tmem = cute.make_tensor(
        tmem_alloc,
        cute.make_layout(cute.make_shape(BLOCK_M))
    )
    
    # Initialize accumulator to zero
    cute.clear(accum_tmem)
    
    # ==================================================================
    # MAIN COMPUTATION LOOP - Loop over N dimension in chunks of BLOCK_K
    # ==================================================================
    
    for k_block in range(N // BLOCK_K):
        k_offset = k_block * BLOCK_K
        
        # ---------------------------------------------------------
        # STEP 1: Load vector tile to SMEM (REUSED for all M blocks)
        # ---------------------------------------------------------
        vector_smem = cute.make_shared_tensor(
            cute.make_layout(cute.make_shape(BLOCK_K))
        )
        
        # Copy from global to shared memory
        cute.copy(
            vector_tile[batch_idx, k_offset : k_offset + BLOCK_K],
            vector_smem
        )
        
        # Load vector scales to TMEM
        vector_scale_tmem = cute.make_tmem_tensor(
            cute.make_layout(cute.make_shape(BLOCK_K // 16))
        )
        
        cute.copy(
            vector_scale_tile[batch_idx, k_offset // 16 : (k_offset + BLOCK_K) // 16],
            vector_scale_tmem
        )
        
        # ---------------------------------------------------------  
        # STEP 2: Load matrix tile to SMEM (specific to this M block)
        # ---------------------------------------------------------
        matrix_smem = cute.make_shared_tensor(
            cute.make_layout(cute.make_shape(BLOCK_M, BLOCK_K))
        )
        
        cute.copy(
            matrix_tile[batch_idx, m_block_idx * BLOCK_M : (m_block_idx + 1) * BLOCK_M, 
                       k_offset : k_offset + BLOCK_K],
            matrix_smem
        )
        
        # Load matrix scales to TMEM
        matrix_scale_tmem = cute.make_tmem_tensor(
            cute.make_layout(cute.make_shape(BLOCK_M, BLOCK_K // 16))
        )
        
        cute.copy(
            matrix_scale_tile[batch_idx, m_block_idx * BLOCK_M : (m_block_idx + 1) * BLOCK_M,
                            k_offset // 16 : (k_offset + BLOCK_K) // 16],
            matrix_scale_tmem
        )
        
        # ---------------------------------------------------------
        # STEP 3: Execute MMA instruction
        # ---------------------------------------------------------
        # Create descriptors for the MMA instruction
        matrix_desc = cute.make_smem_desc(matrix_smem)
        vector_desc = cute.make_smem_desc(vector_smem)
        
        # Call the tcgen05.mma instruction
        # This treats vector as a 1Ã—K "matrix" 
        cute.tcgen05_mma_block_scale_vec_2x(
            kind=cute.kind_mxf4nvf4_t,
            cta_group=cute.cta_group_1,
            d_tmem=accum_tmem,           # Accumulator in TMEM (REUSED!)
            a_desc=vector_desc,           # Vector in SMEM (broadcasted to all M)
            b_desc=matrix_desc,           # Matrix tile in SMEM
            idesc=0,                      # Instruction descriptor
            scale_A_tmem=vector_scale_tmem,   # Vector scales
            scale_B_tmem=matrix_scale_tmem,   # Matrix scales  
            enable_input_d=True if k_block > 0 else False  # Accumulate after first tile
        )
        
        # Synchronize to ensure MMA completes before next iteration
        cute.sync()
    
    # ==================================================================
    # STEP 4: Store accumulated results from TMEM to global memory
    # ==================================================================
    
    # Load from TMEM to registers
    accum_regs = cute.make_register_tensor(
        cute.make_layout(cute.make_shape(BLOCK_M))
    )
    
    cute.copy(accum_tmem, accum_regs)
    
    # Store to global memory
    output_global = cute.make_tensor(
        output_ptr,
        cute.make_layout(cute.make_shape(M))
    )
    
    cute.copy(
        accum_regs,
        output_global[batch_idx, m_block_idx * BLOCK_M : (m_block_idx + 1) * BLOCK_M]
    )
    
    # Deallocate TMEM
    cute.deallocate_tmem(tmem_alloc)