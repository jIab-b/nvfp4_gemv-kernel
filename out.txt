"**NVIDIA on GitHub ✅ success**
> Workflow [19388091329](<https://github.com/gpu-mode/discord-cluster-manager/actions/runs/19388091329>) completed
> Downloading artifacts... done
> ❌ Running benchmarks failed (internal error 1)

Running on:
* GPU: `NVIDIA B200`
* CPU: `INTEL(R) XEON(R) PLATINUM 8570`
* Runtime: `CUDA`
* Platform: `Linux-6.8.0-51-generic-x86_64-with-glibc2.35`
* Torch: `2.9.0+cu130`
# Running failed
Command ```bash
python3 eval.py benchmark /tmp/tmp9rc5umim```
exited with error code **1** after 15.27 seconds.

## Program stderr:
```
multiprocessing.pool.RemoteTraceback: 
\"\"\"
Traceback (most recent call last):
  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 2597, in _run_ninja_build
    subprocess.run(
  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 2.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker
    result = (True, func(*args, **kwds))
  File \"/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py\", line 208, in _run_single_benchmark
    from submission import custom_kernel
  File \"/home/runner/_work/discord-cluster-manager/discord-cluster-manager/submission.py\", line 140, in <module>
    module = load_inline(
  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 2051, in load_inline
    return _jit_compile(
  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 2134, in _jit_compile
    _write_ninja_file_and_build_library(
  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 2286, in _write_ninja_file_and_build_library
    _run_ninja_build(
  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 2614, in _run_ninja_build
    raise RuntimeError(message) from e
RuntimeError: Error building extension 'batched_scaled_gemv_cutlass': [1/2] /usr/local/cuda-13.0/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=batched_scaled_gemv_cutlass -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-13.0/include -isystem /usr/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -I/usr/local/cuda/include -gencode=arch=compute_100a,code=sm_100a -c /home/runner/.cache/torch_extensions/py310_cu130/batched_scaled_gemv_cutlass/cuda.cu -o cuda.cuda.o 
FAILED: [code=2] cuda.cuda.o 
/usr/local/cuda-13.0/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=batched_scaled_gemv_cutlass -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda-13.0/include -isystem /usr/include/python3.10 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -I/usr/local/cuda/include -gencode=arch=compute_100a,code=sm_100a -c /home/runner/.cache/torch_extensions/py310_cu130/batched_scaled_gemv_cutlass/cuda.cu -o cuda.cuda.o 
/home/runner/.cache/torch_extensions/py310_cu130/batched_scaled_gemv_cutlass/cuda.cu(17): error: namespace \"cutlass\" has no member \"nv_fp8_e4m3_t\"
  using ElementScale = cutlass::nv_fp8_e4m3_t;
                                ^

/opt/cutlass/4.3.0/include/cutlass/gemm/collective/builders/sm1xx_common.inl(672): error: static assertion failed with \"NV_F4 only supports RowMajor A, and ColMajorB\"
              static_assert((UmmaMajorA == UMMA::Major::K && UmmaMajorB == UMMA::Major::K), \"NV_F4 only supports RowMajor A, and ColMajorB\");
              ^
          detected during:
            instantiation of \"bool cutlass::gemm::collective::detail::blockscaled::check_input_datatypes<BuilderScheduleTag,ElementPairA,ElementPairB,UmmaMajorA,UmmaMajorB>() [with BuilderScheduleTag=cutlass::gemm::collective::KernelScheduleAuto, ElementPairA=ElementA, ElementPairB=ElementB, UmmaMajorA=cute::UMMA::Major::MN, UmmaMajorB=cute::UMMA::Major::K]\" at line 147 of /opt/cutlass/4.3.0/include/cutlass/gemm/collective/builders/sm100_blockscaled_umma_builder.inl
            instantiation of class \"cutlass::gemm::collective::CollectiveBuilder<ArchTag, cutlass::arch::OpClassBlockScaledTensorOp, ElementPairA, GmemLayoutATag, AlignmentA, ElementPairB, GmemLayoutBTag, AlignmentB, ElementAccumulator, TileShape_MNK, ClusterShape_MNK, StageCountType, BuilderScheduleTag, std::enable_if_t<<expression>, void>> [with ArchTag=ArchTag, ElementPairA=ElementA, GmemLayoutATag=LayoutATag, AlignmentA=32, ElementPairB=ElementB, GmemLayoutBTag=LayoutBTag, AlignmentB=32, ElementAccumulator=ElementAccumulator, TileShape_MNK=MmaTileShape, ClusterShape_MNK=ClusterShape, StageCountType=cutlass::gemm::collective::StageCountAutoCarveout<26624>, BuilderScheduleTag=cutlass::gemm::collective::KernelScheduleAuto]\" at line 57 of /home/runner/.cache/torch_extensions/py310_cu130/batched_scaled_gemv_cutlass/cuda.cu

/opt/cutlass/4.3.0/include/cutlass/gemm/collective/builders/sm1xx_common.inl(783): error: static assertion failed with \"Incorrect SfVectorSize for MX_F4F6F8 is deduced.\"
          static_assert(
          ^
          detected during:
            instantiation of \"auto cutlass::gemm::collective::detail::blockscaled::select_instr<ElementPairA,ElementPairB,ElementAccumulator,UmmaMajorA,UmmaMajorB,BuilderScheduleTag>() [with ElementPairA=ElementA, ElementPairB=ElementB, ElementAccumulator=ElementAccumulator, UmmaMajorA=cute::UMMA::Major::MN, UmmaMajorB=cute::UMMA::Major::K, BuilderScheduleTag=cutlass::gemm::collective::KernelScheduleAuto]\" at line 150 of /opt/cutlass/4.3.0/include/cutlass/gemm/collective/builders/sm100_blockscaled_umma_builder.inl
            instantiation of class \"cutlass::gemm::collective::CollectiveBuilder<ArchTag, cutlass::arch::OpClassBlockScaledTensorOp, ElementPairA, GmemLayoutATag, AlignmentA, ElementPairB, GmemLayoutBTag, AlignmentB, ElementAccumulator, TileShape_MNK, ClusterShape_MNK, StageCountType, BuilderScheduleTag, std::enable_if_t<<expression>, void>> [with ArchTag=ArchTag, ElementPairA=ElementA, GmemLayoutATag=LayoutATag, AlignmentA=32, ElementPairB=ElementB, GmemLayoutBTag=LayoutBTag, AlignmentB=32, ElementAccumulator=ElementAccumulator, TileShape_MNK=MmaTileShape, ClusterShape_MNK=ClusterShape, StageCountType=cutlass::gemm::collective::StageCountAutoCarveout<26624>, BuilderScheduleTag=cutlass::gemm::collective::KernelScheduleAuto]\" at line 57 of /home/runner/.cache/torch_extensions/py310_cu130/batched_scaled_gemv_cutlass/cuda.cu

/opt/cutlass/4.3.0/include/cutlass/gemm/collective/builders/sm1xx_common.inl(794): error: static assertion failed with \"Only MXF4 support with non-TN and MMA.MXF8F6F4.\"
          static_assert(cute::is_same_v<ElementSF, cutlass::float_ue8m0_t> &&
          ^
          detected during:
            instantiation of \"auto cutlass::gemm::collective::detail::blockscaled::select_instr<ElementPairA,ElementPairB,ElementAccumulator,UmmaMajorA,UmmaMajorB,BuilderScheduleTag>() [with ElementPairA=ElementA, ElementPairB=ElementB, ElementAccumulator=ElementAccumulator, UmmaMajorA=cute::UMMA::Major::MN, UmmaMajorB=cute::UMMA::Major::K, BuilderScheduleTag=cutlass::gemm::collective::KernelScheduleAuto]\" at line 150 of /opt/cutlass/4.3.0/include/cutlass/gemm/collective/builders/sm100_blockscaled_umma_builder.inl
            instantiation of class \"cutlass::gemm::collective::CollectiveBuilder<ArchTag, cutlass::arch::OpClassBlockScaledTensorOp, ElementPairA, GmemLayoutATag, AlignmentA, ElementPairB, GmemLayoutBTag, AlignmentB, ElementAccumulator, TileShape_MNK, ClusterShape_MNK, StageCountType, BuilderScheduleTag, std::enable_if_t<<expression>, void>> [with ArchTag=ArchTag, ElementPairA=ElementA, GmemLayoutATag=LayoutATag, AlignmentA=32, ElementPairB=ElementB, GmemLayoutBTag=LayoutBTag, AlignmentB=32, ElementAccumulator=ElementAccumulator, TileShape_MNK=MmaTileShape, ClusterShape_MNK=ClusterShape, StageCountType=cutlass::gemm::collective::StageCountAutoCarveout<26624>, BuilderScheduleTag=cutlass::gemm::collective::KernelScheduleAuto]\" at line 57 of /home/runner/.cache/torch_extensions/py310_cu130/batched_scaled_gemv_cutlass/cuda.cu

/opt/cutlass/4.3.0/include/cutlass/gemm/collective/builders/sm100_blockscaled_umma_builder.inl(164): error: static assertion failed with \"TileSize and MNK Major does not met with MMA Mix 8-bit TMA load requirement\"
    static_assert(detail::sm1xx_gemm_check_for_f8f6f4_mix8bit_requirement<ElementAMma, ElementBMma,
    ^
          detected during instantiation of class \"cutlass::gemm::collective::CollectiveBuilder<ArchTag, cutlass::arch::OpClassBlockScaledTensorOp, ElementPairA, GmemLayoutATag, AlignmentA, ElementPairB, GmemLayoutBTag, AlignmentB, ElementAccumulator, TileShape_MNK, ClusterShape_MNK, StageCountType, BuilderScheduleTag, std::enable_if_t<<expression>, void>> [with ArchTag=ArchTag, ElementPairA=ElementA, GmemLayoutATag=LayoutATag, AlignmentA=32, ElementPairB=ElementB, GmemLayoutBTag=LayoutBTag, AlignmentB=32, ElementAccumulator=ElementAccumulator, TileShape_MNK=MmaTileShape, ClusterShape_MNK=ClusterShape, StageCountType=cutlass::gemm::collective::StageCountAutoCarveout<26624>, BuilderScheduleTag=cutlass::gemm::collective::KernelScheduleAuto]\" at line 57 of /home/runner/.cache/torch_extensions/py310_cu130/batched_scaled_gemv_cutlass/cuda.cu

/opt/cutlass/4.3.0/include/cute/atom/copy_traits_sm90_tma.hpp(739): error: static assertion failed with \"TMA requires CTA_Tile and SLayout top-level size equivalence.\"
    static_assert(decltype(size(slayout) == size(cta_v_map))::value, \"TMA requires CTA_Tile and SLayout top-level size equivalence.\")
    ^
          detected during:
            instantiation of \"auto cute::detail::construct_tma_gbasis<TmaInternalType,GEngine,GLayout,SShape,SStride,VShape,VStride>(const cute::Tensor<GEngine, GLayout> &, const cute::Layout<SShape, SStride> &, const cute::Layout<VShape, VStride> &) [with TmaInternalType=uint16_t, GEngine=cute::ViewEngine<const cutlass::float_ue4m3_t *>, GLayout=cute::Layout<cute::tuple<cute::tuple<cute::tuple<cute::_32, cute::_4>, int>, cute::tuple<cute::tuple<cute::_32, cute::_4>, int>, cute::tuple<cute::_1, int>>, cute::tuple<cute::tuple<cute::tuple<cute::_16, cute::_4>, int>, cute::tuple<cute::tuple<cute::C<0>, cute::C<1>>, cute::_512>, cute::tuple<cute::_0, int32_t>>>, SShape=cute::tuple<cute::tuple<cute::tuple<cute::tuple<cute::_32, cute::_4>, cute::C<1>>, cute::tuple<cute::_32, cute::_1>>, cute::_1, cute::tuple<cute::C<4>, cute::C<0>>>, SStride=cute::tuple<cute::tuple<cute::tuple<cute::tuple<cute::_16, cute::_4>, cute::C<512>>, cute::tuple<cute::C<0>, cute::C<1>>>, cute::_0, cute::tuple<cute::_1, cute::_512>>, VShape=cute::tuple<cute::tuple<cute::tuple<cute::_32, cute::_4>, cute::_32>, cute::_1, cute::_2>, VStride=cute::tuple<cute::tuple<cute::tuple<cute::ScaledBasis<cute::C<1>, 0, 0, 0>, cute::ScaledBasis<cute::C<1>, 0, 0, 1>>, cute::ScaledBasis<cute::C<1>, 1, 0, 0>>, cute::C<0>, cute::ScaledBasis<cute::C<1>, 1, 0, 1>>]\" at line 1135
            instantiation of \"auto cute::detail::make_tma_copy_atom<TmaInternalType,CopyOp,GEngine,GLayout,SLayout,VShape,VStride>(CopyOp, const cute::Tensor<GEngine, GLayout> &, const SLayout &, const uint32_t &, const cute::Layout<VShape, VStride> &) [with TmaInternalType=uint16_t, CopyOp=cute::SM90_TMA_LOAD, GEngine=cute::ViewEngine<const cutlass::float_ue4m3_t *>, GLayout=cute::Layout<cute::tuple<cute::tuple<cute::tuple<cute::_32, cute::_4>, int>, cute::tuple<cute::tuple<cute::_32, cute::_4>, int>, cute::tuple<cute::_1, int>>, cute::tuple<cute::tuple<cute::tuple<cute::_16, cute::_4>, int>, cute::tuple<cute::tuple<cute::C<0>, cute::C<1>>, cute::_512>, cute::tuple<cute::_0, int32_t>>>, SLayout=cute::Layout<cute::tuple<cute::tuple<cute::tuple<cute::tuple<cute::_32, cute::_4>, cute::C<1>>, cute::tuple<cute::_32, cute::_1>>, cute::_1, cute::tuple<cute::C<4>, cute::C<0>>>, cute::tuple<cute::tuple<cute::tuple<cute::tuple<cute::_16, cute::_4>, cute::C<512>>, cute::tuple<cute::C<0>, cute::C<1>>>, cute::_0, cute::tuple<cute::_1, cute::_512>>>, VShape=cute::tuple<cute::tuple<cute::tuple<cute::_32, cute::_4>, cute::_32>, cute::_1, cute::_2>, VStride=cute::tuple<cute::tuple<cute::tuple<cute::ScaledBasis<cute::C<1>, 0, 0, 0>, cute::ScaledBasis<cute::C<1>, 0, 0, 1>>, cute::ScaledBasis<cute::C<1>, 1, 0, 0>>, cute::C<0>, cute::ScaledBasis<cute::C<1>, 1, 0, 1>>]\" at line 445 of /opt/cutlass/4.3.0/include/cute/atom/copy_traits_sm100_tma.hpp
            instantiation of \"auto cute::make_tma_atom_A_sm100(const CopyOp &, const cute::Tensor<GEngine, GLayout> &, const SLayout &, const MMA_Tiler &, const cute::TiledMMA<Args...> &, const ClusterShapeVMNK &) [with TmaInternalType=uint16_t, CopyOp=cute::SM90_TMA_LOAD, GEngine=cute::ViewEngine<const cutlass::float_ue4m3_t *>, GLayout=cute::Layout<cute::tuple<cute::tuple<cute::tuple<cute::_32, cute::_4>, int>, cute::tuple<cute::tuple<cute::_32, cute::_4>, int>, cute::tuple<cute::_1, int>>, cute::tuple<cute::tuple<cute::tuple<cute::_16, cute::_4>, int>, cute::tuple<cute::tuple<cute::C<0>, cute::C<1>>, cute::_512>, cute::tuple<cute::_0, int32_t>>>, SLayout=cute::Layout<cute::tuple<cute::tuple<cute::tuple<cute::tuple<cute::_32, cute::_4>, cute::C<1>>, cute::tuple<cute::_32, cute::_1>>, cute::_1, cute::tuple<cute::C<4>, cute::C<0>>>, cute::tuple<cute::tuple<cute::tuple<cute::tuple<cute::_16, cute::_4>, cute::C<512>>, cute::tuple<cute::C<0>, cute::C<1>>>, cute::_0, cute::tuple<cute::_1, cute::_512>>>, MMA_Tiler=MmaTileShape, Args=<cute::MMA_Atom<cute::SM100_MMA_MXF8F6F4_SS<cutlass::detail::float_e2m1_unpacksmem_t, cutlass::detail::float_e2m1_unpacksmem_t, ElementAccumulator, cutlass::float_ue4m3_t, 128, 64, cute::UMMA::Major::MN, cute::UMMA::Major::K, cute::UMMA::ScaleIn::One, cute::UMMA::ScaleIn::One>>, cute::Layout<cute::tuple<cute::_1, cute::_1, cute::_1>, cute::tuple<cute::_0, cute::_0, cute::C<0>>>, cute::tuple<cute::Underscore, cute::Underscore, cute::Underscore>>, ClusterShapeVMNK=cute::Layout<cute::tuple<cute::tuple<cute::C<1>>, cute::_1, cute::_1, cute::_1>, cute::tuple<cute::tuple<cute::_0>, cute::_0, cute::_0, cute::C<0>>>]\" at line 428 of /opt/cutlass/4.3.0/include/cutlass/gemm/collective/sm100_blockscaled_mma_warpspecialized.hpp
[...] 142 lines omitted```"