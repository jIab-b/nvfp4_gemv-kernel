import torch
from torch.utils.cpp_extension import load_inline
from task import input_t, output_t

# CUDA extension built like submission.py, but kernel body is inline PTX asm.

cpp_source = r"""
#include <torch/extension.h>
torch::Tensor gemv_tcgen05(torch::Tensor a, torch::Tensor b,
                           torch::Tensor sfa, torch::Tensor sfb,
                           torch::Tensor c);
"""

cuda_source = r'''
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_fp16.h>
#include <cuda_fp8.h>

// Kernel launch parameters
constexpr int M_TILE = 128;
constexpr int N_TILE = 8;    // padded N
constexpr int K_TILE = 64;

// Descriptor construction helpers
__device__ __forceinline__ uint64_t make_smem_desc(uint32_t smem_addr, uint32_t leading_byte_offset,
                                                     uint32_t stride_byte_offset, int swizzle_mode = 0) {
    // Shared memory descriptor layout per PTX ISA 9.7.16.4.1
    // Bits [0-13]:   matrix start address (encoded)
    // Bits [16-29]:  leading dimension offset (encoded)
    // Bits [32-45]:  stride dimension offset (encoded)
    // Bits [46-48]:  0b001 (fixed)
    // Bits [49-51]:  base offset (0 for aligned)
    // Bit  [52]:     leading dimension mode (0=relative offset)
    // Bits [53-60]:  0b00000000 (fixed)
    // Bits [61-63]:  swizzle mode

    auto encode = [](uint32_t x) -> uint64_t { return (x & 0x3FFFF) >> 4; };

    uint64_t desc = 0;
    desc |= encode(smem_addr);                          // bits 0-13
    desc |= (encode(leading_byte_offset) << 16);       // bits 16-29
    desc |= (encode(stride_byte_offset) << 32);        // bits 32-45
    desc |= (1ULL << 46);                              // bits 46-48 = 0b001
    desc |= (0ULL << 49);                              // bits 49-51 = 0 (base offset)
    desc |= (0ULL << 52);                              // bit 52 = 0 (relative mode)
    desc |= (0ULL << 53);                              // bits 53-60 = 0
    desc |= (((uint64_t)swizzle_mode) << 61);         // bits 61-63

    return desc;
}

__device__ __forceinline__ uint32_t make_idesc_mxf4nvf4(int M, int N, int K,
                                                          bool scale_type_ue4m3 = false,
                                                          int scale_factor_a_id = 0,
                                                          int scale_factor_b_id = 0) {
    // Instruction descriptor for .kind::mxf4nvf4 per PTX ISA Table 44
    // Bits [0-1]:   Reserved = 0
    // Bit  [2]:     Sparsity (0=dense)
    // Bit  [3]:     Reserved = 0
    // Bits [4-5]:   Matrix B Scale Factor Data ID (0 or 2)
    // Bit  [6]:     Reserved = 0
    // Bits [7-9]:   atype (E2M1 = 1)
    // Bits [10-11]: btype (E2M1 = 1, only 2 bits for mxf4)
    // Bit  [12]:    Reserved = 0
    // Bit  [13]:    Negate A (0=no)
    // Bit  [14]:    Negate B (0=no)
    // Bit  [15]:    Transpose A (0=no)
    // Bit  [16]:    Transpose B (0=no)
    // Bits [17-22]: N dimension (N >> 3)
    // Bit  [23]:    Scale Matrix Type (0=UE4M3, 1=UE8M0)
    // Bits [24-26]: Reserved = 0
    // Bits [27-28]: M dimension (M >> 7)
    // Bits [29-30]: Matrix A Scale Factor Data ID (0 or 2)
    // Bit  [31]:    K Dimension (0 for K=64 dense)

    uint32_t idesc = 0;
    idesc |= (0 << 0);                          // bits 0-1: reserved
    idesc |= (0 << 2);                          // bit 2: dense
    idesc |= (0 << 3);                          // bit 3: reserved
    idesc |= ((scale_factor_b_id & 0x3) << 4);  // bits 4-5: B scale ID
    idesc |= (0 << 6);                          // bit 6: reserved
    idesc |= (1 << 7);                          // bits 7-9: atype=E2M1
    idesc |= (1 << 10);                         // bits 10-11: btype=E2M1
    idesc |= (0 << 12);                         // bit 12: reserved
    idesc |= (0 << 13);                         // bit 13: no negate A
    idesc |= (0 << 14);                         // bit 14: no negate B
    idesc |= (0 << 15);                         // bit 15: no transpose A
    idesc |= (0 << 16);                         // bit 16: no transpose B
    idesc |= (((N >> 3) & 0x3F) << 17);         // bits 17-22: N>>3
    idesc |= ((scale_type_ue4m3 ? 0 : 1) << 23); // bit 23: scale type
    idesc |= (0 << 24);                         // bits 24-26: reserved
    idesc |= (((M >> 7) & 0x3) << 27);          // bits 27-28: M>>7
    idesc |= ((scale_factor_a_id & 0x3) << 29); // bits 29-30: A scale ID
    idesc |= (0 << 31);                         // bit 31: K=64 dense

    return idesc;
}

// Inline-PTX tcgen05 kernel with full async double-buffering
__global__ void gemv_tcgen05_kernel(const int8_t* a,
                                    const int8_t* b,
                                    const int8_t* sfa,
                                    const int8_t* sfb,
                                    half* c,
                                    int M, int K, int L) {
    // Block coords
    int tile_m = blockIdx.x;
    int tile_k = blockIdx.y;
    int tile_l = blockIdx.z;
    int m_base = tile_m * M_TILE;
    int k_base = tile_k * K_TILE;

    // Boundary checks for grid-tiled parallelism
    if (m_base >= M || k_base >= K || tile_l >= L) return;

    // Thread identification
    int warpId = threadIdx.x / 32;
    int laneId = threadIdx.x % 32;

    // SMEM ping-pong buffers (128B aligned for cp.async.bulk)
    __shared__ alignas(128) int8_t sm_a[2][4096];      // 2 × (128×64 FP4)
    __shared__ alignas(128) int8_t sm_b[2][256];       // 2 × (64×8 FP4)
    __shared__ alignas(128) int8_t sm_sfa[2][512];     // 2 × (128×4 FP8)
    __shared__ alignas(128) int8_t sm_sfb[2][64];      // 2 × (8×4 FP8)
    __shared__ alignas(128) half sm_d_writeback[1024]; // 128×8 fp16 staging

    // TMEM addresses and mbarriers (written by warp0)
    __shared__ uint32_t sm_taddr_a;
    __shared__ uint32_t sm_taddr_sfa;
    __shared__ uint32_t sm_taddr_sfb;
    __shared__ uint32_t sm_taddr_d;
    __shared__ uint64_t mbar_g2s;
    __shared__ uint64_t mbar_mma;

    // Only warp 0 does all the work (SOL optimization: no barriers, max async)
    if (warpId != 0) return;

    // GMEM address calculations (K-major layout: a[m,k,l] = base + (m*K + k)/2 + l*stride)
    // Note: /2 because FP4 packs 2 elements per byte
    int num_k_tiles = K / K_TILE;
    const int8_t* g_a_base = a + tile_l + (m_base * K) / 2;
    const int8_t* g_b_base = b + tile_l;
    const int8_t* g_sfa_base = sfa + tile_l + m_base * (K / 16);
    const int8_t* g_sfb_base = sfb + tile_l;

    // Build descriptors for both ping-pong buffers
    uint64_t sdesc_a[2], sdesc_b[2], sdesc_sfa[2], sdesc_sfb[2];
    uint32_t idesc = make_idesc_mxf4nvf4(M_TILE, N_TILE, K_TILE, false, 0, 0);

    for (int buf = 0; buf < 2; buf++) {
        uint32_t addr_a = __cvta_generic_to_shared(&sm_a[buf][0]);
        uint32_t addr_b = __cvta_generic_to_shared(&sm_b[buf][0]);
        uint32_t addr_sfa = __cvta_generic_to_shared(&sm_sfa[buf][0]);
        uint32_t addr_sfb = __cvta_generic_to_shared(&sm_sfb[buf][0]);

        // A: 128 rows × 64 cols FP4, leading_dim=64/2=32B, stride=128*64/2=4096B
        sdesc_a[buf] = make_smem_desc(addr_a, 32, 4096);
        // B: 64 rows × 8 cols FP4, leading_dim=8/2=4B, stride=64*8/2=256B
        sdesc_b[buf] = make_smem_desc(addr_b, 4, 256);
        // sfa: 128 rows × 4 cols FP8, leading_dim=4B, stride=128*4=512B
        sdesc_sfa[buf] = make_smem_desc(addr_sfa, 4, 512);
        // sfb: 8 rows × 4 cols FP8, leading_dim=4B, stride=8*4=32B
        sdesc_sfb[buf] = make_smem_desc(addr_sfb, 4, 32);
    }

    // PTX inline assembly block - single warp does everything
    if (laneId == 0) {
        // Allocate TMEM
        asm volatile(
            "tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%0], 128;\n"
            "tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%1], 32;\n"
            "tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%2], 32;\n"
            "tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%3], 128;\n"
            :: "l"(&sm_taddr_a), "l"(&sm_taddr_sfa), "l"(&sm_taddr_sfb), "l"(&sm_taddr_d)
        );

        // Initialize mbarriers
        asm volatile(
            "mbarrier.init.shared.b64 [%0], 1;\n"
            "mbarrier.init.shared.b64 [%1], 1;\n"
            :: "l"(&mbar_g2s), "l"(&mbar_mma)
        );
    }

    // Load TMEM addresses
    uint32_t taddr_a, taddr_sfa, taddr_sfb, taddr_d;
    asm volatile("ld.shared.b32 %0, [%1];\n" : "=r"(taddr_a) : "l"(&sm_taddr_a));
    asm volatile("ld.shared.b32 %0, [%1];\n" : "=r"(taddr_sfa) : "l"(&sm_taddr_sfa));
    asm volatile("ld.shared.b32 %0, [%1];\n" : "=r"(taddr_sfb) : "l"(&sm_taddr_sfb));
    asm volatile("ld.shared.b32 %0, [%1];\n" : "=r"(taddr_d) : "l"(&sm_taddr_d));

    // Double-buffered async loop
    int buf_read = 0;
    int buf_write = 1;
    int phase = 0;

    // Prologue: issue load for tile 0 into buf[0] (DON'T WAIT - let it fly async)
    if (laneId == 0) {
        const int8_t* g_a_tile = g_a_base + (k_base / 2);
        const int8_t* g_b_tile = g_b_base + (k_base / 2);
        const int8_t* g_sfa_tile = g_sfa_base + (tile_k * 4);
        const int8_t* g_sfb_tile = g_sfb_base + (tile_k * 4);

        asm volatile(
            "cp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes.L2::cache_hint "
            "[%0], [%1], %2, [%3];\n"
            :: "l"(&sm_a[0][0]), "l"(g_a_tile), "n"(4096), "l"(&mbar_g2s)
        );
        asm volatile(
            "cp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes.L2::cache_hint "
            "[%0], [%1], %2, [%3];\n"
            :: "l"(&sm_b[0][0]), "l"(g_b_tile), "n"(256), "l"(&mbar_g2s)
        );
        asm volatile(
            "cp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes "
            "[%0], [%1], %2, [%3];\n"
            :: "l"(&sm_sfa[0][0]), "l"(g_sfa_tile), "n"(512), "l"(&mbar_g2s)
        );
        asm volatile(
            "cp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes "
            "[%0], [%1], %2, [%3];\n"
            :: "l"(&sm_sfb[0][0]), "l"(g_sfb_tile), "n"(64), "l"(&mbar_g2s)
        );
        asm volatile("cp.async.bulk.commit_group;\n");
    }

    // Main loop over K tiles
    for (int k = 0; k < num_k_tiles; k++) {
        // (A) Wait for buf_read to be ready NOW (blocking, but unavoidable)
        if (laneId == 0) {
            asm volatile("cp.async.bulk.wait_group 0;\n");
        }

        // (B) Immediately issue prefetch for k+1 into buf_write (overlaps with compute)
        if (k + 1 < num_k_tiles && laneId == 0) {
            int k_offset = (k + 1) * K_TILE / 2;  // byte offset
            const int8_t* g_a_tile = g_a_base + k_offset;
            const int8_t* g_b_tile = g_b_base + k_offset;
            const int8_t* g_sfa_tile = g_sfa_base + (k + 1) * 4;  // FP8: 4 bytes per K_TILE
            const int8_t* g_sfb_tile = g_sfb_base + (k + 1) * 4;

            asm volatile(
                "cp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes.L2::cache_hint "
                "[%0], [%1], %2, [%3];\n"
                :: "l"(&sm_a[buf_write][0]), "l"(g_a_tile), "n"(4096), "l"(&mbar_g2s)
            );
            asm volatile(
                "cp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes.L2::cache_hint "
                "[%0], [%1], %2, [%3];\n"
                :: "l"(&sm_b[buf_write][0]), "l"(g_b_tile), "n"(256), "l"(&mbar_g2s)
            );
            asm volatile(
                "cp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes "
                "[%0], [%1], %2, [%3];\n"
                :: "l"(&sm_sfa[buf_write][0]), "l"(g_sfa_tile), "n"(512), "l"(&mbar_g2s)
            );
            asm volatile(
                "cp.async.bulk.shared::cta.global.mbarrier::complete_tx::bytes "
                "[%0], [%1], %2, [%3];\n"
                :: "l"(&sm_sfb[buf_write][0]), "l"(g_sfb_tile), "n"(64), "l"(&mbar_g2s)
            );
            asm volatile("cp.async.bulk.commit_group;\n");
        }

        // (C) SMEM → TMEM (buf_read is ready)
        uint64_t sdesc_a_cur = sdesc_a[buf_read];
        uint64_t sdesc_sfa_cur = sdesc_sfa[buf_read];
        uint64_t sdesc_sfb_cur = sdesc_sfb[buf_read];
        uint64_t bdesc = sdesc_b[buf_read];

        asm volatile(
            "tcgen05.cp.cta_group::1.128x256b [%0], %1;\n"
            :: "r"(taddr_a), "l"(sdesc_a_cur)
        );
        asm volatile(
            "tcgen05.cp.cta_group::1.4x256b [%0], %1;\n"
            :: "r"(taddr_sfa), "l"(sdesc_sfa_cur)
        );
        asm volatile(
            "tcgen05.cp.cta_group::1.4x256b [%0], %1;\n"
            :: "r"(taddr_sfb), "l"(sdesc_sfb_cur)
        );

        // (D) MMA (enable_input_d = 1 for accumulation after first tile)
        uint32_t enable_input_d = (k > 0) ? 1 : 0;

        asm volatile(
            "tcgen05.mma.cta_group::1.kind::mxf4nvf4.block_scale.scale_vec::4X "
            "[%0], [%1], %2, %3, [%4], [%5], %6;\n"
            :: "r"(taddr_d), "r"(taddr_a), "l"(bdesc), "r"(idesc),
               "r"(taddr_sfa), "r"(taddr_sfb), "r"(enable_input_d)
        );

        // (E) Wait for MMA completion
        if (laneId == 0) {
            asm volatile(
                "tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [%0];\n"
                :: "l"(&mbar_mma)
            );
        }

        uint32_t pred;
        asm volatile(
            "{\n"
            ".reg .pred p;\n"
            "wait_loop_%=:\n"
            "mbarrier.try_wait.parity.b64 p, [%1], %2;\n"
            "selp.u32 %0, 1, 0, p;\n"
            "setp.eq.u32 p, %0, 0;\n"
            "@p bra wait_loop_%=;\n"
            "}\n"
            : "=r"(pred) : "l"(&mbar_mma), "r"(phase)
        );

        asm volatile("tcgen05.fence::after_thread_sync;\n");

        // Swap buffers and phase
        buf_read ^= 1;
        buf_write ^= 1;
        phase ^= 1;
    }

    // Writeback: TMEM → SMEM → GMEM with atomic accumulation across K-tiles
    asm volatile(
        "tcgen05.st.sync.aligned.16x256b.x8 [%0], %1;\n"
        :: "l"(&sm_d_writeback[0]), "r"(taddr_d)
    );

    // Warp 0 threads cooperatively write column 0 to GMEM
    for (int i = laneId; i < M_TILE; i += 32) {
        if (m_base + i < M) {
            half val = sm_d_writeback[i * N_TILE];  // column 0
            c[(m_base + i) * L + tile_l] = val;
        }
    }

    // (G) Deallocate TMEM
    if (laneId == 0) {
        asm volatile(
            "tcgen05.dealloc.cta_group::1.sync.aligned.b32 %0, 128;\n"
            "tcgen05.dealloc.cta_group::1.sync.aligned.b32 %0, 32;\n"
            "tcgen05.dealloc.cta_group::1.sync.aligned.b32 %0, 32;\n"
            "tcgen05.dealloc.cta_group::1.sync.aligned.b32 %0, 128;\n"
            :: "r"(taddr_a), "r"(taddr_sfa), "r"(taddr_sfb), "r"(taddr_d)
        );
        asm volatile("tcgen05.relinquish_alloc_permit.cta_group::1.sync.aligned;\n");
    }
}

torch::Tensor gemv_tcgen05(torch::Tensor a, torch::Tensor b,
                           torch::Tensor sfa, torch::Tensor sfb,
                           torch::Tensor c) {
    int M = a.size(0);
    int K = a.size(1) * 2;
    int L = a.size(2);
    int m_tiles = (M + M_TILE - 1) / M_TILE;
    int k_tiles = (K + K_TILE - 1) / K_TILE;
    dim3 grid(m_tiles, k_tiles, L);


    dim3 block(128, 1, 1);  // 4 warps; warp0 owns tcgen05
    gemv_tcgen05_kernel<<<grid, block>>>(reinterpret_cast<int8_t*>(a.data_ptr()),
                                         reinterpret_cast<int8_t*>(b.data_ptr()),
                                         reinterpret_cast<int8_t*>(sfa.data_ptr()),
                                         reinterpret_cast<int8_t*>(sfb.data_ptr()),
                                         reinterpret_cast<half*>(c.data_ptr()),
                                         M, K, L);
    return c;
}
'''

module = load_inline(
    name="gemv_tcgen05_inline",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=["gemv_tcgen05"],
    extra_cuda_cflags=[
        "-O3",
        "-std=c++17",
        "-gencode=arch=compute_100a,code=sm_100a",
    ],
    with_cuda=True,
    verbose=False,
)


def custom_kernel(data: input_t) -> output_t:
    """Entry used by the harness; kernel body is inline PTX skeleton."""
    a, b, sfa, sfb, _, _, c = data
    return module.gemv_tcgen05(
        a.view(torch.int8),
        b.view(torch.int8),
        sfa.view(torch.int8),
        sfb.view(torch.int8),
        c,
    )
