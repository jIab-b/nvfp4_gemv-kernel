.version 8.8
.target sm_100a
.address_size 64

//
// Best‑effort tcgen05 GEMV skeleton (not fully validated; for reference/testing).
// Tile sizes: M_TILE=128, N_TILE=8 (padded), K_TILE=64, .cta_group::1.
// Scaling: .kind::mxf4nvf4.block_scale.scale_vec::4X  (1 scale per 16 FP4 vals).
//

.visible .entry gemv_tcgen05(
    .param .u64 a_ptr,     // int8  A [M x K/2 x L]  (fp4 packed, K-major)
    .param .u64 b_ptr,     // int8  B [1 x K/2 x L]
    .param .u64 sfa_ptr,   // int8  scaleA [M x K/16 x L] (fp8)
    .param .u64 sfb_ptr,   // int8  scaleB [1 x K/16 x L] (fp8)
    .param .u64 c_ptr,     // half  C [M x 1 x L]
    .param .u32 M,
    .param .u32 K,
    .param .u32 L
) {
    // Registers
    .reg .b32  %r<32>;
    .reg .b64  %rd<48>;
    .reg .pred %p<8>;

    // Shared scratch for TMEM addresses (one warp writes, others can read)
    .shared .align 4 .b32 sm_taddr_a[2];
    .shared .align 4 .b32 sm_taddr_sfa[2];
    .shared .align 4 .b32 sm_taddr_sfb[2];
    .shared .align 4 .b32 sm_taddr_d[2];

    // mbarrier objects
    .shared .align 8 .b64 mbar_g2s;
    .shared .align 8 .b64 mbar_mma;

    //----------------------------------------------------------------------
    // Prolog: only warp0 executes alloc/init
    //----------------------------------------------------------------------
    mov.u32 %r0, %tid.x;
    setp.ge.u32 %p0, %r0, 32;
    @%p0 bra SKIP_ALLOC;

    // Tensor memory allocs (nCols are powers of two ≥32)
    tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sm_taddr_a],   128;
    tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sm_taddr_sfa], 32;
    tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sm_taddr_sfb], 32;
    tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sm_taddr_d],   128;

    // mbarrier init
    mbarrier.init.shared.b64 [mbar_g2s], 1;
    mbarrier.init.shared.b64 [mbar_mma], 1;

SKIP_ALLOC:
    //----------------------------------------------------------------------
    // Compute tile coordinates
    //----------------------------------------------------------------------
    mov.u32 %r1, %ctaid.x;            // tile id along M
    mov.u32 %r2, %ctaid.y;            // batch id (L)
    mul.lo.u32 %r3, %r1, 128;         // m_base = blockIdx.x * 128
    // guard: if m_base >= M, exit
    ld.param.u32 %r4, [M];
    setp.ge.u32 %p1, %r3, %r4;
    @%p1 bra EXIT;

    //----------------------------------------------------------------------
    // Load base pointers
    //----------------------------------------------------------------------
    ld.param.u64 %rd10, [a_ptr];
    ld.param.u64 %rd11, [b_ptr];
    ld.param.u64 %rd12, [sfa_ptr];
    ld.param.u64 %rd13, [sfb_ptr];
    ld.param.u64 %rd14, [c_ptr];

    ld.param.u32 %rK, [K];
    ld.param.u32 %rL, [L];

    // Offsets for batch l
    mul.wide.u32 %rd20, %r2, %rK;       // K elements per batch
    shr.u64 %rd21, %rd20, 1;            // bytes for A/B (K/2)
    div.u32 %rKsf, %rK, 16;             // K/16 for scales
    mul.wide.u32 %rd22, %r2, %rKsf;     // scale stride

    // Base pointers for this batch
    add.s64 %rd30, %rd10, %rd21;        // a_batch = a_ptr + l*K/2   (approx; ignoring M stride for brevity)
    add.s64 %rd31, %rd11, %rd21;        // b_batch similar
    add.s64 %rd32, %rd12, %rd22;        // sfa_batch
    add.s64 %rd33, %rd13, %rd22;        // sfb_batch

    //----------------------------------------------------------------------
    // Prologue load: tile 0 into SMEM buffer 0 (warp0 only)
    //----------------------------------------------------------------------
    @%p0 bra SKIP_LOAD0;

    // GMEM→SMEM async copies; using byte counts for one tile (128x64 FP4 = 4096B)
    // smem pointers symbolically: %rd40 etc.
    cvta.to.shared.u64 %rd40, sm_taddr_a;   // reuse shared for addresses
    // A tile
    cp.async.bulk.shared::cta.global.L2::128B [%rd40], %rd30, 4096, [mbar_g2s];
    // B tile (64x8 FP4 = 256B)
    add.s64 %rd41, %rd40, 4096;
    cp.async.bulk.shared::cta.global.L2::128B [%rd41], %rd31, 256, [mbar_g2s];
    // scaleA tile (128x4 FP8 = 512B)
    add.s64 %rd42, %rd41, 512;
    cp.async.bulk.shared::cta.global [%rd42], %rd32, 512, [mbar_g2s];
    // scaleB tile (4x8 FP8 = 32B)
    add.s64 %rd43, %rd42, 512;
    cp.async.bulk.shared::cta.global [%rd43], %rd33, 32, [mbar_g2s];

    cp.async.bulk.commit_group;
    cp.async.bulk.wait_group 0;

SKIP_LOAD0:

    //----------------------------------------------------------------------
    // SMEM→TMEM copies (single warp issues)
    //----------------------------------------------------------------------
    @%p0 bra SKIP_CP_TMEM;

    // load taddrs
    ld.shared.u32 %r10, [sm_taddr_a];
    ld.shared.u32 %r11, [sm_taddr_sfa];
    ld.shared.u32 %r12, [sm_taddr_sfb];
    ld.shared.u32 %r13, [sm_taddr_d];

    // Descriptors would normally encode shape/stride; here we assume prebuilt
    // placeholders %rd50 etc. (left as zero for sketch).
    mov.u64 %rd50, 0;
    mov.u64 %rd51, 0;
    mov.u64 %rd52, 0;

    tcgen05.cp.cta_group::1.128x256b [%r10], %rd50;   // A
    tcgen05.cp.cta_group::1.4x256b   [%r11], %rd51;   // scale A
    tcgen05.cp.cta_group::1.4x256b   [%r12], %rd52;   // scale B

SKIP_CP_TMEM:

    //----------------------------------------------------------------------
    // MMA
    //----------------------------------------------------------------------
    @%p0 bra SKIP_MMA;

    // idesc placeholder (would encode m=128,n=8,k=64 kinds, types)
    mov.u64 %rd60, 0;
    // bdesc placeholder (SMEM descriptor for B)
    mov.u64 %rd61, 0;

    tcgen05.mma.cta_group::1.kind::mxf4nvf4.block_scale.scale_vec::4X
        [%r13], [%r10], %rd61, %rd60, [%r11], [%r12], 1;

    tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbar_mma];

    // wait for completion
WAIT_MMA:
    mbarrier.try_wait.parity.b64 %p2, [mbar_mma], 0;
    @!%p2 bra WAIT_MMA;
    tcgen05.fence::after_thread_sync;

SKIP_MMA:

    //----------------------------------------------------------------------
    // Writeback (sketch: TMEM->SMEM then global store of col0)
    //----------------------------------------------------------------------
    // Not fully specified; left as TODO for implementer.

EXIT:
    ret;
}
