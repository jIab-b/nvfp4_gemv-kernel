import torch
from torch.utils.cpp_extension import load_inline
from task import input_t, output_t

# CUDA extension built like submission.py, but kernel body is inline PTX asm.

cpp_source = r"""
#include <torch/extension.h>
torch::Tensor gemv_tcgen05(torch::Tensor a, torch::Tensor b,
                           torch::Tensor sfa, torch::Tensor sfb,
                           torch::Tensor c);
"""

cuda_source = r'''
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_fp16.h>
#include <cuda_fp8.h>

// Kernel launch parameters
constexpr int M_TILE = 128;
constexpr int N_TILE = 8;    // padded N
constexpr int K_TILE = 64;

// Inline-PTX tcgen05 skeleton. Real descriptors/offset math omitted.
__global__ void gemv_tcgen05_kernel(const int8_t* a,
                                    const int8_t* b,
                                    const int8_t* sfa,
                                    const int8_t* sfb,
                                    half* c,
                                    int M, int K, int L) {
    // Block coords
    int tile_m = blockIdx.x;
    int tile_l = blockIdx.y;
    int m_base = tile_m * M_TILE;
    if (m_base >= M || tile_l >= L) return;

    // Shared buffers to hold TMEM addresses (written by warp0)
    __shared__ uint32_t sm_taddr_a[2];
    __shared__ uint32_t sm_taddr_sfa[2];
    __shared__ uint32_t sm_taddr_sfb[2];
    __shared__ uint32_t sm_taddr_d[2];
    __shared__ uint64_t mbar_g2s;
    __shared__ uint64_t mbar_mma;

    // Inline PTX: tcgen05 skeleton (commented instructions to avoid assembler issues)
    asm volatile(R"ptx(
        // ---- alloc TMEM (warp0) ----
        // tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sm_taddr_a], 128;
        // tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sm_taddr_sfa], 32;
        // tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sm_taddr_sfb], 32;
        // tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [sm_taddr_d], 128;
        // mbarrier.init.shared.b64 [mbar_g2s], 1;
        // mbarrier.init.shared.b64 [mbar_mma], 1;

        // ---- GMEM -> SMEM async stage (tile 0) ----
        // cp.async.bulk.shared::cta.global.L2::128B [sm_a], [g_a], 4096, [mbar_g2s];
        // cp.async.bulk.shared::cta.global.L2::128B [sm_b], [g_b], 256,  [mbar_g2s];
        // cp.async.bulk.shared::cta.global [sm_sfa], [g_sfa], 512, [mbar_g2s];
        // cp.async.bulk.shared::cta.global [sm_sfb], [g_sfb], 32,  [mbar_g2s];
        // cp.async.bulk.commit_group;
        // cp.async.bulk.wait_group 0;

        // ---- SMEM -> TMEM ----
        // tcgen05.cp.cta_group::1.128x256b [tA], sdescA;
        // tcgen05.cp.cta_group::1.4x256b   [tSFA], sdescSFA;
        // tcgen05.cp.cta_group::1.4x256b   [tSFB], sdescSFB;

        // ---- MMA ----
        // tcgen05.mma.cta_group::1.kind::mxf4nvf4.block_scale.scale_vec::4X
        //     [tD], [tA], bdesc, idesc, [tSFA], [tSFB], 1;
        // tcgen05.commit.cta_group::1.mbarrier::arrive::one.b64 [mbar_mma];
        // mbarrier.try_wait.parity.b64 p0, [mbar_mma], 0;
        // tcgen05.fence::after_thread_sync;

        // ---- Writeback (TMEM -> SMEM -> GMEM) ----
        // tcgen05.st [sm_d], [tD];
        // ... ld.shared/st.global for column 0 ...
    )ptx");

    // Placeholder: write zeros to output for now to keep correctness defined
    if (threadIdx.x == 0) {
        int idx = m_base + tile_l * M;
        if (idx < M * L) c[idx] = __float2half(0.0f);
    }
}

torch::Tensor gemv_tcgen05(torch::Tensor a, torch::Tensor b,
                           torch::Tensor sfa, torch::Tensor sfb,
                           torch::Tensor c) {
    int M = a.size(0);
    int K = a.size(1) * 2;
    int L = a.size(2);
    dim3 grid((M + M_TILE - 1) / M_TILE, L, 1);
    dim3 block(128, 1, 1);  // 4 warps; warp0 owns tcgen05
    gemv_tcgen05_kernel<<<grid, block>>>(reinterpret_cast<int8_t*>(a.data_ptr()),
                                         reinterpret_cast<int8_t*>(b.data_ptr()),
                                         reinterpret_cast<int8_t*>(sfa.data_ptr()),
                                         reinterpret_cast<int8_t*>(sfb.data_ptr()),
                                         reinterpret_cast<half*>(c.data_ptr()),
                                         M, K, L);
    return c;
}
'''

module = load_inline(
    name="gemv_tcgen05_inline",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=["gemv_tcgen05"],
    extra_cuda_cflags=[
        "-O3",
        "-std=c++17",
        "-gencode=arch=compute_100a,code=sm_100a",
    ],
    with_cuda=True,
    verbose=False,
)


def custom_kernel(data: input_t) -> output_t:
    """Entry used by the harness; kernel body is inline PTX skeleton."""
    a, b, sfa, sfb, _, _, c = data
    return module.gemv_tcgen05(
        a.view(torch.int8),
        b.view(torch.int8),
        sfa.view(torch.int8),
        sfb.view(torch.int8),
        c,
    )
